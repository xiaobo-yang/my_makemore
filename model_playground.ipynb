{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".aukai.......... -> aukai.\n",
      ".ellanore....... -> ellanore.\n",
      ".liem........... -> liem.\n",
      ".aquarius....... -> aquarius.\n",
      ".joangel........ -> joangel.\n",
      ".wryn........... -> wryn.\n",
      ".isabela........ -> isabela.\n",
      ".astryd......... -> astryd.\n",
      ".maleik......... -> maleik.\n",
      ".emerick........ -> emerick.\n",
      ".natasha........ -> natasha.\n",
      ".kasandra....... -> kasandra.\n",
      ".aevin.......... -> aevin.\n",
      ".brason......... -> brason.\n",
      ".naiara......... -> naiara.\n",
      ".alanna......... -> alanna.\n",
      ".raunak......... -> raunak.\n",
      ".gohan.......... -> gohan.\n",
      ".ivie........... -> ivie.\n",
      ".alandis........ -> alandis.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([23595, 16]),\n",
       " torch.Size([2949, 16]),\n",
       " torch.Size([2950, 16]),\n",
       " torch.Size([23595, 16]),\n",
       " torch.Size([2949, 16]),\n",
       " torch.Size([2950, 16]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words = sorted(list(set(words))) # set cause uncontrollable randomnessï¼Œ sorted for reproducibility\n",
    "max_len = max(len(w) for w in words)\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "chs = list(set(''.join(words + ['.']))) # add special token\n",
    "chs = sorted(chs, reverse=False)\n",
    "stoi = {ch: i for i, ch in enumerate(chs)}\n",
    "itos = {i: ch for i, ch in enumerate(chs)}\n",
    "\n",
    "# predict next token use previous tokens\n",
    "vocab_size = len(chs)\n",
    "block_size = max_len + 1\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    x = torch.zeros(max_len + 1, dtype=torch.long)\n",
    "    y = torch.zeros(max_len + 1, dtype=torch.long)\n",
    "    x[1:1+len(w)] = torch.tensor([stoi[ch] for ch in w])\n",
    "    y[:len(w)] = torch.tensor([stoi[ch] for ch in w])\n",
    "    y[len(w)+1:] = -1 # mask the loss at the inactive locations\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "\n",
    "X = torch.stack(X)\n",
    "Y = torch.stack(Y)\n",
    "n1, n2  = int(0.8 * len(X)), int(0.9 * len(X))\n",
    "\n",
    "X_train, X_val, X_test = X.tensor_split([n1, n2])\n",
    "Y_train, Y_val, Y_test = Y.tensor_split([n1, n2])\n",
    "\n",
    "show = 20\n",
    "for x, y in zip(X_train[:show], Y_train[:show]):\n",
    "    sx = ''.join(itos[i.item()] for i in x)\n",
    "    sy = ''.join(itos[i.item()] for i in y if i.item() != -1)\n",
    "    print(f'{sx} -> {sy}')\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# move to manual backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# manual backprop\n",
    "from tiny_torch import *\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    block_size: int = block_size # length of the input sequences of integers\n",
    "    vocab_size: int = vocab_size # the input integers are in range [0 .. vocab_size -1]\n",
    "    # parameters below control the sizes of each model slightly differently\n",
    "    n_layer: int = 4\n",
    "    n_embd: int = 64\n",
    "    n_embd2: int = 64\n",
    "    n_head: int = 4\n",
    "    dtype: torch.dtype = torch.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_compare(model, model_t, lr=0.1, n_steps=100, eval_every=10, bs=32):\n",
    "    loss_fn = CrossEntropyLoss3d()\n",
    "    # train\n",
    "    torch.manual_seed(42)\n",
    "    for  step in range(n_steps):\n",
    "        idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "        x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "        # --- torch ---\n",
    "        # forward\n",
    "        logits_t = model_t(x)\n",
    "        loss_t = F.cross_entropy(logits_t.view(-1, logits_t.size(-1)), y.view(-1), ignore_index=-1)\n",
    "        # backward\n",
    "        loss_t.backward()\n",
    "        # update\n",
    "        for p_t in model_t.parameters():\n",
    "            p_t.data -= lr * p_t.grad\n",
    "            p_t.grad = None\n",
    "        # --- manual ---\n",
    "        # forward\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        # backward\n",
    "        dlogits = loss_fn.backward()\n",
    "        model.backward(dlogits)\n",
    "        # update\n",
    "        for p, g in zip(model.parameters(), model.grads()):\n",
    "            p.data -= lr * g\n",
    "        \n",
    "        \n",
    "        # eval\n",
    "        if step % eval_every == 0:\n",
    "            x, y = X_val, Y_val\n",
    "            with torch.no_grad():\n",
    "                logits_t = model_t(x)\n",
    "                val_loss_t = F.cross_entropy(logits_t.view(-1, logits_t.size(-1)), y.view(-1), ignore_index=-1)\n",
    "            logits = model(x)\n",
    "            val_loss = loss_fn(logits, y)\n",
    "            print(f'step {step:<8} || Train   || {loss.item():.15f} || Val   || {val_loss.item():.15f}')\n",
    "            print(f'              || Train_t || {loss_t.item():.15f} || Val_t || {val_loss_t.item():.15f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    \"\"\"\n",
    "    takes the previous block_size tokens, encodes them with a lookup table,\n",
    "    concatenates the vectors and predicts the next token with an MLP.\n",
    "\n",
    "    Reference:\n",
    "    Bengio et al. 2003 https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.wte = Embedding(config.vocab_size + 1, config.n_embd, dtype=config.dtype) # token embeddings table\n",
    "        # +1 in the line above for a special <BLANK> token that gets inserted if encoding a token\n",
    "        # before the beginning of the input sequence\n",
    "        self.mlp = Sequential([\n",
    "            Linear(self.block_size * config.n_embd, config.n_embd2, dtype=config.dtype),\n",
    "            Tanh(),\n",
    "            Linear(config.n_embd2, self.vocab_size, dtype=config.dtype)\n",
    "        ])\n",
    "        self.mlp[-1].weight.data *= 0.1\n",
    "        self.mlp[-1].bias.data *= 0.01\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of mlp parameters: %d\" % (n_params,))\n",
    "        self.config = config\n",
    "    \n",
    "    def parameters(self):\n",
    "        return list(self.wte.parameters()) + list(self.mlp.parameters())\n",
    "    \n",
    "    def grads(self):\n",
    "        return list(self.wte.grads()) + list(self.mlp.grads())\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def __call__(self, idx, targets=None):\n",
    "\n",
    "        # gather the word embeddings of the previous 3 words\n",
    "        idx_buf = []\n",
    "        embs = []\n",
    "        for k in range(self.block_size):\n",
    "            tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "            idx_buf.append(idx.unsqueeze(-1))\n",
    "            embs.append(tok_emb)\n",
    "            idx = torch.roll(idx, 1, 1)\n",
    "            idx[:, 0] = self.vocab_size # special <BLANK> token\n",
    "\n",
    "        # concat all of the embeddings together and pass through an MLP\n",
    "        x = torch.cat(embs, -1) # (b, t, n_embd * block_size)\n",
    "        logits = self.mlp(x)\n",
    "\n",
    "        # backward buffer\n",
    "        self.idx_buf = torch.cat(idx_buf, -1) # (b, t, t)\n",
    "\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        grad = self.mlp.backward(grad)\n",
    "        # mlp backprop to wte\n",
    "        b, t, _ = grad.shape # (b, t, n_embd * block_size)\n",
    "        grad = grad.view(b * t * self.config.block_size, self.config.n_embd) # (b*t*block_size, n_embd)\n",
    "        wte_weight = self.wte.weight\n",
    "        wte_grad = torch.zeros_like(wte_weight)\n",
    "        wte_grad.index_add_(dim=0, index=self.idx_buf.view(-1), source=grad)\n",
    "        self.wte.weight_grad = wte_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPtorch(nn.Module):\n",
    "    \"\"\"\n",
    "    takes the previous block_size tokens, encodes them with a lookup table,\n",
    "    concatenates the vectors and predicts the next token with an MLP.\n",
    "\n",
    "    Reference:\n",
    "    Bengio et al. 2003 https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.wte = nn.Embedding(config.vocab_size + 1, config.n_embd) # token embeddings table\n",
    "        # +1 in the line above for a special <BLANK> token that gets inserted if encoding a token\n",
    "        # before the beginning of the input sequence\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.block_size * config.n_embd, config.n_embd2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(config.n_embd2, self.vocab_size)\n",
    "        )\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of mlp parameters: %d\" % (n_params,))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # gather the word embeddings of the previous 3 words\n",
    "        embs = []\n",
    "        for k in range(self.block_size):\n",
    "            tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "            idx = torch.roll(idx, 1, 1)\n",
    "            idx[:, 0] = self.vocab_size # special <BLANK> token\n",
    "            embs.append(tok_emb)\n",
    "\n",
    "        # concat all of the embeddings together and pass through an MLP\n",
    "        x = torch.cat(embs, -1) # (b, t, n_embd * block_size)\n",
    "        logits = self.mlp(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mlp parameters: 3995\n",
      "number of mlp parameters: 3995\n",
      "step 0        || Train   || 3.297640663881704 || Val   || 3.267504153916392\n",
      "              || Train_t || 3.297640663881704 || Val_t || 3.267504153916391\n",
      "step 10       || Train   || 3.024899204673497 || Val   || 3.022521506732622\n",
      "              || Train_t || 3.024899204673497 || Val_t || 3.022521506732622\n",
      "step 20       || Train   || 2.910541702548877 || Val   || 2.903174425168292\n",
      "              || Train_t || 2.910541702548878 || Val_t || 2.903174425168292\n",
      "step 30       || Train   || 2.862459630906429 || Val   || 2.846881946272189\n",
      "              || Train_t || 2.862459630906429 || Val_t || 2.846881946272189\n",
      "step 40       || Train   || 2.788301334625511 || Val   || 2.806272830481473\n",
      "              || Train_t || 2.788301334625510 || Val_t || 2.806272830481473\n",
      "step 50       || Train   || 2.816853557882264 || Val   || 2.773501814770002\n",
      "              || Train_t || 2.816853557882264 || Val_t || 2.773501814770002\n",
      "step 60       || Train   || 2.715939249560932 || Val   || 2.740654505262065\n",
      "              || Train_t || 2.715939249560933 || Val_t || 2.740654505262065\n",
      "step 70       || Train   || 2.660503885145936 || Val   || 2.711401752196980\n",
      "              || Train_t || 2.660503885145936 || Val_t || 2.711401752196980\n",
      "step 80       || Train   || 2.723541077242827 || Val   || 2.684838960326417\n",
      "              || Train_t || 2.723541077242828 || Val_t || 2.684838960326417\n",
      "step 90       || Train   || 2.756579343442730 || Val   || 2.665377540141806\n",
      "              || Train_t || 2.756579343442730 || Val_t || 2.665377540141806\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(n_embd=8, n_embd2=24)\n",
    "# models\n",
    "torch.manual_seed(42)\n",
    "model_mlp = MLP(config)\n",
    "model_mlp_t = MLPtorch(config)\n",
    "# copy weights\n",
    "model_mlp_t.wte.weight.data = model_mlp.wte.weight.data.clone()\n",
    "for i, (p_t, p) in enumerate(zip(model_mlp_t.mlp.parameters(), model_mlp.mlp.parameters())):\n",
    "    if p.dim() == 2:\n",
    "        p_t.data = p.data.clone().T # linear layer weight\n",
    "    else:\n",
    "        p_t.data = p.data.clone()\n",
    "\n",
    "\n",
    "train_compare(model_mlp, model_mlp_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_embd2 = config.n_embd2\n",
    "        self.start = torch.zeros(1, config.n_embd2, dtype=config.dtype) # the starting hidden state\n",
    "        self.wte = Embedding(config.vocab_size, config.n_embd, dtype=config.dtype) # token embeddings table\n",
    "        self.Cw = Linear(config.n_embd + config.n_embd2, config.n_embd2, dtype=config.dtype) # rnn cell weight\n",
    "        self.lm_head = Linear(config.n_embd2, self.vocab_size, dtype=config.dtype)\n",
    "        num_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of rnn parameters: %d\" % (num_params,))\n",
    "        # grads\n",
    "        self.start_grad = None\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.start] + list(self.wte.parameters()) + list(self.Cw.parameters()) + list(self.lm_head.parameters())\n",
    "    \n",
    "    def grads(self):\n",
    "        return [self.start_grad] + list(self.wte.grads()) + list(self.Cw.grads()) + list(self.lm_head.grads())\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        b, t = x.size()\n",
    "        emb = self.wte(x) # (b, t, n_embd)\n",
    "        # sequentially iterate over the inputs and update the RNN state each tick\n",
    "        hprev = self.start.expand((b, -1)) # expand out the batch dimension\n",
    "        hiddens = []\n",
    "        emb_cat_hprevs = []\n",
    "        for i in range(t):\n",
    "            xt = emb[:, i, :] # (b, n_embd)\n",
    "            emb_i_cat_hprev = torch.cat([xt, hprev], dim=1)\n",
    "            # --- rnn cell ---\n",
    "            hi = self.Cw(emb_i_cat_hprev)\n",
    "            hi = hi.tanh()\n",
    "            # --------------\n",
    "            hprev = hi\n",
    "            hiddens.append(hi)\n",
    "            emb_cat_hprevs.append(emb_i_cat_hprev)\n",
    "        # decode the outputs\n",
    "        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n",
    "        logits = self.lm_head(hidden)\n",
    "        # backward buffer\n",
    "        self.hidden = hidden\n",
    "        self.emb_cat_hprevs = emb_cat_hprevs\n",
    "        return logits\n",
    "\n",
    "    def backward(self, grad):\n",
    "        hidden, emb_cat_hprevs = self.hidden, self.emb_cat_hprevs\n",
    "        t = hidden.size(1)\n",
    "        dhidden = self.lm_head.backward(grad)\n",
    "        # logits grad to start, wte, Cw grad\n",
    "        dembs = []\n",
    "        dCw, dhprev = 0., 0.\n",
    "        if self.Cw.bias is not None:\n",
    "            dCw_bias = 0.\n",
    "        for i in range(t-1, -1, -1):\n",
    "            # hidden state grad, emb grad\n",
    "            dhi = dhidden[:, i, :] + dhprev # grad from logits + grad from prev hidden state\n",
    "            hi = hidden[:, i, :]\n",
    "            dhi = (1 - hi**2) * dhi # grad of tanh\n",
    "            demb_i_cat_dhi = dhi @ self.Cw.weight.T\n",
    "            demb_i, dhprev = demb_i_cat_dhi.tensor_split([self.n_embd,], dim=1)\n",
    "            dembs.append(demb_i)\n",
    "            # cell weight grad\n",
    "            emb_i_cat_hprev = emb_cat_hprevs[i]\n",
    "            dCw += emb_i_cat_hprev.T @ dhi\n",
    "            if self.Cw.bias is not None:\n",
    "                dCw_bias += dhi.sum(dim=0)\n",
    "        dstart = dhprev.sum(dim=0, keepdim=True)\n",
    "        demb = torch.stack(dembs[::-1], 1)\n",
    "        self.wte.backward(demb)\n",
    "        self.start_grad = dstart\n",
    "        self.Cw.weight_grad = dCw\n",
    "        if self.Cw.bias is not None:\n",
    "            self.Cw.bias_grad = dCw_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNtorch(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_embd2 = config.n_embd2\n",
    "        self.start = nn.Parameter(torch.zeros(1, config.n_embd2)) # the starting hidden state\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd) # token embeddings table\n",
    "        self.Cw = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "        self.lm_head = nn.Linear(config.n_embd2, self.vocab_size)\n",
    "        num_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of rnn parameters: %d\" % (num_params,))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx):\n",
    "        b, t = idx.size()\n",
    "\n",
    "        # embed all the integers up front and all at once for efficiency\n",
    "        emb = self.wte(idx) # (b, t, n_embd)\n",
    "\n",
    "        # sequentially iterate over the inputs and update the RNN state each tick\n",
    "        hprev = self.start.T.expand((b, -1)) # expand out the batch dimension\n",
    "        hiddens = []\n",
    "        for i in range(t):\n",
    "            xt = emb[:, i, :] # (b, n_embd)\n",
    "            xh = torch.cat([xt, hprev], dim=1)\n",
    "            ht = (self.Cw(xh)).tanh()\n",
    "            hprev = ht\n",
    "            hiddens.append(ht)\n",
    "\n",
    "        # decode the outputs\n",
    "        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n",
    "        logits = self.lm_head(hidden)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rnn parameters: 3807\n",
      "number of rnn parameters: 3807\n",
      "step 0        || Train   || 3.362432962612295 || Val   || 3.329043391170546\n",
      "              || Train_t || 3.362432962612295 || Val_t || 3.329043391170547\n",
      "step 10       || Train   || 3.171036656791207 || Val   || 3.114081619871966\n",
      "              || Train_t || 3.171036656791206 || Val_t || 3.114081619871965\n",
      "step 20       || Train   || 2.921689018592812 || Val   || 2.918088278837589\n",
      "              || Train_t || 2.921689018592812 || Val_t || 2.918088278837589\n",
      "step 30       || Train   || 2.807653806375523 || Val   || 2.797327611978270\n",
      "              || Train_t || 2.807653806375522 || Val_t || 2.797327611978270\n",
      "step 40       || Train   || 2.707653574650008 || Val   || 2.711660180532699\n",
      "              || Train_t || 2.707653574650008 || Val_t || 2.711660180532699\n",
      "step 50       || Train   || 2.712063772179941 || Val   || 2.660885418532094\n",
      "              || Train_t || 2.712063772179941 || Val_t || 2.660885418532094\n",
      "step 60       || Train   || 2.629152563651833 || Val   || 2.624115333524215\n",
      "              || Train_t || 2.629152563651833 || Val_t || 2.624115333524215\n",
      "step 70       || Train   || 2.583538394485787 || Val   || 2.595689170497225\n",
      "              || Train_t || 2.583538394485787 || Val_t || 2.595689170497226\n",
      "step 80       || Train   || 2.620622058670813 || Val   || 2.579475046084394\n",
      "              || Train_t || 2.620622058670813 || Val_t || 2.579475046084394\n",
      "step 90       || Train   || 2.612400860524740 || Val   || 2.563765586153120\n",
      "              || Train_t || 2.612400860524739 || Val_t || 2.563765586153121\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(n_embd=8, n_embd2=44)\n",
    "# models\n",
    "torch.manual_seed(42)\n",
    "model_rnn = RNN(config)\n",
    "model_rnn_t = RNNtorch(config)\n",
    "# copy weights\n",
    "model_rnn_t.wte.weight.data = model_rnn.wte.weight.data.clone()\n",
    "for i, (p_t, p) in enumerate(zip(model_rnn_t.parameters(), model_rnn.parameters())):\n",
    "    if p.dim() == 2 and i != 1: # skip the embedding layer\n",
    "        p_t.data = p.data.clone().T # linear layer weight\n",
    "    else:\n",
    "        p_t.data = p.data.clone()\n",
    "\n",
    "train_compare(model_rnn, model_rnn_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_embd2 = config.n_embd2\n",
    "        self.start = torch.zeros(1, config.n_embd2, dtype=config.dtype) # the starting hidden state\n",
    "        self.wte = Embedding(config.vocab_size, config.n_embd, dtype=config.dtype) # token embeddings table\n",
    "        self.Cr = Linear(config.n_embd + config.n_embd2, config.n_embd2, dtype=config.dtype)\n",
    "        self.Cbar = Linear(config.n_embd + config.n_embd2, config.n_embd2, dtype=config.dtype)\n",
    "        self.Cz = Linear(config.n_embd + config.n_embd2, config.n_embd2, dtype=config.dtype)\n",
    "        self.lm_head = Linear(config.n_embd2, self.vocab_size, dtype=config.dtype)\n",
    "        num_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of gru parameters: %d\" % (num_params,))\n",
    "        # grads\n",
    "        self.start_grad = None\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.start] + list(self.wte.parameters()) + list(self.Cr.parameters()) + list(self.Cbar.parameters()) + list(self.Cz.parameters()) + list(self.lm_head.parameters())\n",
    "    \n",
    "    def grads(self):\n",
    "        return [self.start_grad] + list(self.wte.grads()) + list(self.Cr.grads()) + list(self.Cbar.grads()) + list(self.Cz.grads()) + list(self.lm_head.grads())\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        b, t = x.size()\n",
    "        emb = self.wte(x) # (b, t, n_embd)\n",
    "        # sequentially iterate over the inputs and update the RNN state each tick\n",
    "        hprev = self.start.expand((b, -1)) # expand out the batch dimension\n",
    "        hiddens = []\n",
    "        emb_cat_hprevs, emb_cat_hprev_resets, hprevs, hbars, zs, rs = [], [], [], [], [], []\n",
    "        for i in range(t):\n",
    "            emb_i = emb[:, i, :] # (b, n_embd)\n",
    "            # --- gru cell ---\n",
    "            emb_i_cat_hprev = torch.cat([emb_i, hprev], dim=1)\n",
    "            ri = self.Cr(emb_i_cat_hprev)\n",
    "            ri = ri.sigmoid()\n",
    "            hprev_reset = ri * hprev\n",
    "            emb_i_cat_hprev_reset = torch.cat([emb_i, hprev_reset], dim=1)\n",
    "            hbar = self.Cbar(emb_i_cat_hprev_reset)\n",
    "            hbar = hbar.tanh()\n",
    "            zi = self.Cz(emb_i_cat_hprev)\n",
    "            zi = zi.sigmoid()\n",
    "            hi = (1 - zi) * hprev + zi * hbar\n",
    "            # backward buffer\n",
    "            hiddens.append(hi)\n",
    "            emb_cat_hprevs.append(emb_i_cat_hprev)\n",
    "            emb_cat_hprev_resets.append(emb_i_cat_hprev_reset)\n",
    "            hprevs.append(hprev)\n",
    "            hbars.append(hbar)\n",
    "            zs.append(zi)\n",
    "            rs.append(ri)\n",
    "            # update hprev\n",
    "            hprev = hi\n",
    "        # decode the outputs\n",
    "        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n",
    "        logits = self.lm_head(hidden)\n",
    "        # backward buffer\n",
    "        self.hidden = hidden\n",
    "        self.emb_cat_hprevs = emb_cat_hprevs\n",
    "        self.emb_cat_hprev_resets = emb_cat_hprev_resets\n",
    "        self.hprevs = hprevs\n",
    "        self.hbars = hbars\n",
    "        self.zs = zs\n",
    "        self.rs = rs\n",
    "        return logits\n",
    "\n",
    "    def backward(self, dlogits):\n",
    "        hidden, emb_cat_hprevs, emb_cat_hprev_resets, hprevs, hbars, zs, rs = self.hidden, self.emb_cat_hprevs, self.emb_cat_hprev_resets, self.hprevs, self.hbars, self.zs, self.rs\n",
    "        t = hidden.size(1)\n",
    "        dhidden = self.lm_head.backward(dlogits)\n",
    "        # logits grad to start, wte, Cw grad\n",
    "        dembs = []\n",
    "        dCr, dCbar, dCz, dhprev = 0., 0., 0., 0.\n",
    "        if self.Cr.bias is not None:\n",
    "            dCr_bias = 0.\n",
    "        if self.Cbar.bias is not None:\n",
    "            dCbar_bias = 0.\n",
    "        if self.Cz.bias is not None:\n",
    "            dCz_bias = 0.\n",
    "        for i in range(t-1, -1, -1):\n",
    "            # hidden state grad, emb grad\n",
    "            dhi = dhidden[:, i, :] + dhprev # grad from logits + grad from prev hidden state\n",
    "            dhbar = dhi * zs[i]\n",
    "            dhprev = dhi * (1 - zs[i])\n",
    "            dzi = dhi * (hbars[i] - hprevs[i])\n",
    "            dzi = dzi * (1 - zs[i]) * zs[i]\n",
    "            demb_i_cat_hprev = dzi @ self.Cz.weight.T\n",
    "            dCz += emb_cat_hprevs[i].T @ dzi\n",
    "            if self.Cz.bias is not None:\n",
    "                dCz_bias += dzi.sum(dim=0)\n",
    "\n",
    "            dhbar = dhbar * (1 - hbars[i]**2)\n",
    "            demb_i_cat_hprev_reset = dhbar @ self.Cbar.weight.T\n",
    "            dCbar += emb_cat_hprev_resets[i].T @ dhbar\n",
    "            if self.Cbar.bias is not None:\n",
    "                dCbar_bias += dhbar.sum(dim=0)\n",
    "\n",
    "            demb_i, dhprev_reset = demb_i_cat_hprev_reset.tensor_split([self.n_embd,], dim=1)\n",
    "            dri = dhprev_reset * hprevs[i]\n",
    "            dhprev += dhprev_reset * rs[i]\n",
    "            dri = dri * (1 - rs[i]) * rs[i]\n",
    "            demb_i_cat_hprev += dri @ self.Cr.weight.T\n",
    "            dCr += emb_cat_hprevs[i].T @ dri\n",
    "            if self.Cr.bias is not None:\n",
    "                dCr_bias += dri.sum(dim=0)\n",
    "            demb_more, dhprev_more = demb_i_cat_hprev.tensor_split([self.n_embd,], dim=1)\n",
    "            demb_i += demb_more\n",
    "            dhprev += dhprev_more\n",
    "            dembs.append(demb_i)\n",
    "        dstart = dhprev.sum(dim=0, keepdim=True)\n",
    "        demb = torch.stack(dembs[::-1], 1)\n",
    "        self.wte.backward(demb)\n",
    "        self.start_grad = dstart\n",
    "        self.Cr.weight_grad = dCr\n",
    "        if self.Cr.bias is not None:\n",
    "            self.Cr.bias_grad = dCr_bias\n",
    "        self.Cbar.weight_grad = dCbar\n",
    "        if self.Cbar.bias is not None:\n",
    "            self.Cbar.bias_grad = dCbar_bias\n",
    "        self.Cz.weight_grad = dCz\n",
    "        if self.Cz.bias is not None:\n",
    "            self.Cz.bias_grad = dCz_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUtorch(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.start = nn.Parameter(torch.zeros(1, config.n_embd2, dtype=config.dtype)) # the starting hidden state\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd) # token embeddings table\n",
    "        self.Cr = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "        self.Cbar = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "        self.Cz = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "        self.lm_head = nn.Linear(config.n_embd2, self.vocab_size)\n",
    "        num_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of gru parameters: %d\" % (num_params,))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx):\n",
    "        b, t = idx.size()\n",
    "        emb = self.wte(idx) # (b, t, n_embd)\n",
    "        hprev = self.start.T.expand((b, -1)) # expand out the batch dimension\n",
    "        hiddens = []\n",
    "        for i in range(t):\n",
    "            xt = emb[:, i, :] # (b, n_embd)\n",
    "            # --- gru cell ---\n",
    "            xh = torch.cat([xt, hprev], dim=1)\n",
    "            r = (self.Cr(xh)).sigmoid()\n",
    "            hprev_reset = r * hprev\n",
    "            xhr = torch.cat([xt, hprev_reset], dim=1)\n",
    "            hbar = (self.Cbar(xhr)).tanh()\n",
    "            z = (self.Cz(xh)).sigmoid()\n",
    "            z = (self.Cz(xh)).sigmoid()\n",
    "            ht = (1 - z) * hprev + z * hbar\n",
    "            # --------------\n",
    "            hprev = ht\n",
    "            hiddens.append(ht)\n",
    "\n",
    "        # decode the outputs\n",
    "        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n",
    "        logits = self.lm_head(hidden)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of gru parameters: 3915\n",
      "number of gru parameters: 3915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0        || Train   || 3.361403145058750 || Val   || 3.352860787255950\n",
      "              || Train_t || 3.361403145058750 || Val_t || 3.352860787255950\n",
      "step 10       || Train   || 3.269461320582565 || Val   || 3.251121526050696\n",
      "              || Train_t || 3.269461320582565 || Val_t || 3.251121526050696\n",
      "step 20       || Train   || 3.178847946129618 || Val   || 3.166952995381364\n",
      "              || Train_t || 3.178847946129618 || Val_t || 3.166952995381364\n",
      "step 30       || Train   || 3.107527022687791 || Val   || 3.088581327911588\n",
      "              || Train_t || 3.107527022687791 || Val_t || 3.088581327911588\n",
      "step 40       || Train   || 3.039824486713404 || Val   || 3.015034751629193\n",
      "              || Train_t || 3.039824486713405 || Val_t || 3.015034751629194\n",
      "step 50       || Train   || 2.988462129260083 || Val   || 2.953215323540391\n",
      "              || Train_t || 2.988462129260084 || Val_t || 2.953215323540391\n",
      "step 60       || Train   || 2.931377000481202 || Val   || 2.901129118936024\n",
      "              || Train_t || 2.931377000481201 || Val_t || 2.901129118936024\n",
      "step 70       || Train   || 2.843199489923497 || Val   || 2.853368275902411\n",
      "              || Train_t || 2.843199489923497 || Val_t || 2.853368275902412\n",
      "step 80       || Train   || 2.841994397868837 || Val   || 2.815843328564174\n",
      "              || Train_t || 2.841994397868836 || Val_t || 2.815843328564174\n",
      "step 90       || Train   || 2.817380190868578 || Val   || 2.785673505789270\n",
      "              || Train_t || 2.817380190868579 || Val_t || 2.785673505789270\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(n_embd=8, n_embd2=27)\n",
    "# models\n",
    "torch.manual_seed(42)\n",
    "model_gru = GRU(config)\n",
    "model_gru_t = GRUtorch(config)\n",
    "# copy weights\n",
    "model_gru_t.wte.weight.data = model_gru.wte.weight.data.clone()\n",
    "for i, (p_t, p) in enumerate(zip(model_gru_t.parameters(), model_gru.parameters())):\n",
    "    if p.dim() == 2 and i != 1: # skip the embedding layer\n",
    "        p_t.data = p.data.clone().T # linear layer weight\n",
    "    else:\n",
    "        p_t.data = p.data.clone()\n",
    "\n",
    "train_compare(model_gru, model_gru_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = Linear(config.n_embd, 3 * config.n_embd, dtype=config.dtype)\n",
    "        # output projection\n",
    "        self.c_proj = Linear(config.n_embd, config.n_embd, dtype=config.dtype)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.bias = torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "    \n",
    "    def parameters(self):\n",
    "        return list(self.c_attn.parameters()) + list(self.c_proj.parameters())\n",
    "    \n",
    "    def grads(self):\n",
    "        return list(self.c_attn.grads()) + list(self.c_proj.grads())\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k ,v  = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        att_dot = (q @ k.transpose(-2, -1)) * (1.0 / k.size(-1)**0.5)\n",
    "        att_mask = att_dot.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att_mask, dim=-1)\n",
    "        y_trans = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y_preproj = y_trans.transpose(1, 2).contiguous().view(B, T, C) # (B, nh, T, hs) -> (B, T, nh*hs)\n",
    "        y = self.c_proj(y_preproj) # (B, T, C) -> (B, T, C)\n",
    "        # backward buffer\n",
    "        self.y_preproj = y_preproj\n",
    "        self.att = att\n",
    "        self.q = q\n",
    "        self.k = k\n",
    "        self.v = v\n",
    "        self.x = x\n",
    "        return y\n",
    "\n",
    "    def backward(self, dy):\n",
    "        B, T, C = dy.size()\n",
    "        (y_preproj, att, q, k, v, x) = (\n",
    "            self.y_preproj, self.att, self.q, self.k, self.v, self.x\n",
    "        )\n",
    "        dC_proj = (y_preproj.transpose(-2, -1) @ dy).sum(dim=0)\n",
    "        self.c_proj.weight_grad = dC_proj\n",
    "        if self.c_proj.bias is not None:\n",
    "            dC_proj_bias = dy.sum(dim=[0, 1])\n",
    "            self.c_proj.bias_grad = dC_proj_bias\n",
    "        dy_preproj = dy @ self.c_proj.weight.T\n",
    "        dy_trans = dy_preproj.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        datt = dy_trans @ v.transpose(-2, -1)\n",
    "        dv = att.transpose(-2, -1) @ dy_trans\n",
    "        datt_mask = att * (datt - (att * datt).sum(dim=-1, keepdim=True))\n",
    "        datt_dot = datt_mask.masked_fill(self.bias[:,:,:T,:T] == 0, 0)\n",
    "        dq = (datt_dot @ k) * (1.0 / k.size(-1)**0.5)\n",
    "        dk = (datt_dot.transpose(-2, -1) @ q) * (1.0 / k.size(-1)**0.5)\n",
    "        dq = dq.transpose(1, 2).reshape(B, T, C)\n",
    "        dk = dk.transpose(1, 2).reshape(B, T, C)\n",
    "        dv = dv.transpose(1, 2).reshape(B, T, C)\n",
    "        dqkv = torch.cat([dq, dk, dv], dim=2)\n",
    "        dC_atten = (x.transpose(-2, -1) @ dqkv).sum(dim=0)\n",
    "        self.c_attn.weight_grad = dC_atten\n",
    "        if self.c_attn.bias is not None:\n",
    "            dC_atten_bias = dqkv.sum(dim=[0, 1])\n",
    "            self.c_attn.bias_grad = dC_atten_bias\n",
    "        dx = dqkv @ self.c_attn.weight.T\n",
    "        return dx\n",
    "\n",
    "class Block(Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, dtype=config.dtype)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, dtype=config.dtype)\n",
    "        self.mlp = Sequential([\n",
    "            Linear(config.n_embd, 4 * config.n_embd, dtype=config.dtype),\n",
    "            GELU(),\n",
    "            Linear(4 * config.n_embd, config.n_embd, dtype=config.dtype),\n",
    "        ])\n",
    "    \n",
    "    def parameters(self):\n",
    "        return list(self.ln_1.parameters()) + list(self.attn.parameters()) + list(self.ln_2.parameters()) + list(self.mlp.parameters())\n",
    "    \n",
    "    def grads(self):\n",
    "        return list(self.ln_1.grads()) + list(self.attn.grads()) + list(self.ln_2.grads()) + list(self.mlp.grads())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dx):\n",
    "        dx = dx + self.ln_2.backward(self.mlp.backward(dx))\n",
    "        dx = dx + self.ln_1.backward(self.attn.backward(dx))\n",
    "        return dx\n",
    "\n",
    "class Transformer(Module):\n",
    "    \"\"\" Transformer Language Model, exactly as seen in GPT-2 \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.wte = Embedding(config.vocab_size, config.n_embd, dtype=config.dtype)\n",
    "        self.wpe = Embedding(config.block_size, config.n_embd, dtype=config.dtype)\n",
    "        self.transformer = Sequential([Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = LayerNorm(config.n_embd, dtype=config.dtype)\n",
    "        self.lm_head = Linear(config.n_embd, config.vocab_size, bias=False, dtype=config.dtype)\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"number of parameters: {n_params}\")\n",
    "    \n",
    "    def parameters(self):\n",
    "        return list(self.wte.parameters()) + list(self.wpe.parameters()) + list(self.transformer.parameters()) + list(self.ln_f.parameters()) + list(self.lm_head.parameters())\n",
    "    \n",
    "    def grads(self):\n",
    "        return list(self.wte.grads()) + list(self.wpe.grads()) + list(self.transformer.grads()) + list(self.ln_f.grads()) + list(self.lm_head.grads())\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer:\n",
    "            x = block(x)\n",
    "        h = self.ln_f(x)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def backward(self, dlogits):\n",
    "        dh = self.lm_head.backward(dlogits)\n",
    "        dh = self.ln_f.backward(dh)\n",
    "        dx = self.transformer.backward(dh)\n",
    "        dtok_emb = dx\n",
    "        self.wte.backward(dtok_emb)\n",
    "        dpos_emb = dx.sum(dim=0, keepdim=True)\n",
    "        self.wpe.backward(dpos_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class GELUTorch(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class CausalSelfAttentionTorch(nn.Module):\n",
    "    \"\"\"\n",
    "    Copied from https://github.com/karpathy/makemore/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, dtype=config.dtype)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, dtype=config.dtype)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class BlockTorch(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd, dtype=config.dtype)\n",
    "        self.attn = CausalSelfAttentionTorch(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd, dtype=config.dtype)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd, dtype=config.dtype),\n",
    "            GELUTorch(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd, dtype=config.dtype),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class TransformerTorch(nn.Module):\n",
    "    \"\"\" Transformer Language Model, exactly as seen in GPT-2 \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd, dtype=config.dtype),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd, dtype=config.dtype),\n",
    "            h = nn.ModuleList([BlockTorch(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd, dtype=config.dtype),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False, dtype=config.dtype)\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"number of parameters: {n_params}\")\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 3975\n",
      "number of parameters: 3975\n",
      "step 0        || Train   || 3.869099258476809 || Val   || 3.712109911891141\n",
      "              || Train_t || 3.869099258476810 || Val_t || 3.712109911891141\n",
      "step 10       || Train   || 3.298443973910026 || Val   || 3.304099664159999\n",
      "              || Train_t || 3.298661733220529 || Val_t || 3.304245030693154\n",
      "step 20       || Train   || 3.126676118215248 || Val   || 3.075409886092623\n",
      "              || Train_t || 3.126410738807248 || Val_t || 3.075239366439925\n",
      "step 30       || Train   || 2.928018246734532 || Val   || 2.938479257639128\n",
      "              || Train_t || 2.927690023165900 || Val_t || 2.937990841325048\n",
      "step 40       || Train   || 2.828903650949786 || Val   || 2.854066956549762\n",
      "              || Train_t || 2.828983281779418 || Val_t || 2.853591825587482\n",
      "step 50       || Train   || 2.884480142463558 || Val   || 2.800499488799328\n",
      "              || Train_t || 2.883325131417171 || Val_t || 2.799928233831779\n",
      "step 60       || Train   || 2.753418774851199 || Val   || 2.765830346190955\n",
      "              || Train_t || 2.752558935486662 || Val_t || 2.765219652366059\n",
      "step 70       || Train   || 2.673139121436039 || Val   || 2.740691202603831\n",
      "              || Train_t || 2.671998241079754 || Val_t || 2.739982684772813\n",
      "step 80       || Train   || 2.760551234935577 || Val   || 2.721588761234763\n",
      "              || Train_t || 2.759734243999123 || Val_t || 2.720839328759217\n",
      "step 90       || Train   || 2.780131306423743 || Val   || 2.706279791718127\n",
      "              || Train_t || 2.780185030928990 || Val_t || 2.705494523582000\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(n_embd=15, n_embd2=None, n_head=1, n_layer=1)\n",
    "# models\n",
    "torch.manual_seed(42)\n",
    "model = Transformer(config)\n",
    "model_t = TransformerTorch(config)\n",
    "# copy weights\n",
    "for i, (p, p_t) in enumerate(zip(model.parameters(), model_t.parameters())):\n",
    "    if p_t.ndim == 2:\n",
    "        if i >= 2:\n",
    "            p_t.data = p.data.clone().T\n",
    "        else:\n",
    "            p_t.data = p.data.clone()\n",
    "    else:\n",
    "        p_t.data = p.data.clone()\n",
    "\n",
    "train_compare(model, model_t, n_steps=100, eval_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
