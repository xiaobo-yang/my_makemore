{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".aukai.......... -> aukai.\n",
      ".ellanore....... -> ellanore.\n",
      ".liem........... -> liem.\n",
      ".aquarius....... -> aquarius.\n",
      ".joangel........ -> joangel.\n",
      ".wryn........... -> wryn.\n",
      ".isabela........ -> isabela.\n",
      ".astryd......... -> astryd.\n",
      ".maleik......... -> maleik.\n",
      ".emerick........ -> emerick.\n",
      ".natasha........ -> natasha.\n",
      ".kasandra....... -> kasandra.\n",
      ".aevin.......... -> aevin.\n",
      ".brason......... -> brason.\n",
      ".naiara......... -> naiara.\n",
      ".alanna......... -> alanna.\n",
      ".raunak......... -> raunak.\n",
      ".gohan.......... -> gohan.\n",
      ".ivie........... -> ivie.\n",
      ".alandis........ -> alandis.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([23595, 16]),\n",
       " torch.Size([2949, 16]),\n",
       " torch.Size([2950, 16]),\n",
       " torch.Size([23595, 16]),\n",
       " torch.Size([2949, 16]),\n",
       " torch.Size([2950, 16]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words = sorted(list(set(words))) # set cause uncontrollable randomnessï¼Œ sorted for reproducibility\n",
    "max_len = max(len(w) for w in words)\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "chs = list(set(''.join(words + ['.']))) # add special token\n",
    "chs = sorted(chs, reverse=False)\n",
    "stoi = {ch: i for i, ch in enumerate(chs)}\n",
    "itos = {i: ch for i, ch in enumerate(chs)}\n",
    "\n",
    "# predict next token use previous tokens\n",
    "vocab_size = len(chs)\n",
    "block_size = max_len + 1\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    x = torch.zeros(max_len + 1, dtype=torch.long)\n",
    "    y = torch.zeros(max_len + 1, dtype=torch.long)\n",
    "    x[1:1+len(w)] = torch.tensor([stoi[ch] for ch in w])\n",
    "    y[:len(w)] = torch.tensor([stoi[ch] for ch in w])\n",
    "    y[len(w)+1:] = -1 # mask the loss at the inactive locations\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "\n",
    "X = torch.stack(X)\n",
    "Y = torch.stack(Y)\n",
    "n1, n2  = int(0.8 * len(X)), int(0.9 * len(X))\n",
    "\n",
    "X_train, X_val, X_test = X.tensor_split([n1, n2])\n",
    "Y_train, Y_val, Y_test = Y.tensor_split([n1, n2])\n",
    "\n",
    "show = 20\n",
    "for x, y in zip(X_train[:show], Y_train[:show]):\n",
    "    sx = ''.join(itos[i.item()] for i in x)\n",
    "    sy = ''.join(itos[i.item()] for i in y if i.item() != -1)\n",
    "    print(f'{sx} -> {sy}')\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# move to manual backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# manual backprop\n",
    "from tiny_torch import *\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    block_size: int = block_size # length of the input sequences of integers\n",
    "    vocab_size: int = vocab_size # the input integers are in range [0 .. vocab_size -1]\n",
    "    # parameters below control the sizes of each model slightly differently\n",
    "    n_layer: int = 4\n",
    "    n_embd: int = 64\n",
    "    n_embd2: int = 64\n",
    "    n_head: int = 4\n",
    "    dtype: torch.dtype = torch.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_compare(model, model_t, lr=0.1, n_steps=100, eval_every=10, bs=32):\n",
    "    loss_fn = CrossEntropyLoss3d()\n",
    "    # train\n",
    "    torch.manual_seed(42)\n",
    "    for  step in range(n_steps):\n",
    "        idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "        x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "        # --- torch ---\n",
    "        # forward\n",
    "        logits_t = model_t(x)\n",
    "        loss_t = F.cross_entropy(logits_t.view(-1, logits_t.size(-1)), y.view(-1), ignore_index=-1)\n",
    "        # backward\n",
    "        loss_t.backward()\n",
    "        # update\n",
    "        for p_t in model_t.parameters():\n",
    "            p_t.data -= lr * p_t.grad\n",
    "            p_t.grad = None\n",
    "        # --- manual ---\n",
    "        # forward\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        # backward\n",
    "        dlogits = loss_fn.backward()\n",
    "        model.backward(dlogits)\n",
    "        # update\n",
    "        for p, g in zip(model.parameters(), model.grads()):\n",
    "            p.data -= lr * g\n",
    "        \n",
    "        \n",
    "        # eval\n",
    "        if step % eval_every == 0:\n",
    "            x, y = X_val, Y_val\n",
    "            with torch.no_grad():\n",
    "                logits_t = model_t(x)\n",
    "                val_loss_t = F.cross_entropy(logits_t.view(-1, logits_t.size(-1)), y.view(-1), ignore_index=-1)\n",
    "            logits = model(x)\n",
    "            val_loss = loss_fn(logits, y)\n",
    "            print(f'step {step:<8} || Train   || {loss.item():.15f} || Val   || {val_loss.item():.15f}')\n",
    "            print(f'              || Train_t || {loss_t.item():.15f} || Val_t || {val_loss_t.item():.15f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    \"\"\"\n",
    "    Adapted from https://github.com/karpathy/makemore\n",
    "\n",
    "    Reference:\n",
    "    Bengio et al. 2003 https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.wte = Embedding(config.vocab_size + 1, config.n_embd, dtype=config.dtype) # token embeddings table\n",
    "        # +1 in the line above for a special <BLANK> token that gets inserted if encoding a token\n",
    "        # before the beginning of the input sequence\n",
    "        self.mlp = Sequential([\n",
    "            Linear(self.block_size * config.n_embd, config.n_embd2, dtype=config.dtype),\n",
    "            Tanh(),\n",
    "            Linear(config.n_embd2, self.vocab_size, dtype=config.dtype)\n",
    "        ])\n",
    "        self.mlp[-1].weight.data *= 0.1\n",
    "        self.mlp[-1].bias.data *= 0.01\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of mlp parameters: %d\" % (n_params,))\n",
    "        self.config = config\n",
    "    \n",
    "    def parameters(self):\n",
    "        return list(self.wte.parameters()) + list(self.mlp.parameters())\n",
    "    \n",
    "    def grads(self):\n",
    "        return list(self.wte.grads()) + list(self.mlp.grads())\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def __call__(self, idx, targets=None):\n",
    "\n",
    "        # gather the word embeddings of the previous 3 words\n",
    "        idx_buf = []\n",
    "        embs = []\n",
    "        for k in range(self.block_size):\n",
    "            tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "            idx_buf.append(idx.unsqueeze(-1))\n",
    "            embs.append(tok_emb)\n",
    "            idx = torch.roll(idx, 1, 1)\n",
    "            idx[:, 0] = self.vocab_size # special <BLANK> token\n",
    "\n",
    "        # concat all of the embeddings together and pass through an MLP\n",
    "        x = torch.cat(embs, -1) # (b, t, n_embd * block_size)\n",
    "        logits = self.mlp(x)\n",
    "\n",
    "        # backward buffer\n",
    "        self.idx_buf = torch.cat(idx_buf, -1) # (b, t, t)\n",
    "\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        grad = self.mlp.backward(grad)\n",
    "        # mlp backprop to wte\n",
    "        b, t, _ = grad.shape # (b, t, n_embd * block_size)\n",
    "        grad = grad.view(b * t * self.config.block_size, self.config.n_embd) # (b*t*block_size, n_embd)\n",
    "        wte_weight = self.wte.weight\n",
    "        wte_grad = torch.zeros_like(wte_weight)\n",
    "        wte_grad.index_add_(dim=0, index=self.idx_buf.view(-1), source=grad)\n",
    "        self.wte.weight_grad = wte_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPtorch(nn.Module):\n",
    "    \"\"\"\n",
    "    takes the previous block_size tokens, encodes them with a lookup table,\n",
    "    concatenates the vectors and predicts the next token with an MLP.\n",
    "\n",
    "    Reference:\n",
    "    Bengio et al. 2003 https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.wte = nn.Embedding(config.vocab_size + 1, config.n_embd) # token embeddings table\n",
    "        # +1 in the line above for a special <BLANK> token that gets inserted if encoding a token\n",
    "        # before the beginning of the input sequence\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.block_size * config.n_embd, config.n_embd2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(config.n_embd2, self.vocab_size)\n",
    "        )\n",
    "        self.mlp[-1].weight.data *= 0.1\n",
    "        self.mlp[-1].bias.data *= 0.01\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of mlp parameters: %d\" % (n_params,))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # gather the word embeddings of the previous 3 words\n",
    "        embs = []\n",
    "        for k in range(self.block_size):\n",
    "            tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "            idx = torch.roll(idx, 1, 1)\n",
    "            idx[:, 0] = self.vocab_size # special <BLANK> token\n",
    "            embs.append(tok_emb)\n",
    "\n",
    "        # concat all of the embeddings together and pass through an MLP\n",
    "        x = torch.cat(embs, -1) # (b, t, n_embd * block_size)\n",
    "        logits = self.mlp(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mlp parameters: 3995\n",
      "number of mlp parameters: 3995\n",
      "step 0        || Train   || 3.297640663881704 || Val   || 3.267504153916392\n",
      "              || Train_t || 3.297640663881704 || Val_t || 3.267504153916391\n",
      "step 10       || Train   || 3.024899204673497 || Val   || 3.022521506732622\n",
      "              || Train_t || 3.024899204673497 || Val_t || 3.022521506732622\n",
      "step 20       || Train   || 2.910541702548877 || Val   || 2.903174425168292\n",
      "              || Train_t || 2.910541702548878 || Val_t || 2.903174425168292\n",
      "step 30       || Train   || 2.862459630906429 || Val   || 2.846881946272189\n",
      "              || Train_t || 2.862459630906429 || Val_t || 2.846881946272189\n",
      "step 40       || Train   || 2.788301334625511 || Val   || 2.806272830481473\n",
      "              || Train_t || 2.788301334625510 || Val_t || 2.806272830481473\n",
      "step 50       || Train   || 2.816853557882264 || Val   || 2.773501814770002\n",
      "              || Train_t || 2.816853557882264 || Val_t || 2.773501814770002\n",
      "step 60       || Train   || 2.715939249560932 || Val   || 2.740654505262065\n",
      "              || Train_t || 2.715939249560933 || Val_t || 2.740654505262065\n",
      "step 70       || Train   || 2.660503885145936 || Val   || 2.711401752196980\n",
      "              || Train_t || 2.660503885145936 || Val_t || 2.711401752196980\n",
      "step 80       || Train   || 2.723541077242827 || Val   || 2.684838960326417\n",
      "              || Train_t || 2.723541077242828 || Val_t || 2.684838960326417\n",
      "step 90       || Train   || 2.756579343442730 || Val   || 2.665377540141806\n",
      "              || Train_t || 2.756579343442730 || Val_t || 2.665377540141806\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(n_embd=8, n_embd2=24)\n",
    "# models\n",
    "torch.manual_seed(42)\n",
    "model_mlp = MLP(config)\n",
    "model_mlp_t = MLPtorch(config)\n",
    "# copy weights\n",
    "model_mlp_t.wte.weight.data = model_mlp.wte.weight.data.clone()\n",
    "for i, (p_t, p) in enumerate(zip(model_mlp_t.mlp.parameters(), model_mlp.mlp.parameters())):\n",
    "    if p.dim() == 2:\n",
    "        p_t.data = p.data.clone().T # linear layer weight\n",
    "    else:\n",
    "        p_t.data = p.data.clone()\n",
    "\n",
    "\n",
    "train_compare(model_mlp, model_mlp_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_embd2 = config.n_embd2\n",
    "        self.start = torch.zeros(1, config.n_embd2, dtype=config.dtype) # the starting hidden state\n",
    "        self.wte = Embedding(config.vocab_size, config.n_embd, dtype=config.dtype) # token embeddings table\n",
    "        self.Cw = Linear(config.n_embd + config.n_embd2, config.n_embd2, dtype=config.dtype) # rnn cell weight\n",
    "        self.lm_head = Linear(config.n_embd2, self.vocab_size, dtype=config.dtype)\n",
    "        self.lm_head.weight.data *= 0.1\n",
    "        num_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of rnn parameters: %d\" % (num_params,))\n",
    "        # grads\n",
    "        self.start_grad = None\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.start] + list(self.wte.parameters()) + list(self.Cw.parameters()) + list(self.lm_head.parameters())\n",
    "    \n",
    "    def grads(self):\n",
    "        return [self.start_grad] + list(self.wte.grads()) + list(self.Cw.grads()) + list(self.lm_head.grads())\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        b, t = x.size()\n",
    "        emb = self.wte(x) # (b, t, n_embd)\n",
    "        # sequentially iterate over the inputs and update the RNN state each tick\n",
    "        hprev = self.start.expand((b, -1)) # expand out the batch dimension\n",
    "        hiddens = []\n",
    "        emb_cat_hprevs = []\n",
    "        for i in range(t):\n",
    "            xt = emb[:, i, :] # (b, n_embd)\n",
    "            emb_i_cat_hprev = torch.cat([xt, hprev], dim=1)\n",
    "            # --- rnn cell ---\n",
    "            hi = self.Cw(emb_i_cat_hprev)\n",
    "            hi = hi.tanh()\n",
    "            # --------------\n",
    "            hprev = hi\n",
    "            hiddens.append(hi)\n",
    "            emb_cat_hprevs.append(emb_i_cat_hprev)\n",
    "        # decode the outputs\n",
    "        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n",
    "        logits = self.lm_head(hidden)\n",
    "        # backward buffer\n",
    "        self.hidden = hidden\n",
    "        self.emb_cat_hprevs = emb_cat_hprevs\n",
    "        return logits\n",
    "\n",
    "    def backward(self, grad):\n",
    "        hidden, emb_cat_hprevs = self.hidden, self.emb_cat_hprevs\n",
    "        t = hidden.size(1)\n",
    "        dhidden = self.lm_head.backward(grad)\n",
    "        # logits grad to start, wte, Cw grad\n",
    "        dembs = []\n",
    "        dCw, dhprev = 0., 0.\n",
    "        if self.Cw.bias is not None:\n",
    "            dCw_bias = 0.\n",
    "        for i in range(t-1, -1, -1):\n",
    "            # hidden state grad, emb grad\n",
    "            dhi = dhidden[:, i, :] + dhprev # grad from logits + grad from prev hidden state\n",
    "            hi = hidden[:, i, :]\n",
    "            dhi = (1 - hi**2) * dhi # grad of tanh\n",
    "            demb_i_cat_dhi = dhi @ self.Cw.weight.T\n",
    "            demb_i, dhprev = demb_i_cat_dhi.tensor_split([self.n_embd,], dim=1)\n",
    "            dembs.append(demb_i)\n",
    "            # cell weight grad\n",
    "            emb_i_cat_hprev = emb_cat_hprevs[i]\n",
    "            dCw += emb_i_cat_hprev.T @ dhi\n",
    "            if self.Cw.bias is not None:\n",
    "                dCw_bias += dhi.sum(dim=0)\n",
    "        dstart = dhprev.sum(dim=0, keepdim=True)\n",
    "        demb = torch.stack(dembs[::-1], 1)\n",
    "        self.wte.backward(demb)\n",
    "        self.start_grad = dstart\n",
    "        self.Cw.weight_grad = dCw\n",
    "        if self.Cw.bias is not None:\n",
    "            self.Cw.bias_grad = dCw_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNtorch(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_embd2 = config.n_embd2\n",
    "        self.start = nn.Parameter(torch.zeros(1, config.n_embd2)) # the starting hidden state\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd) # token embeddings table\n",
    "        self.Cw = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "        self.lm_head = nn.Linear(config.n_embd2, self.vocab_size)\n",
    "        self.lm_head.weight.data *= 0.1\n",
    "        num_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of rnn parameters: %d\" % (num_params,))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx):\n",
    "        b, t = idx.size()\n",
    "\n",
    "        # embed all the integers up front and all at once for efficiency\n",
    "        emb = self.wte(idx) # (b, t, n_embd)\n",
    "\n",
    "        # sequentially iterate over the inputs and update the RNN state each tick\n",
    "        hprev = self.start.T.expand((b, -1)) # expand out the batch dimension\n",
    "        hiddens = []\n",
    "        for i in range(t):\n",
    "            xt = emb[:, i, :] # (b, n_embd)\n",
    "            xh = torch.cat([xt, hprev], dim=1)\n",
    "            ht = (self.Cw(xh)).tanh()\n",
    "            hprev = ht\n",
    "            hiddens.append(ht)\n",
    "\n",
    "        # decode the outputs\n",
    "        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n",
    "        logits = self.lm_head(hidden)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rnn parameters: 3807\n",
      "number of rnn parameters: 3807\n",
      "step 0        || Train   || 3.294839983578878 || Val   || 3.286344415679592\n",
      "              || Train_t || 3.294839983578878 || Val_t || 3.286344415679593\n",
      "step 10       || Train   || 3.227770821522568 || Val   || 3.219490323898836\n",
      "              || Train_t || 3.227770821522568 || Val_t || 3.219490323898836\n",
      "step 20       || Train   || 3.161388113574983 || Val   || 3.154835365687406\n",
      "              || Train_t || 3.161388113574983 || Val_t || 3.154835365687406\n",
      "step 30       || Train   || 3.092995482586020 || Val   || 3.081645231631132\n",
      "              || Train_t || 3.092995482586020 || Val_t || 3.081645231631132\n",
      "step 40       || Train   || 2.984496074098745 || Val   || 2.977521187897202\n",
      "              || Train_t || 2.984496074098745 || Val_t || 2.977521187897202\n",
      "step 50       || Train   || 2.896630851098716 || Val   || 2.839125178939733\n",
      "              || Train_t || 2.896630851098716 || Val_t || 2.839125178939733\n",
      "step 60       || Train   || 2.769615778890114 || Val   || 2.751578657119224\n",
      "              || Train_t || 2.769615778890114 || Val_t || 2.751578657119224\n",
      "step 70       || Train   || 2.688541394987958 || Val   || 2.702521582713471\n",
      "              || Train_t || 2.688541394987958 || Val_t || 2.702521582713470\n",
      "step 80       || Train   || 2.702914963611737 || Val   || 2.668000945760694\n",
      "              || Train_t || 2.702914963611737 || Val_t || 2.668000945760694\n",
      "step 90       || Train   || 2.688304810590481 || Val   || 2.638813175687505\n",
      "              || Train_t || 2.688304810590481 || Val_t || 2.638813175687505\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(n_embd=8, n_embd2=44)\n",
    "# models\n",
    "torch.manual_seed(42)\n",
    "model_rnn = RNN(config)\n",
    "model_rnn_t = RNNtorch(config)\n",
    "# copy weights\n",
    "model_rnn_t.wte.weight.data = model_rnn.wte.weight.data.clone()\n",
    "for i, (p_t, p) in enumerate(zip(model_rnn_t.parameters(), model_rnn.parameters())):\n",
    "    if p.dim() == 2 and i != 1: # skip the embedding layer\n",
    "        p_t.data = p.data.clone().T # linear layer weight\n",
    "    else:\n",
    "        p_t.data = p.data.clone()\n",
    "\n",
    "train_compare(model_rnn, model_rnn_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_embd2 = config.n_embd2\n",
    "        self.start = torch.zeros(1, config.n_embd2, dtype=config.dtype) # the starting hidden state\n",
    "        self.wte = Embedding(config.vocab_size, config.n_embd, dtype=config.dtype) # token embeddings table\n",
    "        self.Cr = Linear(config.n_embd + config.n_embd2, config.n_embd2, dtype=config.dtype)\n",
    "        self.Cbar = Linear(config.n_embd + config.n_embd2, config.n_embd2, dtype=config.dtype)\n",
    "        self.Cz = Linear(config.n_embd + config.n_embd2, config.n_embd2, dtype=config.dtype)\n",
    "        self.lm_head = Linear(config.n_embd2, self.vocab_size, dtype=config.dtype)\n",
    "        self.lm_head.weight.data *= 0.1\n",
    "        num_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of gru parameters: %d\" % (num_params,))\n",
    "        # grads\n",
    "        self.start_grad = None\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.start] + list(self.wte.parameters()) + list(self.Cr.parameters()) + list(self.Cbar.parameters()) + list(self.Cz.parameters()) + list(self.lm_head.parameters())\n",
    "    \n",
    "    def grads(self):\n",
    "        return [self.start_grad] + list(self.wte.grads()) + list(self.Cr.grads()) + list(self.Cbar.grads()) + list(self.Cz.grads()) + list(self.lm_head.grads())\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        b, t = x.size()\n",
    "        emb = self.wte(x) # (b, t, n_embd)\n",
    "        # sequentially iterate over the inputs and update the RNN state each tick\n",
    "        hprev = self.start.expand((b, -1)) # expand out the batch dimension\n",
    "        hiddens = []\n",
    "        emb_cat_hprevs, emb_cat_hprev_resets, hprevs, hbars, zs, rs = [], [], [], [], [], []\n",
    "        for i in range(t):\n",
    "            emb_i = emb[:, i, :] # (b, n_embd)\n",
    "            # --- gru cell ---\n",
    "            emb_i_cat_hprev = torch.cat([emb_i, hprev], dim=1)\n",
    "            ri = self.Cr(emb_i_cat_hprev)\n",
    "            ri = ri.sigmoid()\n",
    "            hprev_reset = ri * hprev\n",
    "            emb_i_cat_hprev_reset = torch.cat([emb_i, hprev_reset], dim=1)\n",
    "            hbar = self.Cbar(emb_i_cat_hprev_reset)\n",
    "            hbar = hbar.tanh()\n",
    "            zi = self.Cz(emb_i_cat_hprev)\n",
    "            zi = zi.sigmoid()\n",
    "            hi = (1 - zi) * hprev + zi * hbar\n",
    "            # backward buffer\n",
    "            hiddens.append(hi)\n",
    "            emb_cat_hprevs.append(emb_i_cat_hprev)\n",
    "            emb_cat_hprev_resets.append(emb_i_cat_hprev_reset)\n",
    "            hprevs.append(hprev)\n",
    "            hbars.append(hbar)\n",
    "            zs.append(zi)\n",
    "            rs.append(ri)\n",
    "            # update hprev\n",
    "            hprev = hi\n",
    "        # decode the outputs\n",
    "        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n",
    "        logits = self.lm_head(hidden)\n",
    "        # backward buffer\n",
    "        self.hidden = hidden\n",
    "        self.emb_cat_hprevs = emb_cat_hprevs\n",
    "        self.emb_cat_hprev_resets = emb_cat_hprev_resets\n",
    "        self.hprevs = hprevs\n",
    "        self.hbars = hbars\n",
    "        self.zs = zs\n",
    "        self.rs = rs\n",
    "        return logits\n",
    "\n",
    "    def backward(self, dlogits):\n",
    "        hidden, emb_cat_hprevs, emb_cat_hprev_resets, hprevs, hbars, zs, rs = self.hidden, self.emb_cat_hprevs, self.emb_cat_hprev_resets, self.hprevs, self.hbars, self.zs, self.rs\n",
    "        t = hidden.size(1)\n",
    "        dhidden = self.lm_head.backward(dlogits)\n",
    "        # logits grad to start, wte, Cw grad\n",
    "        dembs = []\n",
    "        dCr, dCbar, dCz, dhprev = 0., 0., 0., 0.\n",
    "        if self.Cr.bias is not None:\n",
    "            dCr_bias = 0.\n",
    "        if self.Cbar.bias is not None:\n",
    "            dCbar_bias = 0.\n",
    "        if self.Cz.bias is not None:\n",
    "            dCz_bias = 0.\n",
    "        for i in range(t-1, -1, -1):\n",
    "            # hidden state grad, emb grad\n",
    "            dhi = dhidden[:, i, :] + dhprev # grad from logits + grad from prev hidden state\n",
    "            dhbar = dhi * zs[i]\n",
    "            dhprev = dhi * (1 - zs[i])\n",
    "            dzi = dhi * (hbars[i] - hprevs[i])\n",
    "            dzi = dzi * (1 - zs[i]) * zs[i]\n",
    "            demb_i_cat_hprev = dzi @ self.Cz.weight.T\n",
    "            dCz += emb_cat_hprevs[i].T @ dzi\n",
    "            if self.Cz.bias is not None:\n",
    "                dCz_bias += dzi.sum(dim=0)\n",
    "\n",
    "            dhbar = dhbar * (1 - hbars[i]**2)\n",
    "            demb_i_cat_hprev_reset = dhbar @ self.Cbar.weight.T\n",
    "            dCbar += emb_cat_hprev_resets[i].T @ dhbar\n",
    "            if self.Cbar.bias is not None:\n",
    "                dCbar_bias += dhbar.sum(dim=0)\n",
    "\n",
    "            demb_i, dhprev_reset = demb_i_cat_hprev_reset.tensor_split([self.n_embd,], dim=1)\n",
    "            dri = dhprev_reset * hprevs[i]\n",
    "            dhprev += dhprev_reset * rs[i]\n",
    "            dri = dri * (1 - rs[i]) * rs[i]\n",
    "            demb_i_cat_hprev += dri @ self.Cr.weight.T\n",
    "            dCr += emb_cat_hprevs[i].T @ dri\n",
    "            if self.Cr.bias is not None:\n",
    "                dCr_bias += dri.sum(dim=0)\n",
    "            demb_more, dhprev_more = demb_i_cat_hprev.tensor_split([self.n_embd,], dim=1)\n",
    "            demb_i += demb_more\n",
    "            dhprev += dhprev_more\n",
    "            dembs.append(demb_i)\n",
    "        dstart = dhprev.sum(dim=0, keepdim=True)\n",
    "        demb = torch.stack(dembs[::-1], 1)\n",
    "        self.wte.backward(demb)\n",
    "        self.start_grad = dstart\n",
    "        self.Cr.weight_grad = dCr\n",
    "        if self.Cr.bias is not None:\n",
    "            self.Cr.bias_grad = dCr_bias\n",
    "        self.Cbar.weight_grad = dCbar\n",
    "        if self.Cbar.bias is not None:\n",
    "            self.Cbar.bias_grad = dCbar_bias\n",
    "        self.Cz.weight_grad = dCz\n",
    "        if self.Cz.bias is not None:\n",
    "            self.Cz.bias_grad = dCz_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUtorch(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.start = nn.Parameter(torch.zeros(1, config.n_embd2, dtype=config.dtype)) # the starting hidden state\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd) # token embeddings table\n",
    "        self.Cr = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "        self.Cbar = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "        self.Cz = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "        self.lm_head = nn.Linear(config.n_embd2, self.vocab_size)\n",
    "        self.lm_head.weight.data *= 0.1\n",
    "        num_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of gru parameters: %d\" % (num_params,))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx):\n",
    "        b, t = idx.size()\n",
    "        emb = self.wte(idx) # (b, t, n_embd)\n",
    "        hprev = self.start.T.expand((b, -1)) # expand out the batch dimension\n",
    "        hiddens = []\n",
    "        for i in range(t):\n",
    "            xt = emb[:, i, :] # (b, n_embd)\n",
    "            # --- gru cell ---\n",
    "            xh = torch.cat([xt, hprev], dim=1)\n",
    "            r = (self.Cr(xh)).sigmoid()\n",
    "            hprev_reset = r * hprev\n",
    "            xhr = torch.cat([xt, hprev_reset], dim=1)\n",
    "            hbar = (self.Cbar(xhr)).tanh()\n",
    "            z = (self.Cz(xh)).sigmoid()\n",
    "            z = (self.Cz(xh)).sigmoid()\n",
    "            ht = (1 - z) * hprev + z * hbar\n",
    "            # --------------\n",
    "            hprev = ht\n",
    "            hiddens.append(ht)\n",
    "\n",
    "        # decode the outputs\n",
    "        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n",
    "        logits = self.lm_head(hidden)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of gru parameters: 3915\n",
      "number of gru parameters: 3915\n",
      "step 0        || Train   || 3.299904090396590 || Val   || 3.294552358278001\n",
      "              || Train_t || 3.299904090396590 || Val_t || 3.294552358278001\n",
      "step 10       || Train   || 3.245485248785802 || Val   || 3.241660817615214\n",
      "              || Train_t || 3.245485248785803 || Val_t || 3.241660817615213\n",
      "step 20       || Train   || 3.199123160111661 || Val   || 3.193978427459241\n",
      "              || Train_t || 3.199123160111661 || Val_t || 3.193978427459242\n",
      "step 30       || Train   || 3.153246778772803 || Val   || 3.147593808138929\n",
      "              || Train_t || 3.153246778772804 || Val_t || 3.147593808138929\n",
      "step 40       || Train   || 3.101464398879880 || Val   || 3.102202635505448\n",
      "              || Train_t || 3.101464398879879 || Val_t || 3.102202635505448\n",
      "step 50       || Train   || 3.086557755896768 || Val   || 3.060179919921247\n",
      "              || Train_t || 3.086557755896768 || Val_t || 3.060179919921247\n",
      "step 60       || Train   || 3.029031696231070 || Val   || 3.020068769499067\n",
      "              || Train_t || 3.029031696231070 || Val_t || 3.020068769499067\n",
      "step 70       || Train   || 2.960696141614644 || Val   || 2.978007513108196\n",
      "              || Train_t || 2.960696141614644 || Val_t || 2.978007513108196\n",
      "step 80       || Train   || 2.968800525513914 || Val   || 2.939774154614303\n",
      "              || Train_t || 2.968800525513914 || Val_t || 2.939774154614303\n",
      "step 90       || Train   || 2.940370109888878 || Val   || 2.904376549611244\n",
      "              || Train_t || 2.940370109888878 || Val_t || 2.904376549611244\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(n_embd=8, n_embd2=27)\n",
    "# models\n",
    "torch.manual_seed(42)\n",
    "model_gru = GRU(config)\n",
    "model_gru_t = GRUtorch(config)\n",
    "# copy weights\n",
    "model_gru_t.wte.weight.data = model_gru.wte.weight.data.clone()\n",
    "for i, (p_t, p) in enumerate(zip(model_gru_t.parameters(), model_gru.parameters())):\n",
    "    if p.dim() == 2 and i != 1: # skip the embedding layer\n",
    "        p_t.data = p.data.clone().T # linear layer weight\n",
    "    else:\n",
    "        p_t.data = p.data.clone()\n",
    "\n",
    "train_compare(model_gru, model_gru_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = Linear(config.n_embd, 3 * config.n_embd, dtype=config.dtype)\n",
    "        # output projection\n",
    "        self.c_proj = Linear(config.n_embd, config.n_embd, dtype=config.dtype)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.bias = torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "    \n",
    "    def parameters(self):\n",
    "        return list(self.c_attn.parameters()) + list(self.c_proj.parameters())\n",
    "    \n",
    "    def grads(self):\n",
    "        return list(self.c_attn.grads()) + list(self.c_proj.grads())\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k ,v  = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        att_dot = (q @ k.transpose(-2, -1)) * (1.0 / k.size(-1)**0.5)\n",
    "        att_mask = att_dot.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att_mask, dim=-1)\n",
    "        y_trans = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y_preproj = y_trans.transpose(1, 2).contiguous().view(B, T, C) # (B, nh, T, hs) -> (B, T, nh*hs)\n",
    "        y = self.c_proj(y_preproj) # (B, T, C) -> (B, T, C)\n",
    "        # backward buffer\n",
    "        self.y_preproj = y_preproj\n",
    "        self.att = att\n",
    "        self.q = q\n",
    "        self.k = k\n",
    "        self.v = v\n",
    "        self.x = x\n",
    "        return y\n",
    "\n",
    "    def backward(self, dy):\n",
    "        B, T, C = dy.size()\n",
    "        (y_preproj, att, q, k, v, x) = (\n",
    "            self.y_preproj, self.att, self.q, self.k, self.v, self.x\n",
    "        )\n",
    "        dC_proj = (y_preproj.transpose(-2, -1) @ dy).sum(dim=0)\n",
    "        self.c_proj.weight_grad = dC_proj\n",
    "        if self.c_proj.bias is not None:\n",
    "            dC_proj_bias = dy.sum(dim=[0, 1])\n",
    "            self.c_proj.bias_grad = dC_proj_bias\n",
    "        dy_preproj = dy @ self.c_proj.weight.T\n",
    "        dy_trans = dy_preproj.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        datt = dy_trans @ v.transpose(-2, -1)\n",
    "        dv = att.transpose(-2, -1) @ dy_trans\n",
    "        datt_mask = att * (datt - (att * datt).sum(dim=-1, keepdim=True))\n",
    "        datt_dot = datt_mask.masked_fill(self.bias[:,:,:T,:T] == 0, 0)\n",
    "        dq = (datt_dot @ k) * (1.0 / k.size(-1)**0.5)\n",
    "        dk = (datt_dot.transpose(-2, -1) @ q) * (1.0 / k.size(-1)**0.5)\n",
    "        dq = dq.transpose(1, 2).reshape(B, T, C)\n",
    "        dk = dk.transpose(1, 2).reshape(B, T, C)\n",
    "        dv = dv.transpose(1, 2).reshape(B, T, C)\n",
    "        dqkv = torch.cat([dq, dk, dv], dim=2)\n",
    "        dC_atten = (x.transpose(-2, -1) @ dqkv).sum(dim=0)\n",
    "        self.c_attn.weight_grad = dC_atten\n",
    "        if self.c_attn.bias is not None:\n",
    "            dC_atten_bias = dqkv.sum(dim=[0, 1])\n",
    "            self.c_attn.bias_grad = dC_atten_bias\n",
    "        dx = dqkv @ self.c_attn.weight.T\n",
    "        return dx\n",
    "\n",
    "class Block(Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, dtype=config.dtype)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, dtype=config.dtype)\n",
    "        self.mlp = Sequential([\n",
    "            Linear(config.n_embd, 4 * config.n_embd, dtype=config.dtype),\n",
    "            GELU(),\n",
    "            Linear(4 * config.n_embd, config.n_embd, dtype=config.dtype),\n",
    "        ])\n",
    "    \n",
    "    def parameters(self):\n",
    "        return list(self.ln_1.parameters()) + list(self.attn.parameters()) + list(self.ln_2.parameters()) + list(self.mlp.parameters())\n",
    "    \n",
    "    def grads(self):\n",
    "        return list(self.ln_1.grads()) + list(self.attn.grads()) + list(self.ln_2.grads()) + list(self.mlp.grads())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dx):\n",
    "        dx = dx + self.ln_2.backward(self.mlp.backward(dx))\n",
    "        dx = dx + self.ln_1.backward(self.attn.backward(dx))\n",
    "        return dx\n",
    "\n",
    "class Transformer(Module):\n",
    "    \"\"\" Transformer Language Model, exactly as seen in GPT-2 \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.wte = Embedding(config.vocab_size, config.n_embd, dtype=config.dtype)\n",
    "        self.wpe = Embedding(config.block_size, config.n_embd, dtype=config.dtype)\n",
    "        self.transformer = Sequential([Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = LayerNorm(config.n_embd, dtype=config.dtype)\n",
    "        self.lm_head = Linear(config.n_embd, config.vocab_size, bias=False, dtype=config.dtype)\n",
    "        self.lm_head.weight.data *= 0.1\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"number of transformer parameters: {n_params}\")\n",
    "    \n",
    "    def parameters(self):\n",
    "        return list(self.wte.parameters()) + list(self.wpe.parameters()) + list(self.transformer.parameters()) + list(self.ln_f.parameters()) + list(self.lm_head.parameters())\n",
    "    \n",
    "    def grads(self):\n",
    "        return list(self.wte.grads()) + list(self.wpe.grads()) + list(self.transformer.grads()) + list(self.ln_f.grads()) + list(self.lm_head.grads())\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer:\n",
    "            x = block(x)\n",
    "        h = self.ln_f(x)\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def backward(self, dlogits):\n",
    "        dh = self.lm_head.backward(dlogits)\n",
    "        dh = self.ln_f.backward(dh)\n",
    "        dx = self.transformer.backward(dh)\n",
    "        dtok_emb = dx\n",
    "        self.wte.backward(dtok_emb)\n",
    "        dpos_emb = dx.sum(dim=0, keepdim=True)\n",
    "        self.wpe.backward(dpos_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class GELUTorch(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
    "\n",
    "class CausalSelfAttentionTorch(nn.Module):\n",
    "    \"\"\"\n",
    "    Copied from https://github.com/karpathy/makemore/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, dtype=config.dtype)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, dtype=config.dtype)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class BlockTorch(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd, dtype=config.dtype)\n",
    "        self.attn = CausalSelfAttentionTorch(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd, dtype=config.dtype)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd, dtype=config.dtype),\n",
    "            GELUTorch(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd, dtype=config.dtype),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class TransformerTorch(nn.Module):\n",
    "    \"\"\" Transformer Language Model, exactly as seen in GPT-2 \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd, dtype=config.dtype),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd, dtype=config.dtype),\n",
    "            h = nn.ModuleList([BlockTorch(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd, dtype=config.dtype),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False, dtype=config.dtype)\n",
    "        self.lm_head.weight.data *= 0.1\n",
    "\n",
    "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"number of transformer parameters: {n_params}\")\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of transformer parameters: 3975\n",
      "number of transformer parameters: 3975\n",
      "step 0        || Train   || 3.311522483767896 || Val   || 3.289990276919096\n",
      "              || Train_t || 3.311522483767896 || Val_t || 3.289990276919097\n",
      "step 10       || Train   || 3.178493127699395 || Val   || 3.170298218703802\n",
      "              || Train_t || 3.178492890692110 || Val_t || 3.170297899265177\n",
      "step 20       || Train   || 3.038496903016854 || Val   || 3.039928430373856\n",
      "              || Train_t || 3.038482174365516 || Val_t || 3.039911623615739\n",
      "step 30       || Train   || 2.919560121703425 || Val   || 2.906650093029973\n",
      "              || Train_t || 2.919478901519527 || Val_t || 2.906557755050494\n",
      "step 40       || Train   || 2.768903249274509 || Val   || 2.805299105172039\n",
      "              || Train_t || 2.768674663183205 || Val_t || 2.805097498527474\n",
      "step 50       || Train   || 2.809914813282925 || Val   || 2.748339074704934\n",
      "              || Train_t || 2.809813124802048 || Val_t || 2.748049781719360\n",
      "step 60       || Train   || 2.668978805622858 || Val   || 2.710279227100359\n",
      "              || Train_t || 2.668645667979971 || Val_t || 2.709957316384372\n",
      "step 70       || Train   || 2.620111829898373 || Val   || 2.683813435502524\n",
      "              || Train_t || 2.619830259588838 || Val_t || 2.683481253192955\n",
      "step 80       || Train   || 2.681827567915126 || Val   || 2.664063114617204\n",
      "              || Train_t || 2.681485063752078 || Val_t || 2.663731586610707\n",
      "step 90       || Train   || 2.737968525580527 || Val   || 2.648254600223756\n",
      "              || Train_t || 2.737699610002808 || Val_t || 2.647947698457693\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(n_embd=15, n_embd2=None, n_head=1, n_layer=1)\n",
    "# models\n",
    "torch.manual_seed(42)\n",
    "model_transformer = Transformer(config)\n",
    "model_transformer_t = TransformerTorch(config)\n",
    "# copy weights\n",
    "for i, (p, p_t) in enumerate(zip(model_transformer.parameters(), model_transformer_t.parameters())):\n",
    "    if p_t.ndim == 2:\n",
    "        if i >= 2:\n",
    "            p_t.data = p.data.clone().T\n",
    "        else:\n",
    "            p_t.data = p.data.clone()\n",
    "    else:\n",
    "        p_t.data = p.data.clone()\n",
    "\n",
    "train_compare(model_transformer, model_transformer_t, n_steps=100, eval_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is much larger error than previous model, check it for longer run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of transformer parameters: 3975\n",
      "number of transformer parameters: 3975\n",
      "step 0        || Train   || 3.311522483767896 || Val   || 3.289990276919096\n",
      "              || Train_t || 3.311522483767896 || Val_t || 3.289990276919097\n",
      "step 1000     || Train   || 2.445110275154105 || Val   || 2.369427515501453\n",
      "              || Train_t || 2.442739328399659 || Val_t || 2.368534147066376\n",
      "step 2000     || Train   || 2.268471904132468 || Val   || 2.310909552271787\n",
      "              || Train_t || 2.271095492746350 || Val_t || 2.309797643158759\n",
      "step 3000     || Train   || 2.354346512864395 || Val   || 2.281833666642088\n",
      "              || Train_t || 2.357320315415359 || Val_t || 2.280391346423363\n",
      "step 4000     || Train   || 2.332165341016138 || Val   || 2.258333295700399\n",
      "              || Train_t || 2.330874828638262 || Val_t || 2.257287653908691\n",
      "step 5000     || Train   || 2.273487359486585 || Val   || 2.241632739001550\n",
      "              || Train_t || 2.272988358372061 || Val_t || 2.240859362850584\n",
      "step 6000     || Train   || 2.223155841667997 || Val   || 2.234374541290006\n",
      "              || Train_t || 2.221801409737160 || Val_t || 2.233458528331294\n",
      "step 7000     || Train   || 2.224391932628489 || Val   || 2.229928786999515\n",
      "              || Train_t || 2.222167712021689 || Val_t || 2.229059364598439\n",
      "step 8000     || Train   || 2.059881377382012 || Val   || 2.222496734069302\n",
      "              || Train_t || 2.058296007954847 || Val_t || 2.221440613538345\n",
      "step 9000     || Train   || 2.199491438755430 || Val   || 2.221461160852795\n",
      "              || Train_t || 2.196515023258842 || Val_t || 2.220380411313623\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(n_embd=15, n_embd2=None, n_head=1, n_layer=1)\n",
    "# models\n",
    "torch.manual_seed(42)\n",
    "model_transformer = Transformer(config)\n",
    "model_transformer_t = TransformerTorch(config)\n",
    "# copy weights\n",
    "for i, (p, p_t) in enumerate(zip(model_transformer.parameters(), model_transformer_t.parameters())):\n",
    "    if p_t.ndim == 2:\n",
    "        if i >= 2:\n",
    "            p_t.data = p.data.clone().T\n",
    "        else:\n",
    "            p_t.data = p.data.clone()\n",
    "    else:\n",
    "        p_t.data = p.data.clone()\n",
    "\n",
    "train_compare(model_transformer, model_transformer_t, n_steps=10000, eval_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "error ~ 1e-2\n",
    "\n",
    "- Indeed, ablation study shows even we only use one layernorm in the block(without attn, mlp), we cannot get the same cumulative error as previous simpler models(error ~ 1e-14). This may because the model is much more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|| Test   || 2.218395222755695\n",
      "|| Test_t || 2.218338993436960\n"
     ]
    }
   ],
   "source": [
    "x, y = X_test, Y_test\n",
    "with torch.no_grad():\n",
    "    logits_t = model_transformer_t(x)\n",
    "    test_loss_t = F.cross_entropy(logits_t.view(-1, logits_t.size(-1)), y.view(-1), ignore_index=-1)\n",
    "logits = model_transformer(x)\n",
    "test_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "print(f'|| Test   || {test_loss.item():.15f}\\n|| Test_t || {test_loss_t.item():.15f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None, generator=None):\n",
    "    \"\"\"\n",
    "    Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "    the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "    Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "    \"\"\"\n",
    "    block_size = model.get_block_size()\n",
    "    for _ in range(max_new_tokens):\n",
    "        # if the sequence context is growing too long we must crop it at block_size\n",
    "        idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "        # forward the model to get the logits for the index in the sequence\n",
    "        logits = model(idx_cond)\n",
    "        # pluck the logits at the final step and scale by desired temperature\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to convert logits to (normalized) probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # either sample from the distribution or take the most likely element\n",
    "        if do_sample:\n",
    "            idx_next = torch.multinomial(probs, num_samples=1, generator=generator)\n",
    "        else:\n",
    "            _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "        # append sampled index to the running sequence and continue\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        if idx_next.item() == 0:\n",
    "            break\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mlp parameters: 3995\n",
      "number of rnn parameters: 3933\n",
      "number of gru parameters: 3915\n",
      "number of transformer parameters: 3975\n",
      "step 0    || Train || mlp: 3.2976 | rnn: 3.2990 | gru: 3.2999 | transformer: 3.3115 || Val || mlp: 3.1973 | rnn: 3.2710 | gru: 3.2819 | transformer: 3.2611\n",
      "step 1000 || Train || mlp: 2.3955 | rnn: 2.3438 | gru: 2.3469 | transformer: 2.3820 || Val || mlp: 2.3495 | rnn: 2.2665 | gru: 2.3415 | transformer: 2.3281\n",
      "step 2000 || Train || mlp: 2.2500 | rnn: 2.2039 | gru: 2.2339 | transformer: 2.2332 || Val || mlp: 2.2686 | rnn: 2.2101 | gru: 2.2658 | transformer: 2.2602\n",
      "step 3000 || Train || mlp: 2.2993 | rnn: 2.2984 | gru: 2.2704 | transformer: 2.3452 || Val || mlp: 2.2453 | rnn: 2.1983 | gru: 2.2313 | transformer: 2.2449\n",
      "step 4000 || Train || mlp: 2.2299 | rnn: 2.2409 | gru: 2.2433 | transformer: 2.3017 || Val || mlp: 2.2277 | rnn: 2.1762 | gru: 2.2085 | transformer: 2.2277\n",
      "step 5000 || Train || mlp: 2.2998 | rnn: 2.2009 | gru: 2.2787 | transformer: 2.2366 || Val || mlp: 2.2149 | rnn: 2.1701 | gru: 2.1937 | transformer: 2.2133\n",
      "step 6000 || Train || mlp: 2.2223 | rnn: 2.1450 | gru: 2.1997 | transformer: 2.2055 || Val || mlp: 2.2096 | rnn: 2.1649 | gru: 2.1850 | transformer: 2.2091\n",
      "step 7000 || Train || mlp: 2.1950 | rnn: 2.1762 | gru: 2.2364 | transformer: 2.1868 || Val || mlp: 2.2052 | rnn: 2.1612 | gru: 2.1757 | transformer: 2.2069\n",
      "step 8000 || Train || mlp: 2.0094 | rnn: 1.9640 | gru: 2.0125 | transformer: 1.9976 || Val || mlp: 2.1936 | rnn: 2.1411 | gru: 2.1675 | transformer: 2.1871\n",
      "step 9000 || Train || mlp: 2.2271 | rnn: 2.1573 | gru: 2.1924 | transformer: 2.1713 || Val || mlp: 2.1921 | rnn: 2.1413 | gru: 2.1666 | transformer: 2.1856\n"
     ]
    }
   ],
   "source": [
    "# models\n",
    "torch.manual_seed(42)\n",
    "model_mlp = MLP(ModelConfig(block_size=block_size, vocab_size=vocab_size, n_embd=8, n_embd2=24))\n",
    "torch.manual_seed(42)\n",
    "model_rnn = RNN(ModelConfig(block_size=block_size, vocab_size=vocab_size, n_embd=8, n_embd2=45))\n",
    "torch.manual_seed(42)\n",
    "model_gru = GRU(ModelConfig(block_size=block_size, vocab_size=vocab_size, n_embd=8, n_embd2=27))\n",
    "torch.manual_seed(42)\n",
    "model_transformer = Transformer(ModelConfig(block_size=block_size, vocab_size=vocab_size, n_embd=15, n_embd2=None, n_head=1, n_layer=1))\n",
    "models = {\n",
    "    'mlp': model_mlp,\n",
    "    'rnn': model_rnn,\n",
    "    'gru': model_gru,\n",
    "    'transformer': model_transformer\n",
    "}\n",
    "optimizers = [CrossEntropyLoss3d() for _ in range(len(models))]\n",
    "params = [p for model in models.values() for p in model.parameters()]\n",
    "\n",
    "# args\n",
    "n_steps = 10000\n",
    "eval_every = 1000\n",
    "bs = 32\n",
    "ini_lr = 0.1\n",
    "\n",
    "# train\n",
    "lossi = []\n",
    "torch.manual_seed(42)\n",
    "for  step in range(n_steps):\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "    lr = ini_lr if step < int(n_steps * 0.75) else ini_lr * 0.1\n",
    "\n",
    "    losses = []\n",
    "    for (name, model), optimizer in zip(models.items(), optimizers):\n",
    "        logits = model(x)\n",
    "        loss = optimizer(logits, y)\n",
    "        dlogits = optimizer.backward(loss)\n",
    "        model.backward(dlogits)\n",
    "        losses.append(loss)\n",
    "    lossi.append(losses)\n",
    "    grads = [g for model in models.values() for g in model.grads() ]\n",
    "    for p, g in zip(params, grads):\n",
    "        p.data -= lr * g\n",
    "    \n",
    "    if step % eval_every == 0:\n",
    "        with torch.no_grad():\n",
    "            x, y = X_val, Y_val\n",
    "            val_losses = []\n",
    "            for name, model in models.items():\n",
    "                logits = model(x)\n",
    "                val_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "                val_losses.append(val_loss)\n",
    "            loss_out = f' | '.join(f'{name}: {loss.item():.4f}' for name, loss in zip(models.keys(), losses))\n",
    "            val_loss_out = f' | '.join(f'{name}: {val_loss.item():.4f}' for name, val_loss in zip(models.keys(), val_losses))\n",
    "            print(f'step {step:<4} || Train || {loss_out} || Val || {val_loss_out}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test || mlp: 2.2007 | rnn: 2.1445 | gru: 2.1745 | transformer: 2.1891\n",
      "\n",
      "mlp: .anuguel.     | rnn: .tis.         | gru: .mabidu.      | transformer: .danteben.   \n",
      "mlp: .silay.       | rnn: .darosmah.    | gru: .lanin.       | transformer: .epiach.     \n",
      "mlp: .denlenze.    | rnn: .kenco.       | gru: .jordon.      | transformer: .kalla.      \n",
      "mlp: .mikhiya.     | rnn: .adhvisia.    | gru: .achalla.     | transformer: .tiaretphel  \n",
      "mlp: .dare.        | rnn: .tal.         | gru: .selari.      | transformer: .dakariah.   \n",
      "mlp: .nolen.       | rnn: .roighin.     | gru: .shuri.       | transformer: .payiell.    \n",
      "mlp: .liljahn.     | rnn: .frikcindon   | gru: .hanaky.      | transformer: .krielia.    \n",
      "mlp: .abia.        | rnn: .anode.       | gru: .brinz.       | transformer: .chavi.      \n",
      "mlp: .damayan.     | rnn: .jarita.      | gru: .jiiria.      | transformer: .laiya.      \n",
      "mlp: .mouketa.     | rnn: .elenaq.      | gru: .yazraand.    | transformer: .emmooria.   \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x, y = X_test, Y_test\n",
    "    test_losses = []\n",
    "    for name, model in models.items():\n",
    "        logits = model(x)\n",
    "        test_loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "        test_losses.append(test_loss)\n",
    "    test_loss_out = f' | '.join(f'{name}: {test_loss.item():.4f}' for name, test_loss in zip(models.keys(), test_losses))\n",
    "    print(f'test || {test_loss_out}')\n",
    "\n",
    "print()\n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n",
    "for _ in range(10):\n",
    "    s_list = []\n",
    "    for name, model in models.items():\n",
    "        tks = generate(model, torch.tensor([[0]]), max_new_tokens=10, do_sample=True, generator=g)[0].tolist()\n",
    "        s = ''.join(itos[i] for i in tks)\n",
    "        s_list.append(s)\n",
    "    out = ' | '.join(f'{name}: {s:<13s}' for name, s in zip(models.keys(), s_list))\n",
    "    print(f'{out}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def avg_loss(losses, window=100):\n",
    "    ini_loss = losses[:10]\n",
    "    avg_loss = losses[10:].unfold(0, window, 1).mean(1)\n",
    "    return torch.cat([ini_loss, avg_loss]) # first 10 steps are not averaged, otherwise the initial loss is different\n",
    "\n",
    "lossi = torch.tensor(lossi) if not isinstance(lossi, torch.Tensor) else lossi\n",
    "log_lossi = lossi.log10()\n",
    "model_names = list(models.keys())\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(log_lossi.shape[1]):\n",
    "    plt.plot(avg_loss(log_lossi[:, i]), label=f'{model_names[i]}')\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('loss(log10)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
