{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".aukai.......... -> aukai.\n",
      ".ellanore....... -> ellanore.\n",
      ".liem........... -> liem.\n",
      ".aquarius....... -> aquarius.\n",
      ".joangel........ -> joangel.\n",
      ".wryn........... -> wryn.\n",
      ".isabela........ -> isabela.\n",
      ".astryd......... -> astryd.\n",
      ".maleik......... -> maleik.\n",
      ".emerick........ -> emerick.\n",
      ".natasha........ -> natasha.\n",
      ".kasandra....... -> kasandra.\n",
      ".aevin.......... -> aevin.\n",
      ".brason......... -> brason.\n",
      ".naiara......... -> naiara.\n",
      ".alanna......... -> alanna.\n",
      ".raunak......... -> raunak.\n",
      ".gohan.......... -> gohan.\n",
      ".ivie........... -> ivie.\n",
      ".alandis........ -> alandis.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([23595, 16]),\n",
       " torch.Size([2949, 16]),\n",
       " torch.Size([2950, 16]),\n",
       " torch.Size([23595, 16]),\n",
       " torch.Size([2949, 16]),\n",
       " torch.Size([2950, 16]))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from tiny_torch import *\n",
    "\n",
    "import random\n",
    "import torch\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words = sorted(list(set(words))) # set cause uncontrollable randomnessï¼Œ sorted for reproducibility\n",
    "max_len = max(len(w) for w in words)\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "chs = list(set(''.join(words + ['.']))) # add special token\n",
    "chs = sorted(chs, reverse=False)\n",
    "stoi = {ch: i for i, ch in enumerate(chs)}\n",
    "itos = {i: ch for i, ch in enumerate(chs)}\n",
    "\n",
    "# predict next token use previous tokens\n",
    "vocab_size = len(chs)\n",
    "block_size = max_len + 1\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    x = torch.zeros(max_len + 1, dtype=torch.long)\n",
    "    y = torch.zeros(max_len + 1, dtype=torch.long)\n",
    "    x[1:1+len(w)] = torch.tensor([stoi[ch] for ch in w])\n",
    "    y[:len(w)] = torch.tensor([stoi[ch] for ch in w])\n",
    "    y[len(w)+1:] = -1 # mask the loss at the inactive locations\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "\n",
    "X = torch.stack(X)\n",
    "Y = torch.stack(Y)\n",
    "n1, n2  = int(0.8 * len(X)), int(0.9 * len(X))\n",
    "\n",
    "X_train, X_val, X_test = X.tensor_split([n1, n2])\n",
    "Y_train, Y_val, Y_test = Y.tensor_split([n1, n2])\n",
    "\n",
    "show = 20\n",
    "for x, y in zip(X_train[:show], Y_train[:show]):\n",
    "    sx = ''.join(itos[i.item()] for i in x)\n",
    "    sy = ''.join(itos[i.item()] for i in y if i.item() != -1)\n",
    "    print(f'{sx} -> {sy}')\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# move to manual backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    block_size: int = None # length of the input sequences of integers\n",
    "    vocab_size: int = None # the input integers are in range [0 .. vocab_size -1]\n",
    "    # parameters below control the sizes of each model slightly differently\n",
    "    n_layer: int = 4\n",
    "    n_embd: int = 64\n",
    "    n_embd2: int = 64\n",
    "    n_head: int = 4\n",
    "    dtype: torch.dtype = torch.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss3d(Module):\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'MyCrossEntropyLoss()'\n",
    "\n",
    "    def __call__(self, logits, y, ignore_index=-1):\n",
    "        assert logits.ndim == 3, 'only verify for 3d logits (B, T, C)'\n",
    "        B, T, V = logits.shape\n",
    "        max_logits = logits.max(dim=-1, keepdim=True).values\n",
    "        probs = (logits - max_logits).softmax(dim=-1)\n",
    "        mask = (y != ignore_index)\n",
    "        loss = -probs[torch.arange(B)[:, None], torch.arange(T)[None, :], y].log() * mask # indices also need to be broadcasted, we use [None, :]\n",
    "        loss = loss.sum() / mask.sum()\n",
    "        # backward buffer\n",
    "        self.probs = probs\n",
    "        self.y = y\n",
    "        self.mask = mask\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def backward(self, grad=1.0): # grad is the gradient of the loss function, usually 1.0\n",
    "        probs, y, mask = self.probs, self.y, self.mask\n",
    "        B, T, V = probs.shape\n",
    "        x_grad = probs.clone()\n",
    "        x_grad[torch.arange(B)[:, None], torch.arange(T)[None, :], y] -= 1.0\n",
    "        x_grad = x_grad * mask.unsqueeze(-1)\n",
    "        x_grad = x_grad / mask.sum() * grad\n",
    "        return x_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    \"\"\"\n",
    "    takes the previous block_size tokens, encodes them with a lookup table,\n",
    "    concatenates the vectors and predicts the next token with an MLP.\n",
    "\n",
    "    Reference:\n",
    "    Bengio et al. 2003 https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.wte = Embedding(config.vocab_size + 1, config.n_embd, dtype=config.dtype) # token embeddings table\n",
    "        # +1 in the line above for a special <BLANK> token that gets inserted if encoding a token\n",
    "        # before the beginning of the input sequence\n",
    "        self.mlp = Sequential([\n",
    "            Linear(self.block_size * config.n_embd, config.n_embd2, dtype=config.dtype),\n",
    "            Tanh(),\n",
    "            Linear(config.n_embd2, self.vocab_size, dtype=config.dtype)\n",
    "        ])\n",
    "        self.mlp[-1].weight.data *= 0.1\n",
    "        self.mlp[-1].bias.data *= 0.01\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of transformer parameters: %d\" % (n_params,))\n",
    "        self.config = config\n",
    "    \n",
    "    def parameters(self):\n",
    "        return list(self.wte.parameters()) + list(self.mlp.parameters())\n",
    "    \n",
    "    def grads(self):\n",
    "        return list(self.wte.grads()) + list(self.mlp.grads())\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def __call__(self, idx, targets=None):\n",
    "\n",
    "        # gather the word embeddings of the previous 3 words\n",
    "        idx_buf = []\n",
    "        embs = []\n",
    "        for k in range(self.block_size):\n",
    "            tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "            idx_buf.append(idx.unsqueeze(-1))\n",
    "            embs.append(tok_emb)\n",
    "            idx = torch.roll(idx, 1, 1)\n",
    "            idx[:, 0] = self.vocab_size # special <BLANK> token\n",
    "\n",
    "        # concat all of the embeddings together and pass through an MLP\n",
    "        x = torch.cat(embs, -1) # (b, t, n_embd * block_size)\n",
    "        logits = self.mlp(x)\n",
    "\n",
    "        # backward buffer\n",
    "        self.idx_buf = torch.cat(idx_buf, -1) # (b, t, t)\n",
    "\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        grad = self.mlp.backward(grad)\n",
    "        # mlp backprop to wte\n",
    "        b, t, _ = grad.shape # (b, t, n_embd * block_size)\n",
    "        grad = grad.view(b * t * self.config.block_size, self.config.n_embd) # (b*t*block_size, n_embd)\n",
    "        wte_weight = self.wte.weight\n",
    "        wte_grad = torch.zeros_like(wte_weight)\n",
    "        wte_grad.index_add_(dim=0, index=self.idx_buf.view(-1), source=grad)\n",
    "        self.wte.weight_grad = wte_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLPtorch(nn.Module):\n",
    "    \"\"\"\n",
    "    takes the previous block_size tokens, encodes them with a lookup table,\n",
    "    concatenates the vectors and predicts the next token with an MLP.\n",
    "\n",
    "    Reference:\n",
    "    Bengio et al. 2003 https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.wte = nn.Embedding(config.vocab_size + 1, config.n_embd) # token embeddings table\n",
    "        # +1 in the line above for a special <BLANK> token that gets inserted if encoding a token\n",
    "        # before the beginning of the input sequence\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.block_size * config.n_embd, config.n_embd2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(config.n_embd2, self.vocab_size)\n",
    "        )\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of transformer parameters: %d\" % (n_params,))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # gather the word embeddings of the previous 3 words\n",
    "        embs = []\n",
    "        for k in range(self.block_size):\n",
    "            tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "            idx = torch.roll(idx, 1, 1)\n",
    "            idx[:, 0] = self.vocab_size # special <BLANK> token\n",
    "            embs.append(tok_emb)\n",
    "\n",
    "        # concat all of the embeddings together and pass through an MLP\n",
    "        x = torch.cat(embs, -1) # (b, t, n_embd * block_size)\n",
    "        logits = self.mlp(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of transformer parameters: 3995\n",
      "number of transformer parameters: 3995\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "config = ModelConfig(block_size=block_size, vocab_size=vocab_size, n_embd=8, n_embd2=24)\n",
    "# models\n",
    "torch.manual_seed(42)\n",
    "model_mlp = MLP(config)\n",
    "model_mlp_t = MLPtorch(config)\n",
    "# copy weights\n",
    "model_mlp_t.wte.weight.data = model_mlp.wte.weight.data.clone()\n",
    "for i, (p_t, p) in enumerate(zip(model_mlp_t.mlp.parameters(), model_mlp.mlp.parameters())):\n",
    "    if p.dim() == 2:\n",
    "        p_t.data = p.data.clone().T # linear layer weight\n",
    "    else:\n",
    "        p_t.data = p.data.clone()\n",
    "# loss\n",
    "loss_fn = CrossEntropyLoss3d()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0    || Train || 3.296238 || Val || 3.290067 || 3.290067\n",
      "step 10   || Train || 3.229404 || Val || 3.226768 || 3.226768\n",
      "step 20   || Train || 3.161818 || Val || 3.153679 || 3.153679\n",
      "step 30   || Train || 3.069992 || Val || 3.056860 || 3.056860\n",
      "step 40   || Train || 2.948184 || Val || 2.958030 || 2.958030\n",
      "step 50   || Train || 2.949450 || Val || 2.900734 || 2.900734\n",
      "step 60   || Train || 2.862980 || Val || 2.867335 || 2.867335\n",
      "step 70   || Train || 2.798518 || Val || 2.845743 || 2.845743\n",
      "step 80   || Train || 2.855532 || Val || 2.830552 || 2.830552\n",
      "step 90   || Train || 2.882640 || Val || 2.817070 || 2.817070\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'mlp': model_mlp,\n",
    "    'mlp_t': model_mlp_t,\n",
    "}\n",
    "\n",
    "# args\n",
    "n_steps = 100\n",
    "eval_every = 10\n",
    "bs = 32\n",
    "ini_lr = 0.1\n",
    "\n",
    "# train\n",
    "lossi = []\n",
    "torch.manual_seed(42)\n",
    "for  step in range(n_steps):\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "    lr = 0.1 #ini_lr if step < int(n_steps * 0.75) else ini_lr * 0.1\n",
    "\n",
    "    # --- torch ---\n",
    "    # forward\n",
    "    logits_t = model_mlp_t(x)\n",
    "    loss_t = F.cross_entropy(logits_t.view(-1, logits_t.size(-1)), y.view(-1), ignore_index=-1)\n",
    "    # backward\n",
    "    loss_t.backward()\n",
    "    # update\n",
    "    for p_t in model_mlp_t.parameters():\n",
    "        p_t.data -= lr * p_t.grad\n",
    "        p_t.grad = None\n",
    "    # --- manual ---\n",
    "    # forward\n",
    "    logits = model_mlp(x)\n",
    "    loss = loss_fn(logits, y)\n",
    "    # backward\n",
    "    grad = loss_fn.backward()\n",
    "    model_mlp.backward(grad)\n",
    "    # update\n",
    "    for p, g in zip(model_mlp.parameters(), model_mlp.grads()):\n",
    "        p.data -= lr * g\n",
    "    \n",
    "    \n",
    "    # eval\n",
    "    if step % eval_every == 0:\n",
    "        x, y = X_val, Y_val\n",
    "        with torch.no_grad():\n",
    "            logits_t = model_mlp_t(x)\n",
    "            val_loss_t = F.cross_entropy(logits_t.view(-1, logits_t.size(-1)), y.view(-1), ignore_index=-1)\n",
    "        logits = model_mlp(x)\n",
    "        val_loss = loss_fn(logits, y)\n",
    "        print(f'step {step:<4} || Train || {loss.item():.6f} || Val || {val_loss.item():.6f} || {val_loss_t.item():.6f}')\n",
    "        \n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_embd2 = config.n_embd2\n",
    "        self.start = torch.zeros(1, config.n_embd2, dtype=config.dtype) # the starting hidden state\n",
    "        self.wte = Embedding(config.vocab_size, config.n_embd, dtype=config.dtype) # token embeddings table\n",
    "        self.Cw = Linear(config.n_embd + config.n_embd2, config.n_embd2, dtype=config.dtype) # rnn cell weight\n",
    "        self.lm_head = Linear(config.n_embd2, self.vocab_size, dtype=config.dtype)\n",
    "        # grads\n",
    "        self.start_grad = None\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.start] + list(self.wte.parameters()) + list(self.Cw.parameters()) + list(self.lm_head.parameters())\n",
    "    \n",
    "    def grads(self):\n",
    "        return [self.start_grad] + list(self.wte.grads()) + list(self.Cw.grads()) + list(self.lm_head.grads())\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        b, t = x.size()\n",
    "        emb = self.wte(x) # (b, t, n_embd)\n",
    "        # sequentially iterate over the inputs and update the RNN state each tick\n",
    "        hprev = self.start.expand((b, -1)) # expand out the batch dimension\n",
    "        hiddens = []\n",
    "        emb_cat_hprevs = []\n",
    "        for i in range(t):\n",
    "            xt = emb[:, i, :] # (b, n_embd)\n",
    "            emb_i_cat_hprev = torch.cat([xt, hprev], dim=1)\n",
    "            # --- rnn cell ---\n",
    "            hi = self.Cw(emb_i_cat_hprev)\n",
    "            hi = hi.tanh()\n",
    "            # --------------\n",
    "            hprev = hi\n",
    "            hiddens.append(hi)\n",
    "            emb_cat_hprevs.append(emb_i_cat_hprev)\n",
    "        # decode the outputs\n",
    "        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n",
    "        logits = self.lm_head(hidden)\n",
    "        # backward buffer\n",
    "        self.hidden = hidden\n",
    "        self.emb_cat_hprevs = emb_cat_hprevs\n",
    "        return logits\n",
    "\n",
    "    def backward(self, grad):\n",
    "        hidden, emb_cat_hprevs = self.hidden, self.emb_cat_hprevs\n",
    "        t = hidden.size(1)\n",
    "        dhidden = self.lm_head.backward(grad)\n",
    "        # logits grad to start, wte, Cw grad\n",
    "        dembs = []\n",
    "        dCw, dhprev = 0., 0.\n",
    "        if self.Cw.bias is not None:\n",
    "            dCw_bias = 0.\n",
    "        for i in range(t-1, -1, -1):\n",
    "            # hidden state grad, emb grad\n",
    "            dhi = dhidden[:, i, :] + dhprev # grad from logits + grad from prev hidden state\n",
    "            hi = hidden[:, i, :]\n",
    "            dhi = (1 - hi**2) * dhi # grad of tanh\n",
    "            demb_i_cat_dhi = dhi @ self.Cw.weight.T\n",
    "            demb_i, dhprev = demb_i_cat_dhi.tensor_split([self.n_embd,], dim=1)\n",
    "            dembs.append(demb_i)\n",
    "            # cell weight grad\n",
    "            emb_i_cat_hprev = emb_cat_hprevs[i]\n",
    "            dCw += emb_i_cat_hprev.T @ dhi\n",
    "            if self.Cw.bias is not None:\n",
    "                dCw_bias += dhi.sum(dim=0)\n",
    "        dstart = dhprev.sum(dim=0, keepdim=True)\n",
    "        demb = torch.stack(dembs[::-1], 1)\n",
    "        self.wte.backward(demb)\n",
    "        self.start_grad = dstart\n",
    "        self.Cw.weight_grad = dCw\n",
    "        if self.Cw.bias is not None:\n",
    "            self.Cw.bias_grad = dCw_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNtorch(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_embd2 = config.n_embd2\n",
    "        self.start = nn.Parameter(torch.zeros(1, config.n_embd2)) # the starting hidden state\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd) # token embeddings table\n",
    "        self.Cw = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "        self.lm_head = nn.Linear(config.n_embd2, self.vocab_size)\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx):\n",
    "        b, t = idx.size()\n",
    "\n",
    "        # embed all the integers up front and all at once for efficiency\n",
    "        emb = self.wte(idx) # (b, t, n_embd)\n",
    "\n",
    "        # sequentially iterate over the inputs and update the RNN state each tick\n",
    "        hprev = self.start.T.expand((b, -1)) # expand out the batch dimension\n",
    "        hiddens = []\n",
    "        for i in range(t):\n",
    "            xt = emb[:, i, :] # (b, n_embd)\n",
    "            xh = torch.cat([xt, hprev], dim=1)\n",
    "            ht = (self.Cw(xh)).tanh()\n",
    "            hprev = ht\n",
    "            hiddens.append(ht)\n",
    "\n",
    "        # decode the outputs\n",
    "        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n",
    "        logits = self.lm_head(hidden)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "config = ModelConfig(block_size=16, vocab_size=27, n_layer=None, n_embd=2, n_embd2=3, n_head=None)\n",
    "# models\n",
    "torch.manual_seed(42)\n",
    "model_rnn = RNN(config)\n",
    "model_rnn_t = RNNtorch(config)\n",
    "# copy weights\n",
    "model_rnn_t.wte.weight.data = model_rnn.wte.weight.data.clone()\n",
    "for i, (p_t, p) in enumerate(zip(model_rnn_t.parameters(), model_rnn.parameters())):\n",
    "    if p.dim() == 2 and i != 1: # skip the embedding layer\n",
    "        p_t.data = p.data.clone().T # linear layer weight\n",
    "    else:\n",
    "        p_t.data = p.data.clone()\n",
    "# loss\n",
    "loss_fn = CrossEntropyLoss3d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0    || Train || 3.306277 || Val || 3.278128 || Val_t || 3.278128\n",
      "step 10   || Train || 3.254764 || Val || 3.225257 || Val_t || 3.225257\n",
      "step 20   || Train || 3.187834 || Val || 3.179277 || Val_t || 3.179277\n",
      "step 30   || Train || 3.144083 || Val || 3.136274 || Val_t || 3.136274\n",
      "step 40   || Train || 3.103379 || Val || 3.095912 || Val_t || 3.095912\n",
      "step 50   || Train || 3.091513 || Val || 3.060530 || Val_t || 3.060530\n",
      "step 60   || Train || 3.041352 || Val || 3.028438 || Val_t || 3.028438\n",
      "step 70   || Train || 2.983515 || Val || 2.997443 || Val_t || 2.997443\n",
      "step 80   || Train || 3.013324 || Val || 2.970027 || Val_t || 2.970027\n",
      "step 90   || Train || 2.978646 || Val || 2.946448 || Val_t || 2.946448\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'rnn': model_rnn,\n",
    "    'rnn_t': model_rnn_t,\n",
    "}\n",
    "\n",
    "# args\n",
    "n_steps = 100\n",
    "eval_every = 10\n",
    "bs = 32\n",
    "ini_lr = 0.1\n",
    "\n",
    "# train\n",
    "lossi = []\n",
    "torch.manual_seed(42)\n",
    "for  step in range(n_steps):\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "    lr = 0.1 #ini_lr if step < int(n_steps * 0.75) else ini_lr * 0.1\n",
    "\n",
    "    # --- torch ---\n",
    "    # forward\n",
    "    logits_t = model_rnn_t(x)\n",
    "    loss_t = F.cross_entropy(logits_t.view(-1, logits_t.size(-1)), y.view(-1), ignore_index=-1)\n",
    "    # backward\n",
    "    loss_t.backward()\n",
    "    # update\n",
    "    for p_t in model_rnn_t.parameters():\n",
    "        p_t.data -= lr * p_t.grad\n",
    "        p_t.grad = None\n",
    "    # --- manual ---\n",
    "    # forward\n",
    "    logits = model_rnn(x)\n",
    "    loss = loss_fn(logits, y)\n",
    "    # backward\n",
    "    grad = loss_fn.backward()\n",
    "    model_rnn.backward(grad)\n",
    "    # update\n",
    "    for p, g in zip(model_rnn.parameters(), model_rnn.grads()):\n",
    "        p.data -= lr * g\n",
    "    \n",
    "    \n",
    "    # eval\n",
    "    if step % eval_every == 0:\n",
    "        x, y = X_val, Y_val\n",
    "        with torch.no_grad():\n",
    "            logits_t = model_rnn_t(x)\n",
    "            val_loss_t = F.cross_entropy(logits_t.view(-1, logits_t.size(-1)), y.view(-1), ignore_index=-1)\n",
    "        logits = model_rnn(x)\n",
    "        val_loss = loss_fn(logits, y)\n",
    "        print(f'step {step:<4} || Train || {loss.item():.6f} || Val || {val_loss.item():.6f} || Val_t || {val_loss_t.item():.6f}')\n",
    "        \n",
    "    # break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
