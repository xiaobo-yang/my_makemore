{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".aukai.......... -> aukai.\n",
      ".ellanore....... -> ellanore.\n",
      ".liem........... -> liem.\n",
      ".aquarius....... -> aquarius.\n",
      ".joangel........ -> joangel.\n",
      ".wryn........... -> wryn.\n",
      ".isabela........ -> isabela.\n",
      ".astryd......... -> astryd.\n",
      ".maleik......... -> maleik.\n",
      ".emerick........ -> emerick.\n",
      ".natasha........ -> natasha.\n",
      ".kasandra....... -> kasandra.\n",
      ".aevin.......... -> aevin.\n",
      ".brason......... -> brason.\n",
      ".naiara......... -> naiara.\n",
      ".alanna......... -> alanna.\n",
      ".raunak......... -> raunak.\n",
      ".gohan.......... -> gohan.\n",
      ".ivie........... -> ivie.\n",
      ".alandis........ -> alandis.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([23595, 16]),\n",
       " torch.Size([2949, 16]),\n",
       " torch.Size([2950, 16]),\n",
       " torch.Size([23595, 16]),\n",
       " torch.Size([2949, 16]),\n",
       " torch.Size([2950, 16]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words = sorted(list(set(words))) # set cause uncontrollable randomnessï¼Œ sorted for reproducibility\n",
    "max_len = max(len(w) for w in words)\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "chs = list(set(''.join(words + ['.']))) # add special token\n",
    "chs = sorted(chs, reverse=False)\n",
    "stoi = {ch: i for i, ch in enumerate(chs)}\n",
    "itos = {i: ch for i, ch in enumerate(chs)}\n",
    "\n",
    "# predict next token use previous tokens\n",
    "vocab_size = len(chs)\n",
    "block_size = max_len + 1\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    x = torch.zeros(max_len + 1, dtype=torch.long)\n",
    "    y = torch.zeros(max_len + 1, dtype=torch.long)\n",
    "    x[1:1+len(w)] = torch.tensor([stoi[ch] for ch in w])\n",
    "    y[:len(w)] = torch.tensor([stoi[ch] for ch in w])\n",
    "    y[len(w)+1:] = -1 # mask the loss at the inactive locations\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "\n",
    "X = torch.stack(X)\n",
    "Y = torch.stack(Y)\n",
    "n1, n2  = int(0.8 * len(X)), int(0.9 * len(X))\n",
    "\n",
    "X_train, X_val, X_test = X.tensor_split([n1, n2])\n",
    "Y_train, Y_val, Y_test = Y.tensor_split([n1, n2])\n",
    "\n",
    "show = 20\n",
    "for x, y in zip(X_train[:show], Y_train[:show]):\n",
    "    sx = ''.join(itos[i.item()] for i in x)\n",
    "    sy = ''.join(itos[i.item()] for i in y if i.item() != -1)\n",
    "    print(f'{sx} -> {sy}')\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# move to manual backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# manual backprop\n",
    "from tiny_torch import *\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    block_size: int = block_size # length of the input sequences of integers\n",
    "    vocab_size: int = vocab_size # the input integers are in range [0 .. vocab_size -1]\n",
    "    # parameters below control the sizes of each model slightly differently\n",
    "    n_layer: int = 4\n",
    "    n_embd: int = 64\n",
    "    n_embd2: int = 64\n",
    "    n_head: int = 4\n",
    "    dtype: torch.dtype = torch.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_compare(model, model_t, lr=0.1, n_steps=100, eval_every=10, bs=32):\n",
    "    loss_fn = CrossEntropyLoss3d()\n",
    "    # train\n",
    "    torch.manual_seed(42)\n",
    "    for  step in range(n_steps):\n",
    "        idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "        x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "        # --- torch ---\n",
    "        # forward\n",
    "        logits_t = model_t(x)\n",
    "        loss_t = F.cross_entropy(logits_t.view(-1, logits_t.size(-1)), y.view(-1), ignore_index=-1)\n",
    "        # backward\n",
    "        loss_t.backward()\n",
    "        # update\n",
    "        for p_t in model_t.parameters():\n",
    "            p_t.data -= lr * p_t.grad\n",
    "            p_t.grad = None\n",
    "        # --- manual ---\n",
    "        # forward\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        # backward\n",
    "        dlogits = loss_fn.backward()\n",
    "        model.backward(dlogits)\n",
    "        # update\n",
    "        for p, g in zip(model.parameters(), model.grads()):\n",
    "            p.data -= lr * g\n",
    "        \n",
    "        \n",
    "        # eval\n",
    "        if step % eval_every == 0:\n",
    "            x, y = X_val, Y_val\n",
    "            with torch.no_grad():\n",
    "                logits_t = model_t(x)\n",
    "                val_loss_t = F.cross_entropy(logits_t.view(-1, logits_t.size(-1)), y.view(-1), ignore_index=-1)\n",
    "            logits = model(x)\n",
    "            val_loss = loss_fn(logits, y)\n",
    "            print(f'step {step:<8} || Train   || {loss.item():.15f} || Val   || {val_loss.item():.15f}')\n",
    "            print(f'              || Train_t || {loss_t.item():.15f} || Val_t || {val_loss_t.item():.15f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    \"\"\"\n",
    "    takes the previous block_size tokens, encodes them with a lookup table,\n",
    "    concatenates the vectors and predicts the next token with an MLP.\n",
    "\n",
    "    Reference:\n",
    "    Bengio et al. 2003 https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.wte = Embedding(config.vocab_size + 1, config.n_embd, dtype=config.dtype) # token embeddings table\n",
    "        # +1 in the line above for a special <BLANK> token that gets inserted if encoding a token\n",
    "        # before the beginning of the input sequence\n",
    "        self.mlp = Sequential([\n",
    "            Linear(self.block_size * config.n_embd, config.n_embd2, dtype=config.dtype),\n",
    "            Tanh(),\n",
    "            Linear(config.n_embd2, self.vocab_size, dtype=config.dtype)\n",
    "        ])\n",
    "        self.mlp[-1].weight.data *= 0.1\n",
    "        self.mlp[-1].bias.data *= 0.01\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of mlp parameters: %d\" % (n_params,))\n",
    "        self.config = config\n",
    "    \n",
    "    def parameters(self):\n",
    "        return list(self.wte.parameters()) + list(self.mlp.parameters())\n",
    "    \n",
    "    def grads(self):\n",
    "        return list(self.wte.grads()) + list(self.mlp.grads())\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def __call__(self, idx, targets=None):\n",
    "\n",
    "        # gather the word embeddings of the previous 3 words\n",
    "        idx_buf = []\n",
    "        embs = []\n",
    "        for k in range(self.block_size):\n",
    "            tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "            idx_buf.append(idx.unsqueeze(-1))\n",
    "            embs.append(tok_emb)\n",
    "            idx = torch.roll(idx, 1, 1)\n",
    "            idx[:, 0] = self.vocab_size # special <BLANK> token\n",
    "\n",
    "        # concat all of the embeddings together and pass through an MLP\n",
    "        x = torch.cat(embs, -1) # (b, t, n_embd * block_size)\n",
    "        logits = self.mlp(x)\n",
    "\n",
    "        # backward buffer\n",
    "        self.idx_buf = torch.cat(idx_buf, -1) # (b, t, t)\n",
    "\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        grad = self.mlp.backward(grad)\n",
    "        # mlp backprop to wte\n",
    "        b, t, _ = grad.shape # (b, t, n_embd * block_size)\n",
    "        grad = grad.view(b * t * self.config.block_size, self.config.n_embd) # (b*t*block_size, n_embd)\n",
    "        wte_weight = self.wte.weight\n",
    "        wte_grad = torch.zeros_like(wte_weight)\n",
    "        wte_grad.index_add_(dim=0, index=self.idx_buf.view(-1), source=grad)\n",
    "        self.wte.weight_grad = wte_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPtorch(nn.Module):\n",
    "    \"\"\"\n",
    "    takes the previous block_size tokens, encodes them with a lookup table,\n",
    "    concatenates the vectors and predicts the next token with an MLP.\n",
    "\n",
    "    Reference:\n",
    "    Bengio et al. 2003 https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.wte = nn.Embedding(config.vocab_size + 1, config.n_embd) # token embeddings table\n",
    "        # +1 in the line above for a special <BLANK> token that gets inserted if encoding a token\n",
    "        # before the beginning of the input sequence\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.block_size * config.n_embd, config.n_embd2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(config.n_embd2, self.vocab_size)\n",
    "        )\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of mlp parameters: %d\" % (n_params,))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # gather the word embeddings of the previous 3 words\n",
    "        embs = []\n",
    "        for k in range(self.block_size):\n",
    "            tok_emb = self.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "            idx = torch.roll(idx, 1, 1)\n",
    "            idx[:, 0] = self.vocab_size # special <BLANK> token\n",
    "            embs.append(tok_emb)\n",
    "\n",
    "        # concat all of the embeddings together and pass through an MLP\n",
    "        x = torch.cat(embs, -1) # (b, t, n_embd * block_size)\n",
    "        logits = self.mlp(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mlp parameters: 3995\n",
      "number of mlp parameters: 3995\n",
      "step 0        || Train   || 3.297640663881704 || Val   || 3.267504153916392\n",
      "              || Train_t || 3.297640663881704 || Val_t || 3.267504153916391\n",
      "step 10       || Train   || 3.024899204673497 || Val   || 3.022521506732622\n",
      "              || Train_t || 3.024899204673497 || Val_t || 3.022521506732622\n",
      "step 20       || Train   || 2.910541702548877 || Val   || 2.903174425168292\n",
      "              || Train_t || 2.910541702548878 || Val_t || 2.903174425168292\n",
      "step 30       || Train   || 2.862459630906429 || Val   || 2.846881946272189\n",
      "              || Train_t || 2.862459630906429 || Val_t || 2.846881946272189\n",
      "step 40       || Train   || 2.788301334625511 || Val   || 2.806272830481473\n",
      "              || Train_t || 2.788301334625510 || Val_t || 2.806272830481473\n",
      "step 50       || Train   || 2.816853557882264 || Val   || 2.773501814770002\n",
      "              || Train_t || 2.816853557882264 || Val_t || 2.773501814770002\n",
      "step 60       || Train   || 2.715939249560932 || Val   || 2.740654505262065\n",
      "              || Train_t || 2.715939249560933 || Val_t || 2.740654505262065\n",
      "step 70       || Train   || 2.660503885145936 || Val   || 2.711401752196980\n",
      "              || Train_t || 2.660503885145936 || Val_t || 2.711401752196980\n",
      "step 80       || Train   || 2.723541077242827 || Val   || 2.684838960326417\n",
      "              || Train_t || 2.723541077242828 || Val_t || 2.684838960326417\n",
      "step 90       || Train   || 2.756579343442730 || Val   || 2.665377540141806\n",
      "              || Train_t || 2.756579343442730 || Val_t || 2.665377540141806\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(n_embd=8, n_embd2=24)\n",
    "# models\n",
    "torch.manual_seed(42)\n",
    "model_mlp = MLP(config)\n",
    "model_mlp_t = MLPtorch(config)\n",
    "# copy weights\n",
    "model_mlp_t.wte.weight.data = model_mlp.wte.weight.data.clone()\n",
    "for i, (p_t, p) in enumerate(zip(model_mlp_t.mlp.parameters(), model_mlp.mlp.parameters())):\n",
    "    if p.dim() == 2:\n",
    "        p_t.data = p.data.clone().T # linear layer weight\n",
    "    else:\n",
    "        p_t.data = p.data.clone()\n",
    "\n",
    "\n",
    "train_compare(model_mlp, model_mlp_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_embd2 = config.n_embd2\n",
    "        self.start = torch.zeros(1, config.n_embd2, dtype=config.dtype) # the starting hidden state\n",
    "        self.wte = Embedding(config.vocab_size, config.n_embd, dtype=config.dtype) # token embeddings table\n",
    "        self.Cw = Linear(config.n_embd + config.n_embd2, config.n_embd2, dtype=config.dtype) # rnn cell weight\n",
    "        self.lm_head = Linear(config.n_embd2, self.vocab_size, dtype=config.dtype)\n",
    "        num_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of rnn parameters: %d\" % (num_params,))\n",
    "        # grads\n",
    "        self.start_grad = None\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.start] + list(self.wte.parameters()) + list(self.Cw.parameters()) + list(self.lm_head.parameters())\n",
    "    \n",
    "    def grads(self):\n",
    "        return [self.start_grad] + list(self.wte.grads()) + list(self.Cw.grads()) + list(self.lm_head.grads())\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        b, t = x.size()\n",
    "        emb = self.wte(x) # (b, t, n_embd)\n",
    "        # sequentially iterate over the inputs and update the RNN state each tick\n",
    "        hprev = self.start.expand((b, -1)) # expand out the batch dimension\n",
    "        hiddens = []\n",
    "        emb_cat_hprevs = []\n",
    "        for i in range(t):\n",
    "            xt = emb[:, i, :] # (b, n_embd)\n",
    "            emb_i_cat_hprev = torch.cat([xt, hprev], dim=1)\n",
    "            # --- rnn cell ---\n",
    "            hi = self.Cw(emb_i_cat_hprev)\n",
    "            hi = hi.tanh()\n",
    "            # --------------\n",
    "            hprev = hi\n",
    "            hiddens.append(hi)\n",
    "            emb_cat_hprevs.append(emb_i_cat_hprev)\n",
    "        # decode the outputs\n",
    "        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n",
    "        logits = self.lm_head(hidden)\n",
    "        # backward buffer\n",
    "        self.hidden = hidden\n",
    "        self.emb_cat_hprevs = emb_cat_hprevs\n",
    "        return logits\n",
    "\n",
    "    def backward(self, grad):\n",
    "        hidden, emb_cat_hprevs = self.hidden, self.emb_cat_hprevs\n",
    "        t = hidden.size(1)\n",
    "        dhidden = self.lm_head.backward(grad)\n",
    "        # logits grad to start, wte, Cw grad\n",
    "        dembs = []\n",
    "        dCw, dhprev = 0., 0.\n",
    "        if self.Cw.bias is not None:\n",
    "            dCw_bias = 0.\n",
    "        for i in range(t-1, -1, -1):\n",
    "            # hidden state grad, emb grad\n",
    "            dhi = dhidden[:, i, :] + dhprev # grad from logits + grad from prev hidden state\n",
    "            hi = hidden[:, i, :]\n",
    "            dhi = (1 - hi**2) * dhi # grad of tanh\n",
    "            demb_i_cat_dhi = dhi @ self.Cw.weight.T\n",
    "            demb_i, dhprev = demb_i_cat_dhi.tensor_split([self.n_embd,], dim=1)\n",
    "            dembs.append(demb_i)\n",
    "            # cell weight grad\n",
    "            emb_i_cat_hprev = emb_cat_hprevs[i]\n",
    "            dCw += emb_i_cat_hprev.T @ dhi\n",
    "            if self.Cw.bias is not None:\n",
    "                dCw_bias += dhi.sum(dim=0)\n",
    "        dstart = dhprev.sum(dim=0, keepdim=True)\n",
    "        demb = torch.stack(dembs[::-1], 1)\n",
    "        self.wte.backward(demb)\n",
    "        self.start_grad = dstart\n",
    "        self.Cw.weight_grad = dCw\n",
    "        if self.Cw.bias is not None:\n",
    "            self.Cw.bias_grad = dCw_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNtorch(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_embd2 = config.n_embd2\n",
    "        self.start = nn.Parameter(torch.zeros(1, config.n_embd2)) # the starting hidden state\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd) # token embeddings table\n",
    "        self.Cw = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "        self.lm_head = nn.Linear(config.n_embd2, self.vocab_size)\n",
    "        num_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of rnn parameters: %d\" % (num_params,))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx):\n",
    "        b, t = idx.size()\n",
    "\n",
    "        # embed all the integers up front and all at once for efficiency\n",
    "        emb = self.wte(idx) # (b, t, n_embd)\n",
    "\n",
    "        # sequentially iterate over the inputs and update the RNN state each tick\n",
    "        hprev = self.start.T.expand((b, -1)) # expand out the batch dimension\n",
    "        hiddens = []\n",
    "        for i in range(t):\n",
    "            xt = emb[:, i, :] # (b, n_embd)\n",
    "            xh = torch.cat([xt, hprev], dim=1)\n",
    "            ht = (self.Cw(xh)).tanh()\n",
    "            hprev = ht\n",
    "            hiddens.append(ht)\n",
    "\n",
    "        # decode the outputs\n",
    "        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n",
    "        logits = self.lm_head(hidden)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of rnn parameters: 3807\n",
      "number of rnn parameters: 3807\n",
      "step 0        || Train   || 3.362432962612295 || Val   || 3.329043391170546\n",
      "              || Train_t || 3.362432962612295 || Val_t || 3.329043391170547\n",
      "step 10       || Train   || 3.171036656791207 || Val   || 3.114081619871966\n",
      "              || Train_t || 3.171036656791206 || Val_t || 3.114081619871965\n",
      "step 20       || Train   || 2.921689018592812 || Val   || 2.918088278837589\n",
      "              || Train_t || 2.921689018592812 || Val_t || 2.918088278837589\n",
      "step 30       || Train   || 2.807653806375523 || Val   || 2.797327611978270\n",
      "              || Train_t || 2.807653806375522 || Val_t || 2.797327611978270\n",
      "step 40       || Train   || 2.707653574650008 || Val   || 2.711660180532699\n",
      "              || Train_t || 2.707653574650008 || Val_t || 2.711660180532699\n",
      "step 50       || Train   || 2.712063772179941 || Val   || 2.660885418532094\n",
      "              || Train_t || 2.712063772179941 || Val_t || 2.660885418532094\n",
      "step 60       || Train   || 2.629152563651833 || Val   || 2.624115333524215\n",
      "              || Train_t || 2.629152563651833 || Val_t || 2.624115333524215\n",
      "step 70       || Train   || 2.583538394485787 || Val   || 2.595689170497225\n",
      "              || Train_t || 2.583538394485787 || Val_t || 2.595689170497226\n",
      "step 80       || Train   || 2.620622058670813 || Val   || 2.579475046084394\n",
      "              || Train_t || 2.620622058670813 || Val_t || 2.579475046084394\n",
      "step 90       || Train   || 2.612400860524740 || Val   || 2.563765586153120\n",
      "              || Train_t || 2.612400860524739 || Val_t || 2.563765586153121\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(n_embd=8, n_embd2=44)\n",
    "# models\n",
    "torch.manual_seed(42)\n",
    "model_rnn = RNN(config)\n",
    "model_rnn_t = RNNtorch(config)\n",
    "# copy weights\n",
    "model_rnn_t.wte.weight.data = model_rnn.wte.weight.data.clone()\n",
    "for i, (p_t, p) in enumerate(zip(model_rnn_t.parameters(), model_rnn.parameters())):\n",
    "    if p.dim() == 2 and i != 1: # skip the embedding layer\n",
    "        p_t.data = p.data.clone().T # linear layer weight\n",
    "    else:\n",
    "        p_t.data = p.data.clone()\n",
    "\n",
    "train_compare(model_rnn, model_rnn_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.n_embd = config.n_embd\n",
    "        self.n_embd2 = config.n_embd2\n",
    "        self.start = torch.zeros(1, config.n_embd2, dtype=config.dtype) # the starting hidden state\n",
    "        self.wte = Embedding(config.vocab_size, config.n_embd, dtype=config.dtype) # token embeddings table\n",
    "        self.Cr = Linear(config.n_embd + config.n_embd2, config.n_embd2, dtype=config.dtype)\n",
    "        self.Cbar = Linear(config.n_embd + config.n_embd2, config.n_embd2, dtype=config.dtype)\n",
    "        self.Cz = Linear(config.n_embd + config.n_embd2, config.n_embd2, dtype=config.dtype)\n",
    "        self.lm_head = Linear(config.n_embd2, self.vocab_size, dtype=config.dtype)\n",
    "        num_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of gru parameters: %d\" % (num_params,))\n",
    "        # grads\n",
    "        self.start_grad = None\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.start] + list(self.wte.parameters()) + list(self.Cr.parameters()) + list(self.Cbar.parameters()) + list(self.Cz.parameters()) + list(self.lm_head.parameters())\n",
    "    \n",
    "    def grads(self):\n",
    "        return [self.start_grad] + list(self.wte.grads()) + list(self.Cr.grads()) + list(self.Cbar.grads()) + list(self.Cz.grads()) + list(self.lm_head.grads())\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def __call__(self, x):\n",
    "        b, t = x.size()\n",
    "        emb = self.wte(x) # (b, t, n_embd)\n",
    "        # sequentially iterate over the inputs and update the RNN state each tick\n",
    "        hprev = self.start.expand((b, -1)) # expand out the batch dimension\n",
    "        hiddens = []\n",
    "        emb_cat_hprevs, emb_cat_hprev_resets, hprevs, hbars, zs, rs = [], [], [], [], [], []\n",
    "        for i in range(t):\n",
    "            emb_i = emb[:, i, :] # (b, n_embd)\n",
    "            # --- gru cell ---\n",
    "            emb_i_cat_hprev = torch.cat([emb_i, hprev], dim=1)\n",
    "            ri = self.Cr(emb_i_cat_hprev)\n",
    "            ri = ri.sigmoid()\n",
    "            hprev_reset = ri * hprev\n",
    "            emb_i_cat_hprev_reset = torch.cat([emb_i, hprev_reset], dim=1)\n",
    "            hbar = self.Cbar(emb_i_cat_hprev_reset)\n",
    "            hbar = hbar.tanh()\n",
    "            zi = self.Cz(emb_i_cat_hprev)\n",
    "            zi = zi.sigmoid()\n",
    "            hi = (1 - zi) * hprev + zi * hbar\n",
    "            # backward buffer\n",
    "            hiddens.append(hi)\n",
    "            emb_cat_hprevs.append(emb_i_cat_hprev)\n",
    "            emb_cat_hprev_resets.append(emb_i_cat_hprev_reset)\n",
    "            hprevs.append(hprev)\n",
    "            hbars.append(hbar)\n",
    "            zs.append(zi)\n",
    "            rs.append(ri)\n",
    "            # update hprev\n",
    "            hprev = hi\n",
    "        # decode the outputs\n",
    "        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n",
    "        logits = self.lm_head(hidden)\n",
    "        # backward buffer\n",
    "        self.hidden = hidden\n",
    "        self.emb_cat_hprevs = emb_cat_hprevs\n",
    "        self.emb_cat_hprev_resets = emb_cat_hprev_resets\n",
    "        self.hprevs = hprevs\n",
    "        self.hbars = hbars\n",
    "        self.zs = zs\n",
    "        self.rs = rs\n",
    "        return logits\n",
    "\n",
    "    def backward(self, dlogits):\n",
    "        hidden, emb_cat_hprevs, emb_cat_hprev_resets, hprevs, hbars, zs, rs = self.hidden, self.emb_cat_hprevs, self.emb_cat_hprev_resets, self.hprevs, self.hbars, self.zs, self.rs\n",
    "        t = hidden.size(1)\n",
    "        dhidden = self.lm_head.backward(dlogits)\n",
    "        # logits grad to start, wte, Cw grad\n",
    "        dembs = []\n",
    "        dCr, dCbar, dCz, dhprev = 0., 0., 0., 0.\n",
    "        if self.Cr.bias is not None:\n",
    "            dCr_bias = 0.\n",
    "        if self.Cbar.bias is not None:\n",
    "            dCbar_bias = 0.\n",
    "        if self.Cz.bias is not None:\n",
    "            dCz_bias = 0.\n",
    "        for i in range(t-1, -1, -1):\n",
    "            # hidden state grad, emb grad\n",
    "            dhi = dhidden[:, i, :] + dhprev # grad from logits + grad from prev hidden state\n",
    "            dhbar = dhi * zs[i]\n",
    "            dhprev = dhi * (1 - zs[i])\n",
    "            dzi = dhi * (hbars[i] - hprevs[i])\n",
    "            dzi = dzi * (1 - zs[i]) * zs[i]\n",
    "            demb_i_cat_hprev = dzi @ self.Cz.weight.T\n",
    "            dCz += emb_cat_hprevs[i].T @ dzi\n",
    "            if self.Cz.bias is not None:\n",
    "                dCz_bias += dzi.sum(dim=0)\n",
    "\n",
    "            dhbar = dhbar * (1 - hbars[i]**2)\n",
    "            demb_i_cat_hprev_reset = dhbar @ self.Cbar.weight.T\n",
    "            dCbar += emb_cat_hprev_resets[i].T @ dhbar\n",
    "            if self.Cbar.bias is not None:\n",
    "                dCbar_bias += dhbar.sum(dim=0)\n",
    "\n",
    "            demb_i, dhprev_reset = demb_i_cat_hprev_reset.tensor_split([self.n_embd,], dim=1)\n",
    "            dri = dhprev_reset * hprevs[i]\n",
    "            dhprev += dhprev_reset * rs[i]\n",
    "            dri = dri * (1 - rs[i]) * rs[i]\n",
    "            demb_i_cat_hprev += dri @ self.Cr.weight.T\n",
    "            dCr += emb_cat_hprevs[i].T @ dri\n",
    "            if self.Cr.bias is not None:\n",
    "                dCr_bias += dri.sum(dim=0)\n",
    "            demb_more, dhprev_more = demb_i_cat_hprev.tensor_split([self.n_embd,], dim=1)\n",
    "            demb_i += demb_more\n",
    "            dhprev += dhprev_more\n",
    "            dembs.append(demb_i)\n",
    "        dstart = dhprev.sum(dim=0, keepdim=True)\n",
    "        demb = torch.stack(dembs[::-1], 1)\n",
    "        self.wte.backward(demb)\n",
    "        self.start_grad = dstart\n",
    "        self.Cr.weight_grad = dCr\n",
    "        if self.Cr.bias is not None:\n",
    "            self.Cr.bias_grad = dCr_bias\n",
    "        self.Cbar.weight_grad = dCbar\n",
    "        if self.Cbar.bias is not None:\n",
    "            self.Cbar.bias_grad = dCbar_bias\n",
    "        self.Cz.weight_grad = dCz\n",
    "        if self.Cz.bias is not None:\n",
    "            self.Cz.bias_grad = dCz_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUtorch(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.start = nn.Parameter(torch.zeros(1, config.n_embd2, dtype=config.dtype)) # the starting hidden state\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd) # token embeddings table\n",
    "        self.Cr = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "        self.Cbar = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "        self.Cz = nn.Linear(config.n_embd + config.n_embd2, config.n_embd2)\n",
    "        self.lm_head = nn.Linear(config.n_embd2, self.vocab_size)\n",
    "        num_params = sum(p.numel() for p in self.parameters())\n",
    "        print(\"number of gru parameters: %d\" % (num_params,))\n",
    "\n",
    "    def get_block_size(self):\n",
    "        return self.block_size\n",
    "\n",
    "    def forward(self, idx):\n",
    "        b, t = idx.size()\n",
    "        emb = self.wte(idx) # (b, t, n_embd)\n",
    "        hprev = self.start.T.expand((b, -1)) # expand out the batch dimension\n",
    "        hiddens = []\n",
    "        for i in range(t):\n",
    "            xt = emb[:, i, :] # (b, n_embd)\n",
    "            # --- gru cell ---\n",
    "            xh = torch.cat([xt, hprev], dim=1)\n",
    "            r = (self.Cr(xh)).sigmoid()\n",
    "            hprev_reset = r * hprev\n",
    "            xhr = torch.cat([xt, hprev_reset], dim=1)\n",
    "            hbar = (self.Cbar(xhr)).tanh()\n",
    "            z = (self.Cz(xh)).sigmoid()\n",
    "            z = (self.Cz(xh)).sigmoid()\n",
    "            ht = (1 - z) * hprev + z * hbar\n",
    "            # --------------\n",
    "            hprev = ht\n",
    "            hiddens.append(ht)\n",
    "\n",
    "        # decode the outputs\n",
    "        hidden = torch.stack(hiddens, 1) # (b, t, n_embd2)\n",
    "        logits = self.lm_head(hidden)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of gru parameters: 3915\n",
      "number of gru parameters: 3915\n",
      "step 0        || Train   || 3.361403145058750 || Val   || 3.352860787255950\n",
      "              || Train_t || 3.361403145058750 || Val_t || 3.352860787255950\n",
      "step 10       || Train   || 3.269461320582565 || Val   || 3.251121526050696\n",
      "              || Train_t || 3.269461320582565 || Val_t || 3.251121526050696\n",
      "step 20       || Train   || 3.178847946129618 || Val   || 3.166952995381364\n",
      "              || Train_t || 3.178847946129618 || Val_t || 3.166952995381364\n",
      "step 30       || Train   || 3.107527022687791 || Val   || 3.088581327911588\n",
      "              || Train_t || 3.107527022687791 || Val_t || 3.088581327911588\n",
      "step 40       || Train   || 3.039824486713404 || Val   || 3.015034751629193\n",
      "              || Train_t || 3.039824486713405 || Val_t || 3.015034751629194\n",
      "step 50       || Train   || 2.988462129260083 || Val   || 2.953215323540391\n",
      "              || Train_t || 2.988462129260084 || Val_t || 2.953215323540391\n",
      "step 60       || Train   || 2.931377000481202 || Val   || 2.901129118936024\n",
      "              || Train_t || 2.931377000481201 || Val_t || 2.901129118936024\n",
      "step 70       || Train   || 2.843199489923497 || Val   || 2.853368275902411\n",
      "              || Train_t || 2.843199489923497 || Val_t || 2.853368275902412\n",
      "step 80       || Train   || 2.841994397868837 || Val   || 2.815843328564174\n",
      "              || Train_t || 2.841994397868836 || Val_t || 2.815843328564174\n",
      "step 90       || Train   || 2.817380190868578 || Val   || 2.785673505789270\n",
      "              || Train_t || 2.817380190868579 || Val_t || 2.785673505789270\n"
     ]
    }
   ],
   "source": [
    "config = ModelConfig(n_embd=8, n_embd2=27)\n",
    "# models\n",
    "torch.manual_seed(42)\n",
    "model_gru = GRU(config)\n",
    "model_gru_t = GRUtorch(config)\n",
    "# copy weights\n",
    "model_gru_t.wte.weight.data = model_gru.wte.weight.data.clone()\n",
    "for i, (p_t, p) in enumerate(zip(model_gru_t.parameters(), model_gru.parameters())):\n",
    "    if p.dim() == 2 and i != 1: # skip the embedding layer\n",
    "        p_t.data = p.data.clone().T # linear layer weight\n",
    "    else:\n",
    "        p_t.data = p.data.clone()\n",
    "\n",
    "train_compare(model_gru, model_gru_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
