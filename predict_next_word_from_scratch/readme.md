# Predict Next Word from Scratch ğŸš€

Ever wondered how GPT predicts the next word? Let's build one from the ground up! This project is your guided tour through the evolution of language models - from the simplest bigram to the mighty transformer.

## What's Inside?

We'll journey through increasingly sophisticated models, implementing each one from scratch:

- ğŸ¯ Bigram Model - The simplest way to predict text
- ğŸ§  Multi-Layer Perceptron (MLP) - Adding some neural magic
- â†©ï¸ RNN - Teaching our model to remember
- ğŸ”„ GRU - A smarter way to handle sequences
- âš¡ Transformer - The architecture that revolutionized NLP

All models are trained on a vocabulary of English words using `tiny_torch` - our minimalist PyTorch implementation that helps understand the fundamentals.

## Why From Scratch?

Building these models from scratch isn't just an exercise - it's the best way to truly understand how modern language models work. Each component is implemented with clear, educational code that prioritizes learning over efficiency.

## Acknowledgement

This project is largely inspired by Andrej Karpathy's excellent [makemore](https://github.com/karpathy/makemore) project. I add more models and more backpropagation components to the project.
