# Predict Next Word from Scratch 🚀

Ever wondered how GPT predicts the next word? Let's build one from the ground up! This project is your guided tour through the evolution of language models - from the simplest bigram to the mighty transformer.

## What's Inside?

We'll journey through increasingly sophisticated models, implementing each one from scratch:

- 🎯 Bigram Model - The simplest way to predict text
- 🧠 Multi-Layer Perceptron (MLP) - Adding some neural magic
- ↩️ RNN - Teaching our model to remember
- 🔄 GRU - A smarter way to handle sequences
- ⚡ Transformer - The architecture that revolutionized NLP

All models are trained on a vocabulary of English words using `tiny_torch` - our minimalist PyTorch implementation that helps understand the fundamentals.

## Why From Scratch?

Building these models from scratch isn't just an exercise - it's the best way to truly understand how modern language models work. Each component is implemented with clear, educational code that prioritizes learning over efficiency.

## Acknowledgement

This project is largely inspired by Andrej Karpathy's excellent [makemore](https://github.com/karpathy/makemore) project. I add more models and more backpropagation components to the project.
