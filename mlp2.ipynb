{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([169062, 3]),\n",
       " torch.Size([21133, 3]),\n",
       " torch.Size([21133, 3]),\n",
       " torch.Size([169062]),\n",
       " torch.Size([21133]),\n",
       " torch.Size([21133]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words = list(set(words))\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "len(words)\n",
    "\n",
    "chs = list(set(''.join(words + ['.'])))\n",
    "chs = sorted(chs, reverse=False)\n",
    "stoi = {ch: i for i, ch in enumerate(chs)}\n",
    "itos = {i: ch for i, ch in enumerate(chs)}\n",
    "\n",
    "# predict next token use previous 3 tokens\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    context = '...'\n",
    "    for ch in w + '.':\n",
    "        x = [stoi[c] for c in context]\n",
    "        y = stoi[ch]\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        context = context[1:] + ch\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "n1, n2  = int(0.8 * len(X)), int(0.9 * len(X))\n",
    "\n",
    "X_train, X_val, X_test = X.tensor_split([n1, n2])\n",
    "Y_train, Y_val, Y_test = Y.tensor_split([n1, n2])\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalize the initial parameters\n",
    "otherwise the initial loss will be too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 24.335227966308594, val loss: 26.72002410888672\n",
      "step: 5000, train loss: 2.751279592514038, val loss: 2.7108864784240723\n",
      "step: 10000, train loss: 2.34926176071167, val loss: 2.532747507095337\n",
      "step: 15000, train loss: 2.5620970726013184, val loss: 2.4622185230255127\n",
      "step: 20000, train loss: 2.406903028488159, val loss: 2.406139373779297\n",
      "step: 25000, train loss: 2.750678539276123, val loss: 2.4181902408599854\n",
      "step: 30000, train loss: 2.4322433471679688, val loss: 2.4484927654266357\n",
      "step: 35000, train loss: 2.6349072456359863, val loss: 2.420732259750366\n",
      "step: 40000, train loss: 1.9264978170394897, val loss: 2.3425233364105225\n",
      "step: 45000, train loss: 1.9710713624954224, val loss: 2.338916778564453\n",
      "step: 50000, train loss: 2.170379638671875, val loss: 2.339043378829956\n",
      "step: 55000, train loss: 2.1003916263580322, val loss: 2.3541834354400635\n",
      "step: 60000, train loss: 3.1957919597625732, val loss: 2.3791303634643555\n",
      "step: 65000, train loss: 2.4687180519104004, val loss: 2.3697609901428223\n",
      "step: 70000, train loss: 2.284069776535034, val loss: 2.334017276763916\n",
      "step: 75000, train loss: 2.6321544647216797, val loss: 2.2971699237823486\n",
      "step: 80000, train loss: 1.7960376739501953, val loss: 2.2930352687835693\n",
      "step: 85000, train loss: 2.218618392944336, val loss: 2.3009588718414307\n",
      "step: 90000, train loss: 2.3473217487335205, val loss: 2.3564727306365967\n",
      "step: 95000, train loss: 2.323827028274536, val loss: 2.2622787952423096\n",
      "step: 100000, train loss: 2.501126766204834, val loss: 2.3087546825408936\n",
      "step: 105000, train loss: 2.0556399822235107, val loss: 2.168558359146118\n",
      "step: 110000, train loss: 1.903432011604309, val loss: 2.1751787662506104\n",
      "step: 115000, train loss: 2.1213340759277344, val loss: 2.1662724018096924\n",
      "step: 120000, train loss: 2.084320545196533, val loss: 2.165789842605591\n",
      "step: 125000, train loss: 2.5746524333953857, val loss: 2.1677780151367188\n",
      "step: 130000, train loss: 2.3753843307495117, val loss: 2.164478063583374\n",
      "step: 135000, train loss: 2.0631203651428223, val loss: 2.169764518737793\n",
      "step: 140000, train loss: 2.310573101043701, val loss: 2.1620399951934814\n",
      "step: 145000, train loss: 2.2068419456481934, val loss: 2.1657140254974365\n",
      "step: 150000, train loss: 2.248950242996216, val loss: 2.1653404235839844\n",
      "step: 155000, train loss: 1.8470402956008911, val loss: 2.166043519973755\n",
      "step: 160000, train loss: 1.8275150060653687, val loss: 2.165815830230713\n",
      "step: 165000, train loss: 2.543524742126465, val loss: 2.1649084091186523\n",
      "step: 170000, train loss: 2.13118052482605, val loss: 2.1633546352386475\n",
      "step: 175000, train loss: 2.0244877338409424, val loss: 2.1660172939300537\n",
      "step: 180000, train loss: 1.686617374420166, val loss: 2.165832996368408\n",
      "step: 185000, train loss: 2.2652363777160645, val loss: 2.1678524017333984\n",
      "step: 190000, train loss: 2.1622302532196045, val loss: 2.1618118286132812\n",
      "step: 195000, train loss: 1.7642427682876587, val loss: 2.163745403289795\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "torch.manual_seed(42)\n",
    "C = torch.randn(27, n_embd)\n",
    "w1 = torch.randn(n_embd * 3, n_hidden)\n",
    "b1 = torch.randn(n_hidden)\n",
    "w2 = torch.randn(n_hidden, 27)\n",
    "b2 = torch.randn(27)\n",
    "params = [C, w1, b1, w2, b2]\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "bs = 32\n",
    "n_steps = 200000\n",
    "\n",
    "\n",
    "for step in range(n_steps):\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x = X_train[idx]\n",
    "    y = Y_train[idx]\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    logits = torch.tanh(emb @ w1 + b1) @ w2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    if step % 5000 == 0:\n",
    "        with torch.no_grad():\n",
    "            emb = C[X_val].view(X_val.shape[0], -1)\n",
    "            logits = torch.tanh(emb @ w1 + b1) @ w2 + b2\n",
    "            val_loss = F.cross_entropy(logits, Y_val)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    loss.backward()\n",
    "    lr = 0.1 if step < 100000 else 0.01\n",
    "    for p in params:\n",
    "        p.data -= lr * p.grad\n",
    "        p.grad = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1569)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    emb = C[X_test].view(X_test.shape[0], -1)\n",
    "    logits = torch.tanh(emb @ w1 + b1) @ w2 + b2\n",
    "    val_loss = F.cross_entropy(logits, Y_test)\n",
    "\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 3.290433406829834, val loss: 3.300619125366211\n",
      "step: 5000, train loss: 2.7291624546051025, val loss: 2.379938840866089\n",
      "step: 10000, train loss: 2.177419424057007, val loss: 2.343893051147461\n",
      "step: 15000, train loss: 2.50797176361084, val loss: 2.305969476699829\n",
      "step: 20000, train loss: 2.309122323989868, val loss: 2.2802305221557617\n",
      "step: 25000, train loss: 2.6506752967834473, val loss: 2.31345796585083\n",
      "step: 30000, train loss: 2.178121566772461, val loss: 2.3158769607543945\n",
      "step: 35000, train loss: 2.4785425662994385, val loss: 2.3063669204711914\n",
      "step: 40000, train loss: 1.9919476509094238, val loss: 2.28222918510437\n",
      "step: 45000, train loss: 1.9372347593307495, val loss: 2.2604293823242188\n",
      "step: 50000, train loss: 2.2875349521636963, val loss: 2.275184154510498\n",
      "step: 55000, train loss: 2.08091402053833, val loss: 2.2790064811706543\n",
      "step: 60000, train loss: 3.0925285816192627, val loss: 2.2785468101501465\n",
      "step: 65000, train loss: 2.2678749561309814, val loss: 2.299006462097168\n",
      "step: 70000, train loss: 2.201146125793457, val loss: 2.2673070430755615\n",
      "step: 75000, train loss: 2.5672221183776855, val loss: 2.276127815246582\n",
      "step: 80000, train loss: 2.068460464477539, val loss: 2.2672011852264404\n",
      "step: 85000, train loss: 2.14023756980896, val loss: 2.2655978202819824\n",
      "step: 90000, train loss: 2.3027641773223877, val loss: 2.275329828262329\n",
      "step: 95000, train loss: 2.2971012592315674, val loss: 2.2493884563446045\n",
      "step: 100000, train loss: 2.560284376144409, val loss: 2.2732322216033936\n",
      "step: 105000, train loss: 1.967363953590393, val loss: 2.149801731109619\n",
      "step: 110000, train loss: 1.8061693906784058, val loss: 2.150787591934204\n",
      "step: 115000, train loss: 2.025686025619507, val loss: 2.145705223083496\n",
      "step: 120000, train loss: 2.046382427215576, val loss: 2.146841287612915\n",
      "step: 125000, train loss: 2.585435390472412, val loss: 2.146507501602173\n",
      "step: 130000, train loss: 2.139775514602661, val loss: 2.144859790802002\n",
      "step: 135000, train loss: 2.044940710067749, val loss: 2.149829626083374\n",
      "step: 140000, train loss: 2.2547152042388916, val loss: 2.1430468559265137\n",
      "step: 145000, train loss: 2.2395386695861816, val loss: 2.146061658859253\n",
      "step: 150000, train loss: 2.1905248165130615, val loss: 2.1449387073516846\n",
      "step: 155000, train loss: 1.7347410917282104, val loss: 2.1468570232391357\n",
      "step: 160000, train loss: 1.9735996723175049, val loss: 2.146008014678955\n",
      "step: 165000, train loss: 2.4389891624450684, val loss: 2.1448709964752197\n",
      "step: 170000, train loss: 2.143293857574463, val loss: 2.144667625427246\n",
      "step: 175000, train loss: 1.9745113849639893, val loss: 2.146381378173828\n",
      "step: 180000, train loss: 1.6145055294036865, val loss: 2.146169662475586\n",
      "step: 185000, train loss: 2.333291530609131, val loss: 2.1474859714508057\n",
      "step: 190000, train loss: 1.9766643047332764, val loss: 2.143108367919922\n",
      "step: 195000, train loss: 1.6319307088851929, val loss: 2.1471941471099854\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "\n",
    "torch.manual_seed(42)\n",
    "C = torch.randn(27, n_embd)\n",
    "w1 = torch.randn(n_embd * 3, n_hidden)\n",
    "b1 = torch.randn(n_hidden)\n",
    "w2 = torch.randn(n_hidden, 27) * 0.01\n",
    "b2 = torch.randn(27) * 0\n",
    "params = [C, w1, b1, w2, b2]\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "bs = 32\n",
    "n_steps = 200000\n",
    "\n",
    "\n",
    "for step in range(n_steps):\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x = X_train[idx]\n",
    "    y = Y_train[idx]\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    logits = torch.tanh(emb @ w1 + b1) @ w2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    if step % 5000 == 0:\n",
    "        with torch.no_grad():\n",
    "            emb = C[X_val].view(X_val.shape[0], -1)\n",
    "            logits = torch.tanh(emb @ w1 + b1) @ w2 + b2\n",
    "            val_loss = F.cross_entropy(logits, Y_val)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    loss.backward()\n",
    "    lr = 0.1 if step < 100000 else 0.01\n",
    "    for p in params:\n",
    "        p.data -= lr * p.grad\n",
    "        p.grad = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1420)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    emb = C[X_test].view(X_test.shape[0], -1)\n",
    "    logits = torch.tanh(emb @ w1 + b1) @ w2 + b2\n",
    "    val_loss = F.cross_entropy(logits, Y_test)\n",
    "\n",
    "val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
