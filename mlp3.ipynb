{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([169062, 3]),\n",
       " torch.Size([21133, 3]),\n",
       " torch.Size([21133, 3]),\n",
       " torch.Size([169062]),\n",
       " torch.Size([21133]),\n",
       " torch.Size([21133]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words = list(set(words))\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "len(words)\n",
    "\n",
    "chs = list(set(''.join(words + ['.'])))\n",
    "chs = sorted(chs, reverse=False)\n",
    "stoi = {ch: i for i, ch in enumerate(chs)}\n",
    "itos = {i: ch for i, ch in enumerate(chs)}\n",
    "\n",
    "# predict next token use previous 3 tokens\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    context = '...'\n",
    "    for ch in w + '.':\n",
    "        x = [stoi[c] for c in context]\n",
    "        y = stoi[ch]\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        context = context[1:] + ch\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "n1, n2  = int(0.8 * len(X)), int(0.9 * len(X))\n",
    "\n",
    "X_train, X_val, X_test = X.tensor_split([n1, n2])\n",
    "Y_train, Y_val, Y_test = Y.tensor_split([n1, n2])\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implement backward from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "vocab_size = 27\n",
    "block_size = 3\n",
    "\n",
    "def get_params():\n",
    "    torch.manual_seed(42)\n",
    "    C = torch.randn(vocab_size, n_embd)\n",
    "    w1 = torch.randn(n_embd * block_size, n_hidden) * (n_embd * block_size)**-0.5\n",
    "    w2 = torch.randn(n_hidden, vocab_size) * (5/3) * (n_hidden)**-0.5 * 0.1 # 0.1 is for less confident at initialization\n",
    "    b2 = torch.randn(vocab_size) * 0\n",
    "    bnw = torch.ones(n_hidden)\n",
    "    bnb = torch.zeros(n_hidden)\n",
    "    params = [C, w1, w2, b2, bnw, bnb]\n",
    "    for p in params:\n",
    "        p.requires_grad = True\n",
    "    return params\n",
    "\n",
    "params = get_params()\n",
    "C, w1, w2, b2, bnw, bnb = params\n",
    "bs = 32\n",
    "idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "x, y = X_train[idx], Y_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward and torch backward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "# forward\n",
    "emb = C[x].view(x.shape[0], -1)\n",
    "emb.retain_grad()\n",
    "hpreact = emb @ w1\n",
    "hpreact.retain_grad()\n",
    "bnmeani = mean_proj @ hpreact\n",
    "bnmeani.retain_grad()\n",
    "bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "bnstdi.retain_grad()\n",
    "hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "hpreact_bn.retain_grad()\n",
    "h = hpreact_bn.tanh()\n",
    "h.retain_grad()\n",
    "logits = h @ w2 + b2\n",
    "logits.retain_grad()\n",
    "# 2. loss\n",
    "exp_l = logits.exp()\n",
    "exp_l.retain_grad()\n",
    "count = exp_l.sum(dim=-1, keepdim=True)\n",
    "count.retain_grad()\n",
    "probs = exp_l / count\n",
    "probs.retain_grad()\n",
    "nlls = -probs.log()\n",
    "nlls.retain_grad()\n",
    "loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "\n",
    "# backward\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manual backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_grad():\n",
    "    # buffer grad\n",
    "    nlls_grad = torch.zeros(bs, vocab_size)\n",
    "    probs_grad = torch.zeros(bs, vocab_size)\n",
    "    count_grad = torch.zeros(bs, 1)\n",
    "    exp_l_grad = torch.zeros(bs, vocab_size)\n",
    "    logits_grad = torch.zeros(bs, vocab_size)\n",
    "    h_grad = torch.zeros(bs, n_hidden)\n",
    "    hpreact_bn_grad = torch.zeros(bs, n_hidden)\n",
    "    bnmeani_grad = torch.zeros(1, n_hidden)\n",
    "    bnstdi_grad = torch.zeros(1, n_hidden)\n",
    "    bnvari_grad = torch.zeros(1, n_hidden)\n",
    "    hpreact_grad = torch.zeros(bs, n_hidden)\n",
    "    emb_grad = torch.zeros(bs, n_embd * block_size)\n",
    "    # param grad\n",
    "    C_grad = torch.zeros(vocab_size, n_embd)\n",
    "    w1_grad = torch.zeros(n_embd * block_size, n_hidden)\n",
    "    w2_grad = torch.zeros(n_hidden, vocab_size)\n",
    "    b2_grad = torch.zeros(vocab_size)\n",
    "    bnw_grad = torch.zeros(n_hidden)\n",
    "    bnb_grad = torch.zeros(n_hidden)\n",
    "    buffer_grads = [\n",
    "            nlls_grad, probs_grad, count_grad, exp_l_grad, logits_grad, \n",
    "            h_grad, hpreact_bn_grad, bnmeani_grad, bnstdi_grad, bnvari_grad, hpreact_grad, emb_grad\n",
    "        ]\n",
    "    param_grads = [C_grad, w1_grad, w2_grad, b2_grad, bnw_grad, bnb_grad]\n",
    "    return buffer_grads, param_grads\n",
    "\n",
    "def zero_grad():\n",
    "    for g in buffer_grads:\n",
    "        g.fill_(0)\n",
    "    for g in param_grads:\n",
    "        g.fill_(0)\n",
    "\n",
    "buffer_grads, param_grads = init_grad()\n",
    "(\n",
    "    nlls_grad, probs_grad, count_grad, exp_l_grad, logits_grad, \n",
    "    h_grad, hpreact_bn_grad, bnmeani_grad, bnstdi_grad, bnvari_grad, hpreact_grad, emb_grad\n",
    ") = buffer_grads\n",
    "C_grad, w1_grad, w2_grad, b2_grad, bnw_grad, bnb_grad = param_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same grad for loss calculation: True\n",
      "same grad for logits calculation: True\n",
      "same grad for params: True\n"
     ]
    }
   ],
   "source": [
    "def backward(check=False):\n",
    "    # 1. loss\n",
    "    nlls_grad[torch.arange(y.shape[0]), y] = 1 / bs\n",
    "    probs_grad[torch.arange(y.shape[0]), y] = -1 / probs.data[torch.arange(y.shape[0]), y] * nlls_grad[torch.arange(y.shape[0]), y]\n",
    "    count_grad = -(exp_l.data * probs_grad).sum(dim=-1, keepdim=True) / count.data**2\n",
    "    exp_l_grad = probs_grad / count.data + count_grad  # one is from e/c to e, one is from c=\\sum e to e\n",
    "    logits_grad = exp_l.data * exp_l_grad\n",
    "\n",
    "    # 2. logits\n",
    "    h_grad = logits_grad @ w2.data.T\n",
    "    hpreact_bn_grad = h_grad * (1 - h.data**2)\n",
    "    # bn\n",
    "    bnmeani_grad = ((-bnw.data / bnstdi.data) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "    bnstdi_grad = (-((hpreact.data - bnmeani.data) * bnw.data / bnstdi.data**2) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "    # hpreact\n",
    "    hpreact_grad_mean = bnmeani_grad * torch.ones_like(hpreact.data) / bs\n",
    "    hpreact_grad_std = bnstdi_grad * (1 / 2 / bnstdi.data) * (1 / bs) * (2 * var_proj @ hpreact.data)\n",
    "    hpreact_grad_direct = hpreact_bn_grad * (bnw.data / bnstdi.data)\n",
    "    hpreact_grad = hpreact_grad_mean + hpreact_grad_std + hpreact_grad_direct\n",
    "    # emb\n",
    "    emb_grad = hpreact_grad @ w1.data.T\n",
    "    \n",
    "    # 3. params\n",
    "    C_grad.index_add_(dim=0, index=x.view(-1), source=emb_grad.view(-1, n_embd)) # add emb_grad[i] to C[x[i]]\n",
    "    w1_grad = emb.data.T @ hpreact_grad\n",
    "    w2_grad = h.data.T @ logits_grad\n",
    "    b2_grad = logits_grad.sum(dim=0)\n",
    "    bnw_grad = ((hpreact.data - bnmeani.data) / bnstdi.data * hpreact_bn_grad).sum(dim=0)\n",
    "    bnb_grad = hpreact_bn_grad.sum(dim=0)\n",
    "    if check:\n",
    "        is_equal1 = [torch.allclose(nlls_grad, nlls.grad), torch.allclose(probs_grad, probs.grad), torch.allclose(count_grad, count.grad), torch.allclose(exp_l_grad, exp_l.grad), torch.allclose(logits_grad, logits.grad)]\n",
    "        is_equal2 = [torch.allclose(h_grad, h.grad), torch.allclose(hpreact_bn_grad, hpreact_bn.grad), torch.allclose(bnmeani_grad, bnmeani.grad), torch.allclose(bnstdi_grad, bnstdi.grad), torch.allclose(hpreact_grad, hpreact.grad), torch.allclose(emb_grad, emb.grad)]\n",
    "        is_equal3 = [torch.allclose(C_grad, C.grad), torch.allclose(w1_grad, w1.grad), torch.allclose(w2_grad, w2.grad), torch.allclose(b2_grad, b2.grad), torch.allclose(bnw_grad, bnw.grad), torch.allclose(bnb_grad, bnb.grad)]\n",
    "        return all(is_equal1), all(is_equal2), all(is_equal3)\n",
    "\n",
    "equal1, equal2, equal3 = backward(check=True)\n",
    "print('same grad for loss calculation:', equal1)\n",
    "print('same grad for logits calculation:', equal2)\n",
    "print('same grad for params:', equal3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 3.318601369857788, val loss: 3.314592123031616, grad_equal: True\n",
      "step: 1000, train loss: 3.2841665744781494, val loss: 3.279362678527832, grad_equal: True\n",
      "step: 2000, train loss: 3.237403392791748, val loss: 3.261941432952881, grad_equal: True\n",
      "step: 3000, train loss: 3.278920888900757, val loss: 3.250934600830078, grad_equal: True\n",
      "step: 4000, train loss: 3.2361202239990234, val loss: 3.2448129653930664, grad_equal: True\n",
      "step: 5000, train loss: 3.265164375305176, val loss: 3.2416939735412598, grad_equal: True\n",
      "step: 6000, train loss: 3.2153725624084473, val loss: 3.241469621658325, grad_equal: True\n",
      "step: 7000, train loss: 3.263246536254883, val loss: 3.2412421703338623, grad_equal: True\n",
      "step: 8000, train loss: 3.2219350337982178, val loss: 3.241028070449829, grad_equal: True\n",
      "step: 9000, train loss: 3.2198879718780518, val loss: 3.2408359050750732, grad_equal: True\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# model\n",
    "params = get_params()\n",
    "C, w1, w2, b2, bnw, bnb = params\n",
    "bnmean_running = torch.zeros(n_hidden)\n",
    "bnstd_running = torch.ones(n_hidden)\n",
    "buffer_grads, param_grads = init_grad()\n",
    "(\n",
    "    nlls_grad, probs_grad, count_grad, exp_l_grad, logits_grad, \n",
    "    h_grad, hpreact_bn_grad, bnmeani_grad, bnstdi_grad, bnvari_grad, hpreact_grad, emb_grad\n",
    ") = buffer_grads\n",
    "C_grad, w1_grad, w2_grad, b2_grad, bnw_grad, bnb_grad = param_grads\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 10000\n",
    "ini_lr = 1.0\n",
    "\n",
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # forward\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    emb.retain_grad()\n",
    "    hpreact = emb @ w1\n",
    "    hpreact.retain_grad()\n",
    "    bnmeani = mean_proj @ hpreact\n",
    "    bnmeani.retain_grad()\n",
    "    bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "    bnstdi.retain_grad()\n",
    "    hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "    hpreact_bn.retain_grad()\n",
    "    h = hpreact_bn.tanh()\n",
    "    h.retain_grad()\n",
    "    logits = h @ w2 + b2\n",
    "    logits.retain_grad()\n",
    "    # 2. loss\n",
    "    exp_l = logits.exp()\n",
    "    exp_l.retain_grad()\n",
    "    count = exp_l.sum(dim=-1, keepdim=True)\n",
    "    count.retain_grad()\n",
    "    probs = exp_l / count\n",
    "    probs.retain_grad()\n",
    "    nlls = -probs.log()\n",
    "    nlls.retain_grad()\n",
    "    loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "\n",
    "    # backward\n",
    "    loss.backward() # for compare, need to implement before manual backward()\n",
    "    equal1, equal2, equal3 = backward(check=True)\n",
    "    if step % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            emb = C[X_val].view(X_val.shape[0], -1)\n",
    "            hpreact = emb @ w1\n",
    "            hpreact = (hpreact - hpreact.mean(dim=0, keepdim=True)) / hpreact.std(dim=0, keepdim=True) * bnw + bnb\n",
    "            h = hpreact.tanh()\n",
    "            logits = h @ w2 + b2\n",
    "            val_loss = F.cross_entropy(logits, Y_val)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}, grad_equal: {all([equal1, equal2, equal3])}')\n",
    "    \n",
    "    # update\n",
    "    for p, g in zip(params, param_grads):\n",
    "        p.data -= lr * g\n",
    "        zero_grad()\n",
    "        p.grad = None # for compare\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = bnmean_running * 0.99 + bnmeani * 0.01\n",
    "        bnstd_running = bnstd_running * 0.99 + bnstdi * 0.01\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 3.318601369857788, val loss: 3.314592123031616\n",
      "step: 1000, train loss: 2.234222173690796, val loss: 2.458853006362915\n",
      "step: 2000, train loss: 2.4280223846435547, val loss: 2.4311578273773193\n",
      "step: 3000, train loss: 2.7830185890197754, val loss: 2.400797128677368\n",
      "step: 4000, train loss: 2.239293336868286, val loss: 2.3843557834625244\n",
      "step: 5000, train loss: 2.6441760063171387, val loss: 2.380204200744629\n",
      "step: 6000, train loss: 2.298123598098755, val loss: 2.313563823699951\n",
      "step: 7000, train loss: 2.212418794631958, val loss: 2.3081648349761963\n",
      "step: 8000, train loss: 2.199744701385498, val loss: 2.3080670833587646\n",
      "step: 9000, train loss: 2.272951364517212, val loss: 2.3036866188049316\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# model\n",
    "params = get_params()\n",
    "C, w1, w2, b2, bnw, bnb = params\n",
    "bnmean_running = torch.zeros(n_hidden)\n",
    "bnstd_running = torch.ones(n_hidden)\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 10000\n",
    "ini_lr = 1.0\n",
    "\n",
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # forward\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    emb.retain_grad()\n",
    "    hpreact = emb @ w1\n",
    "    hpreact.retain_grad()\n",
    "    bnmeani = mean_proj @ hpreact\n",
    "    bnmeani.retain_grad()\n",
    "    bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "    bnstdi.retain_grad()\n",
    "    hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "    hpreact_bn.retain_grad()\n",
    "    h = hpreact_bn.tanh()\n",
    "    h.retain_grad()\n",
    "    logits = h @ w2 + b2\n",
    "    logits.retain_grad()\n",
    "    # 2. loss\n",
    "    exp_l = logits.exp()\n",
    "    exp_l.retain_grad()\n",
    "    count = exp_l.sum(dim=-1, keepdim=True)\n",
    "    count.retain_grad()\n",
    "    probs = exp_l / count\n",
    "    probs.retain_grad()\n",
    "    nlls = -probs.log()\n",
    "    nlls.retain_grad()\n",
    "    loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "\n",
    "    # backward\n",
    "    loss.backward() # for compare, need to implement before manual backward()\n",
    "    if step % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            emb = C[X_val].view(X_val.shape[0], -1)\n",
    "            hpreact = emb @ w1\n",
    "            hpreact = (hpreact - hpreact.mean(dim=0, keepdim=True)) / hpreact.std(dim=0, keepdim=True) * bnw + bnb\n",
    "            h = hpreact.tanh()\n",
    "            logits = h @ w2 + b2\n",
    "            val_loss = F.cross_entropy(logits, Y_val)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    for p in params:\n",
    "        p.data -= lr * p.grad\n",
    "        p.grad = None # for compare\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = bnmean_running * 0.99 + bnmeani * 0.01\n",
    "        bnstd_running = bnstd_running * 0.99 + bnstdi * 0.01\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
