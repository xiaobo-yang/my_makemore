{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([169062, 3]),\n",
       " torch.Size([21133, 3]),\n",
       " torch.Size([21133, 3]),\n",
       " torch.Size([169062]),\n",
       " torch.Size([21133]),\n",
       " torch.Size([21133]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words = list(set(words))\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "len(words)\n",
    "\n",
    "chs = list(set(''.join(words + ['.'])))\n",
    "chs = sorted(chs, reverse=False)\n",
    "stoi = {ch: i for i, ch in enumerate(chs)}\n",
    "itos = {i: ch for i, ch in enumerate(chs)}\n",
    "\n",
    "# predict next token use previous 3 tokens\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    context = '...'\n",
    "    for ch in w + '.':\n",
    "        x = [stoi[c] for c in context]\n",
    "        y = stoi[ch]\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        context = context[1:] + ch\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "n1, n2  = int(0.8 * len(X)), int(0.9 * len(X))\n",
    "\n",
    "X_train, X_val, X_test = X.tensor_split([n1, n2])\n",
    "Y_train, Y_val, Y_test = Y.tensor_split([n1, n2])\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implement backward from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "vocab_size = 27\n",
    "block_size = 3\n",
    "\n",
    "def get_params():\n",
    "    torch.manual_seed(42)\n",
    "    C = torch.randn(vocab_size, n_embd)\n",
    "    w1 = torch.randn(n_embd * block_size, n_hidden) * (n_embd * block_size)**-0.5\n",
    "    w2 = torch.randn(n_hidden, vocab_size) * (5/3) * (n_hidden)**-0.5 * 0.1 # 0.1 is for less confident at initialization\n",
    "    b2 = torch.randn(vocab_size) * 0\n",
    "    bnw = torch.ones(n_hidden)\n",
    "    bnb = torch.zeros(n_hidden)\n",
    "    params = [C, w1, w2, b2, bnw, bnb]\n",
    "    for p in params:\n",
    "        p.requires_grad = True\n",
    "    return params\n",
    "\n",
    "params = get_params()\n",
    "C, w1, w2, b2, bnw, bnb = params\n",
    "bs = 32\n",
    "idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "x, y = X_train[idx], Y_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward and torch backward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "# forward\n",
    "emb = C[x].view(x.shape[0], -1)\n",
    "emb.retain_grad()\n",
    "hpreact = emb @ w1\n",
    "hpreact.retain_grad()\n",
    "bnmeani = mean_proj @ hpreact\n",
    "bnmeani.retain_grad()\n",
    "bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "bnstdi.retain_grad()\n",
    "hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "hpreact_bn.retain_grad()\n",
    "h = hpreact_bn.tanh()\n",
    "h.retain_grad()\n",
    "logits = h @ w2 + b2\n",
    "logits.retain_grad()\n",
    "# 2. loss\n",
    "exp_l = logits.exp()\n",
    "exp_l.retain_grad()\n",
    "count = exp_l.sum(dim=-1, keepdim=True)\n",
    "count.retain_grad()\n",
    "probs = exp_l / count\n",
    "probs.retain_grad()\n",
    "nlls = -probs.log()\n",
    "nlls.retain_grad()\n",
    "loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "\n",
    "# backward\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manual backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer grad\n",
    "nlls_grad = torch.zeros(bs, vocab_size)\n",
    "probs_grad = torch.zeros(bs, vocab_size)\n",
    "count_grad = torch.zeros(bs, 1)\n",
    "exp_l_grad = torch.zeros(bs, vocab_size)\n",
    "logits_grad = torch.zeros(bs, vocab_size)\n",
    "h_grad = torch.zeros(bs, n_hidden)\n",
    "hpreact_bn_grad = torch.zeros(bs, n_hidden)\n",
    "bnmeani_grad = torch.zeros(1, n_hidden)\n",
    "bnstdi_grad = torch.zeros(1, n_hidden)\n",
    "bnvari_grad = torch.zeros(1, n_hidden)\n",
    "hpreact_grad = torch.zeros(bs, n_hidden)\n",
    "emb_grad = torch.zeros(bs, n_embd * block_size)\n",
    "# param grad\n",
    "C_grad = torch.zeros(vocab_size, n_embd)\n",
    "w1_grad = torch.zeros(n_embd * block_size, n_hidden)\n",
    "w2_grad = torch.zeros(n_hidden, vocab_size)\n",
    "b2_grad = torch.zeros(vocab_size)\n",
    "bnw_grad = torch.zeros(n_hidden)\n",
    "bnb_grad = torch.zeros(n_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same grad for loss calculation: [True, True, True, True, True]\n",
      "same grad for logits calculation: [True, True, True, True, True, True]\n",
      "same grad for params: [True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "# 1. loss\n",
    "nlls_grad[torch.arange(y.shape[0]), y] = 1 / bs\n",
    "probs_grad[torch.arange(y.shape[0]), y] = -1 / probs.data[torch.arange(y.shape[0]), y] * nlls_grad[torch.arange(y.shape[0]), y]\n",
    "count_grad = -(exp_l.data * probs_grad).sum(dim=-1, keepdim=True) / count.data**2\n",
    "exp_l_grad = probs_grad / count.data + count_grad  # one is from e/c to e, one is from c=\\sum e to e\n",
    "logits_grad = exp_l.data * exp_l_grad\n",
    "\n",
    "# 2. logits\n",
    "h_grad = logits_grad @ w2.data.T\n",
    "hpreact_bn_grad = h_grad * (1 - h.data**2)\n",
    "# bn\n",
    "bnmeani_grad = ((-bnw.data / bnstdi.data) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "bnstdi_grad = (-((hpreact.data - bnmeani.data) * bnw.data / bnstdi.data**2) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "# hpreact\n",
    "hpreact_grad_mean = bnmeani_grad * torch.ones_like(hpreact.data) / bs\n",
    "hpreact_grad_std = bnstdi_grad * (1 / 2 / bnstdi.data) * (1 / bs) * (2 * var_proj @ hpreact.data)\n",
    "hpreact_grad_direct = hpreact_bn_grad * (bnw.data / bnstdi.data)\n",
    "hpreact_grad = hpreact_grad_mean + hpreact_grad_std + hpreact_grad_direct\n",
    "# emb\n",
    "emb_grad = hpreact_grad @ w1.data.T\n",
    "\n",
    "# 3. params\n",
    "C_grad.index_add_(dim=0, index=x.view(-1), source=emb_grad.view(-1, n_embd)) # add emb_grad[i] to C[x[i]]\n",
    "w1_grad = emb.data.T @ hpreact_grad\n",
    "w2_grad = h.data.T @ logits_grad\n",
    "b2_grad = logits_grad.sum(dim=0)\n",
    "bnw_grad = ((hpreact.data - bnmeani.data) / bnstdi.data * hpreact_bn_grad).sum(dim=0)\n",
    "bnb_grad = hpreact_bn_grad.sum(dim=0)\n",
    "\n",
    "# check\n",
    "is_equal1 = [torch.allclose(nlls_grad, nlls.grad), torch.allclose(probs_grad, probs.grad), torch.allclose(count_grad, count.grad), torch.allclose(exp_l_grad, exp_l.grad), torch.allclose(logits_grad, logits.grad)]\n",
    "is_equal2 = [torch.allclose(h_grad, h.grad), torch.allclose(hpreact_bn_grad, hpreact_bn.grad), torch.allclose(bnmeani_grad, bnmeani.grad), torch.allclose(bnstdi_grad, bnstdi.grad), torch.allclose(hpreact_grad, hpreact.grad), torch.allclose(emb_grad, emb.grad)]\n",
    "is_equal3 = [torch.allclose(C_grad, C.grad), torch.allclose(w1_grad, w1.grad), torch.allclose(w2_grad, w2.grad), torch.allclose(b2_grad, b2.grad), torch.allclose(bnw_grad, bnw.grad), torch.allclose(bnb_grad, bnb.grad)]\n",
    "print('same grad for loss calculation:', is_equal1)\n",
    "print('same grad for logits calculation:', is_equal2)\n",
    "print('same grad for params:', is_equal3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 3.3074488639831543, val loss: 3.3160440921783447\n",
      "step: 1000, train loss: 2.583611488342285, val loss: 2.4248502254486084\n",
      "step: 2000, train loss: 2.4143431186676025, val loss: 2.390408992767334\n",
      "step: 3000, train loss: 2.1222798824310303, val loss: 2.379809856414795\n",
      "step: 4000, train loss: 2.1513724327087402, val loss: 2.374274969100952\n",
      "step: 5000, train loss: 2.349586009979248, val loss: 2.386371374130249\n",
      "step: 6000, train loss: 2.2481563091278076, val loss: 2.2990658283233643\n",
      "step: 7000, train loss: 2.0973422527313232, val loss: 2.2958080768585205\n",
      "step: 8000, train loss: 2.5018582344055176, val loss: 2.2962090969085693\n",
      "step: 9000, train loss: 2.072721242904663, val loss: 2.288125991821289\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# model\n",
    "torch.manual_seed(42)\n",
    "C = torch.randn(vocab_size, n_embd)\n",
    "w1 = torch.randn(n_embd * block_size, n_hidden) * (n_embd * block_size)**-0.5\n",
    "w2 = torch.randn(n_hidden, vocab_size) * (5/3) * (n_hidden)**-0.5 * 0.1 # 0.1 is for less confident at initialization\n",
    "b2 = torch.randn(vocab_size) * 0\n",
    "bnw = torch.ones(n_hidden)\n",
    "bnb = torch.zeros(n_hidden)\n",
    "params = [C, w1, w2, b2, bnw, bnb]\n",
    "bnmean_running = torch.zeros(n_hidden)\n",
    "bnstd_running = torch.ones(n_hidden)\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 10000\n",
    "ini_lr = 1.0\n",
    "\n",
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # ---------------- forward --------------------\n",
    "    # 1. logits\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    hpreact = emb @ w1\n",
    "    bnmeani = mean_proj @ hpreact\n",
    "    bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "    hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "    h = hpreact_bn.tanh()\n",
    "    logits = h @ w2 + b2\n",
    "    # 2. loss\n",
    "    exp_l = logits.exp()\n",
    "    count = exp_l.sum(dim=-1, keepdim=True)\n",
    "    probs = exp_l / count\n",
    "    nlls = -probs.log()\n",
    "    loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "    \n",
    "\n",
    "\n",
    "    # ---------------- backward --------------------\n",
    "    # 0. zero grad\n",
    "    # buffer grad\n",
    "    nlls_grad = torch.zeros(bs, vocab_size)\n",
    "    probs_grad = torch.zeros(bs, vocab_size)\n",
    "    count_grad = torch.zeros(bs, 1)\n",
    "    exp_l_grad = torch.zeros(bs, vocab_size)\n",
    "    logits_grad = torch.zeros(bs, vocab_size)\n",
    "    h_grad = torch.zeros(bs, n_hidden)\n",
    "    hpreact_bn_grad = torch.zeros(bs, n_hidden)\n",
    "    bnmeani_grad = torch.zeros(1, n_hidden)\n",
    "    bnstdi_grad = torch.zeros(1, n_hidden)\n",
    "    bnvari_grad = torch.zeros(1, n_hidden)\n",
    "    hpreact_grad = torch.zeros(bs, n_hidden)\n",
    "    emb_grad = torch.zeros(bs, n_embd * block_size)\n",
    "    # param grad\n",
    "    C_grad = torch.zeros(vocab_size, n_embd)\n",
    "    w1_grad = torch.zeros(n_embd * block_size, n_hidden)\n",
    "    w2_grad = torch.zeros(n_hidden, vocab_size)\n",
    "    b2_grad = torch.zeros(vocab_size)\n",
    "    bnw_grad = torch.zeros(n_hidden)\n",
    "    bnb_grad = torch.zeros(n_hidden)\n",
    "\n",
    "    # 1. loss\n",
    "    nlls_grad[torch.arange(y.shape[0]), y] = 1 / bs\n",
    "    probs_grad[torch.arange(y.shape[0]), y] = -1 / probs.data[torch.arange(y.shape[0]), y] * nlls_grad[torch.arange(y.shape[0]), y]\n",
    "    count_grad = -(exp_l.data * probs_grad).sum(dim=-1, keepdim=True) / count.data**2\n",
    "    exp_l_grad = probs_grad / count.data + count_grad  # one is from e/c to e, one is from c=\\sum e to e\n",
    "    logits_grad = exp_l.data * exp_l_grad\n",
    "\n",
    "    # 2. logits\n",
    "    h_grad = logits_grad @ w2.data.T\n",
    "    hpreact_bn_grad = h_grad * (1 - h.data**2)\n",
    "    # bn\n",
    "    bnmeani_grad = ((-bnw.data / bnstdi.data) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "    bnstdi_grad = (-((hpreact.data - bnmeani.data) * bnw.data / bnstdi.data**2) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "    # hpreact\n",
    "    hpreact_grad_mean = bnmeani_grad * torch.ones_like(hpreact.data) / bs\n",
    "    hpreact_grad_std = bnstdi_grad * (1 / 2 / bnstdi.data) * (1 / bs) * (2 * var_proj @ hpreact.data)\n",
    "    hpreact_grad_direct = hpreact_bn_grad * (bnw.data / bnstdi.data)\n",
    "    hpreact_grad = hpreact_grad_mean + hpreact_grad_std + hpreact_grad_direct\n",
    "    # emb\n",
    "    emb_grad = hpreact_grad @ w1.data.T\n",
    "    \n",
    "    # 3. params\n",
    "    C_grad.index_add_(dim=0, index=x.view(-1), source=emb_grad.view(-1, n_embd)) # add emb_grad[i] to C[x[i]]\n",
    "    w1_grad = emb.data.T @ hpreact_grad\n",
    "    w2_grad = h.data.T @ logits_grad\n",
    "    b2_grad = logits_grad.sum(dim=0)\n",
    "    bnw_grad = ((hpreact.data - bnmeani.data) / bnstdi.data * hpreact_bn_grad).sum(dim=0)\n",
    "    bnb_grad = hpreact_bn_grad.sum(dim=0)\n",
    "    param_grads = [C_grad, w1_grad, w2_grad, b2_grad, bnw_grad, bnb_grad]\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            emb = C[X_val].view(X_val.shape[0], -1)\n",
    "            hpreact = emb @ w1\n",
    "            hpreact = (hpreact - hpreact.mean(dim=0, keepdim=True)) / hpreact.std(dim=0, keepdim=True) * bnw + bnb\n",
    "            h = hpreact.tanh()\n",
    "            logits = h @ w2 + b2\n",
    "            val_loss = F.cross_entropy(logits, Y_val)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    for p, g in zip(params, param_grads):\n",
    "        p.data -= lr * g\n",
    "    \n",
    "    bnmean_running = bnmean_running * 0.99 + bnmeani * 0.01\n",
    "    bnstd_running = bnstd_running * 0.99 + bnstdi * 0.01\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1794, -0.1739,  0.0228,  ...,  0.0201, -0.1145,  0.1450],\n",
       "        [-0.3108,  0.1481, -0.2057,  ..., -0.3503,  0.2971,  0.1593],\n",
       "        [-0.0755,  0.0891, -0.0533,  ..., -0.1936, -0.2113, -0.3970],\n",
       "        ...,\n",
       "        [ 0.0905, -0.2106, -0.5778,  ..., -0.1088, -0.1970, -0.3650],\n",
       "        [-0.1003,  0.1913, -0.4233,  ...,  0.3125,  0.0998, -0.2520],\n",
       "        [ 0.2542,  0.2179, -0.3316,  ..., -0.4188, -0.2184, -0.4282]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17936445772647858"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[0,0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 3.3074488639831543, val loss: 3.3160440921783447\n",
      "step: 1000, train loss: 2.583611488342285, val loss: 2.4248502254486084\n",
      "step: 2000, train loss: 2.4143431186676025, val loss: 2.390408992767334\n",
      "step: 3000, train loss: 2.1222798824310303, val loss: 2.379809856414795\n",
      "step: 4000, train loss: 2.1513726711273193, val loss: 2.374274969100952\n",
      "step: 5000, train loss: 2.349586009979248, val loss: 2.386371374130249\n",
      "step: 6000, train loss: 2.2481563091278076, val loss: 2.2990663051605225\n",
      "step: 7000, train loss: 2.0973422527313232, val loss: 2.2958080768585205\n",
      "step: 8000, train loss: 2.5018584728240967, val loss: 2.2962090969085693\n",
      "step: 9000, train loss: 2.072721242904663, val loss: 2.288125991821289\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# model\n",
    "params = get_params()\n",
    "C, w1, w2, b2, bnw, bnb = params\n",
    "bnmean_running = torch.zeros(n_hidden)\n",
    "bnstd_running = torch.ones(n_hidden)\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 10000\n",
    "ini_lr = 1.0\n",
    "\n",
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # forward\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    hpreact = emb @ w1\n",
    "    bnmeani = mean_proj @ hpreact\n",
    "    bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "    hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "    h = hpreact_bn.tanh()\n",
    "    logits = h @ w2 + b2\n",
    "    # 2. loss\n",
    "    exp_l = logits.exp()\n",
    "    count = exp_l.sum(dim=-1, keepdim=True)\n",
    "    probs = exp_l / count\n",
    "    nlls = -probs.log()\n",
    "    loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "\n",
    "    # backward\n",
    "    loss.backward() # for compare, need to implement before manual backward()\n",
    "    if step % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            emb = C[X_val].view(X_val.shape[0], -1)\n",
    "            hpreact = emb @ w1\n",
    "            hpreact = (hpreact - hpreact.mean(dim=0, keepdim=True)) / hpreact.std(dim=0, keepdim=True) * bnw + bnb\n",
    "            h = hpreact.tanh()\n",
    "            logits = h @ w2 + b2\n",
    "            val_loss = F.cross_entropy(logits, Y_val)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    for p in params:\n",
    "        p.data -= lr * p.grad\n",
    "        p.grad = None\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = bnmean_running * 0.99 + bnmeani * 0.01\n",
    "        bnstd_running = bnstd_running * 0.99 + bnstdi * 0.01\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1794, -0.1739,  0.0228,  ...,  0.0201, -0.1145,  0.1450],\n",
       "        [-0.3108,  0.1481, -0.2057,  ..., -0.3503,  0.2971,  0.1593],\n",
       "        [-0.0755,  0.0891, -0.0533,  ..., -0.1936, -0.2113, -0.3970],\n",
       "        ...,\n",
       "        [ 0.0905, -0.2106, -0.5778,  ..., -0.1088, -0.1970, -0.3650],\n",
       "        [-0.1003,  0.1913, -0.4233,  ...,  0.3125,  0.0998, -0.2520],\n",
       "        [ 0.2542,  0.2179, -0.3316,  ..., -0.4188, -0.2184, -0.4282]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17936447262763977"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[0,0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
