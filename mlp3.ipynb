{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([169062, 3]),\n",
       " torch.Size([21133, 3]),\n",
       " torch.Size([21133, 3]),\n",
       " torch.Size([169062]),\n",
       " torch.Size([21133]),\n",
       " torch.Size([21133]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words = list(set(words))\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "len(words)\n",
    "\n",
    "chs = list(set(''.join(words + ['.'])))\n",
    "chs = sorted(chs, reverse=False)\n",
    "stoi = {ch: i for i, ch in enumerate(chs)}\n",
    "itos = {i: ch for i, ch in enumerate(chs)}\n",
    "\n",
    "# predict next token use previous 3 tokens\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    context = '...'\n",
    "    for ch in w + '.':\n",
    "        x = [stoi[c] for c in context]\n",
    "        y = stoi[ch]\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        context = context[1:] + ch\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "n1, n2  = int(0.8 * len(X)), int(0.9 * len(X))\n",
    "\n",
    "X_train, X_val, X_test = X.tensor_split([n1, n2])\n",
    "Y_train, Y_val, Y_test = Y.tensor_split([n1, n2])\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implement backward from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "vocab_size = 27\n",
    "block_size = 3\n",
    "\n",
    "def get_params():\n",
    "    torch.manual_seed(42)\n",
    "    C = torch.randn(vocab_size, n_embd)\n",
    "    w1 = torch.randn(n_embd * block_size, n_hidden) * (n_embd * block_size)**-0.5\n",
    "    w2 = torch.randn(n_hidden, vocab_size) * (5/3) * (n_hidden)**-0.5 * 0.1 # 0.1 is for less confident at initialization\n",
    "    b2 = torch.randn(vocab_size) * 0\n",
    "    bnw = torch.ones(n_hidden)\n",
    "    bnb = torch.zeros(n_hidden)\n",
    "    params = [C, w1, w2, b2, bnw, bnb]\n",
    "    for p in params:\n",
    "        p.requires_grad = True\n",
    "    return params\n",
    "\n",
    "params = get_params()\n",
    "C, w1, w2, b2, bnw, bnb = params\n",
    "bs = 32\n",
    "idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "x, y = X_train[idx], Y_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward and torch backward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "# forward\n",
    "emb = C[x].view(x.shape[0], -1)\n",
    "emb.retain_grad()\n",
    "hpreact = emb @ w1\n",
    "hpreact.retain_grad()\n",
    "bnmeani = mean_proj @ hpreact\n",
    "bnmeani.retain_grad()\n",
    "bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "bnstdi.retain_grad()\n",
    "hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "hpreact_bn.retain_grad()\n",
    "h = hpreact_bn.tanh()\n",
    "h.retain_grad()\n",
    "logits = h @ w2 + b2\n",
    "logits.retain_grad()\n",
    "# 2. loss\n",
    "exp_l = logits.exp()\n",
    "exp_l.retain_grad()\n",
    "count = exp_l.sum(dim=-1, keepdim=True)\n",
    "count.retain_grad()\n",
    "probs = exp_l / count\n",
    "probs.retain_grad()\n",
    "nlls = -probs.log()\n",
    "nlls.retain_grad()\n",
    "loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "\n",
    "# backward\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manual backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer grad\n",
    "nlls_grad = torch.zeros(bs, vocab_size)\n",
    "probs_grad = torch.zeros(bs, vocab_size)\n",
    "count_grad = torch.zeros(bs, 1)\n",
    "exp_l_grad = torch.zeros(bs, vocab_size)\n",
    "logits_grad = torch.zeros(bs, vocab_size)\n",
    "h_grad = torch.zeros(bs, n_hidden)\n",
    "hpreact_bn_grad = torch.zeros(bs, n_hidden)\n",
    "bnmeani_grad = torch.zeros(1, n_hidden)\n",
    "bnstdi_grad = torch.zeros(1, n_hidden)\n",
    "bnvari_grad = torch.zeros(1, n_hidden)\n",
    "hpreact_grad = torch.zeros(bs, n_hidden)\n",
    "emb_grad = torch.zeros(bs, n_embd * block_size)\n",
    "# param grad\n",
    "C_grad = torch.zeros(vocab_size, n_embd)\n",
    "w1_grad = torch.zeros(n_embd * block_size, n_hidden)\n",
    "w2_grad = torch.zeros(n_hidden, vocab_size)\n",
    "b2_grad = torch.zeros(vocab_size)\n",
    "bnw_grad = torch.zeros(n_hidden)\n",
    "bnb_grad = torch.zeros(n_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same grad for loss calculation: [True, True, True, True, True]\n",
      "same grad for logits calculation: [True, True, True, True, True, True]\n",
      "same grad for params: [True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "# 1. loss\n",
    "nlls_grad[torch.arange(y.shape[0]), y] = 1 / bs\n",
    "probs_grad[torch.arange(y.shape[0]), y] = -1 / probs.data[torch.arange(y.shape[0]), y] * nlls_grad[torch.arange(y.shape[0]), y]\n",
    "count_grad = -(exp_l.data * probs_grad).sum(dim=-1, keepdim=True) / count.data**2\n",
    "exp_l_grad = probs_grad / count.data + count_grad  # one is from e/c to e, one is from c=\\sum e to e\n",
    "logits_grad = exp_l.data * exp_l_grad\n",
    "\n",
    "# 2. logits\n",
    "h_grad = logits_grad @ w2.data.T\n",
    "hpreact_bn_grad = h_grad * (1 - h.data**2)\n",
    "# bn\n",
    "bnmeani_grad = ((-bnw.data / bnstdi.data) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "bnstdi_grad = (-((hpreact.data - bnmeani.data) * bnw.data / bnstdi.data**2) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "# hpreact\n",
    "hpreact_grad_mean = bnmeani_grad * torch.ones_like(hpreact.data) / bs\n",
    "hpreact_grad_std = bnstdi_grad * (1 / 2 / bnstdi.data) * (1 / bs) * (2 * var_proj @ hpreact.data)\n",
    "hpreact_grad_direct = hpreact_bn_grad * (bnw.data / bnstdi.data)\n",
    "hpreact_grad = hpreact_grad_mean + hpreact_grad_std + hpreact_grad_direct\n",
    "# emb\n",
    "emb_grad = hpreact_grad @ w1.data.T\n",
    "\n",
    "# 3. params\n",
    "C_grad.index_add_(dim=0, index=x.view(-1), source=emb_grad.view(-1, n_embd)) # add emb_grad[i] to C[x[i]]\n",
    "w1_grad = emb.data.T @ hpreact_grad\n",
    "w2_grad = h.data.T @ logits_grad\n",
    "b2_grad = logits_grad.sum(dim=0)\n",
    "bnw_grad = ((hpreact.data - bnmeani.data) / bnstdi.data * hpreact_bn_grad).sum(dim=0)\n",
    "bnb_grad = hpreact_bn_grad.sum(dim=0)\n",
    "\n",
    "# check\n",
    "is_equal1 = [torch.allclose(nlls_grad, nlls.grad), torch.allclose(probs_grad, probs.grad), torch.allclose(count_grad, count.grad), torch.allclose(exp_l_grad, exp_l.grad), torch.allclose(logits_grad, logits.grad)]\n",
    "is_equal2 = [torch.allclose(h_grad, h.grad), torch.allclose(hpreact_bn_grad, hpreact_bn.grad), torch.allclose(bnmeani_grad, bnmeani.grad), torch.allclose(bnstdi_grad, bnstdi.grad), torch.allclose(hpreact_grad, hpreact.grad), torch.allclose(emb_grad, emb.grad)]\n",
    "is_equal3 = [torch.allclose(C_grad, C.grad), torch.allclose(w1_grad, w1.grad), torch.allclose(w2_grad, w2.grad), torch.allclose(b2_grad, b2.grad), torch.allclose(bnw_grad, bnw.grad), torch.allclose(bnb_grad, bnb.grad)]\n",
    "print('same grad for loss calculation:', is_equal1)\n",
    "print('same grad for logits calculation:', is_equal2)\n",
    "print('same grad for params:', is_equal3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 3.3074488639831543, val loss: 3.3160440921783447\n",
      "step: 1000, train loss: 2.583611488342285, val loss: 2.4248502254486084\n",
      "step: 2000, train loss: 2.4143431186676025, val loss: 2.390408992767334\n",
      "step: 3000, train loss: 2.1222798824310303, val loss: 2.379809856414795\n",
      "step: 4000, train loss: 2.1513724327087402, val loss: 2.374274969100952\n",
      "step: 5000, train loss: 2.349586009979248, val loss: 2.386371374130249\n",
      "step: 6000, train loss: 2.2481563091278076, val loss: 2.2990658283233643\n",
      "step: 7000, train loss: 2.0973422527313232, val loss: 2.2958080768585205\n",
      "step: 8000, train loss: 2.5018582344055176, val loss: 2.2962090969085693\n",
      "step: 9000, train loss: 2.072721242904663, val loss: 2.288125991821289\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# model\n",
    "torch.manual_seed(42)\n",
    "C = torch.randn(vocab_size, n_embd)\n",
    "w1 = torch.randn(n_embd * block_size, n_hidden) * (n_embd * block_size)**-0.5\n",
    "w2 = torch.randn(n_hidden, vocab_size) * (5/3) * (n_hidden)**-0.5 * 0.1 # 0.1 is for less confident at initialization\n",
    "b2 = torch.randn(vocab_size) * 0\n",
    "bnw = torch.ones(n_hidden)\n",
    "bnb = torch.zeros(n_hidden)\n",
    "params = [C, w1, w2, b2, bnw, bnb]\n",
    "bnmean_running = torch.zeros(n_hidden)\n",
    "bnstd_running = torch.ones(n_hidden)\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 10000\n",
    "ini_lr = 1.0\n",
    "\n",
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # ---------------- forward --------------------\n",
    "    # 1. logits\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    hpreact = emb @ w1\n",
    "    bnmeani = mean_proj @ hpreact\n",
    "    bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "    hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "    h = hpreact_bn.tanh()\n",
    "    logits = h @ w2 + b2\n",
    "    # 2. loss\n",
    "    exp_l = logits.exp()\n",
    "    count = exp_l.sum(dim=-1, keepdim=True)\n",
    "    probs = exp_l / count\n",
    "    nlls = -probs.log()\n",
    "    loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "    \n",
    "\n",
    "\n",
    "    # ---------------- backward --------------------\n",
    "    # 0. zero grad\n",
    "    # buffer grad\n",
    "    nlls_grad = torch.zeros(bs, vocab_size)\n",
    "    probs_grad = torch.zeros(bs, vocab_size)\n",
    "    count_grad = torch.zeros(bs, 1)\n",
    "    exp_l_grad = torch.zeros(bs, vocab_size)\n",
    "    logits_grad = torch.zeros(bs, vocab_size)\n",
    "    h_grad = torch.zeros(bs, n_hidden)\n",
    "    hpreact_bn_grad = torch.zeros(bs, n_hidden)\n",
    "    bnmeani_grad = torch.zeros(1, n_hidden)\n",
    "    bnstdi_grad = torch.zeros(1, n_hidden)\n",
    "    bnvari_grad = torch.zeros(1, n_hidden)\n",
    "    hpreact_grad = torch.zeros(bs, n_hidden)\n",
    "    emb_grad = torch.zeros(bs, n_embd * block_size)\n",
    "    # param grad\n",
    "    C_grad = torch.zeros(vocab_size, n_embd)\n",
    "    w1_grad = torch.zeros(n_embd * block_size, n_hidden)\n",
    "    w2_grad = torch.zeros(n_hidden, vocab_size)\n",
    "    b2_grad = torch.zeros(vocab_size)\n",
    "    bnw_grad = torch.zeros(n_hidden)\n",
    "    bnb_grad = torch.zeros(n_hidden)\n",
    "\n",
    "    # 1. loss\n",
    "    nlls_grad[torch.arange(y.shape[0]), y] = 1 / bs\n",
    "    probs_grad[torch.arange(y.shape[0]), y] = -1 / probs.data[torch.arange(y.shape[0]), y] * nlls_grad[torch.arange(y.shape[0]), y]\n",
    "    count_grad = -(exp_l.data * probs_grad).sum(dim=-1, keepdim=True) / count.data**2\n",
    "    exp_l_grad = probs_grad / count.data + count_grad  # one is from e/c to e, one is from c=\\sum e to e\n",
    "    logits_grad = exp_l.data * exp_l_grad\n",
    "\n",
    "    # 2. logits\n",
    "    h_grad = logits_grad @ w2.data.T\n",
    "    hpreact_bn_grad = h_grad * (1 - h.data**2)\n",
    "    # bn\n",
    "    bnmeani_grad = ((-bnw.data / bnstdi.data) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "    bnstdi_grad = (-((hpreact.data - bnmeani.data) * bnw.data / bnstdi.data**2) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "    # hpreact\n",
    "    hpreact_grad_mean = bnmeani_grad * torch.ones_like(hpreact.data) / bs\n",
    "    hpreact_grad_std = bnstdi_grad * (1 / 2 / bnstdi.data) * (1 / bs) * (2 * var_proj @ hpreact.data)\n",
    "    hpreact_grad_direct = hpreact_bn_grad * (bnw.data / bnstdi.data)\n",
    "    hpreact_grad = hpreact_grad_mean + hpreact_grad_std + hpreact_grad_direct\n",
    "    # emb\n",
    "    emb_grad = hpreact_grad @ w1.data.T\n",
    "    \n",
    "    # 3. params\n",
    "    C_grad.index_add_(dim=0, index=x.view(-1), source=emb_grad.view(-1, n_embd)) # add emb_grad[i] to C[x[i]]\n",
    "    w1_grad = emb.data.T @ hpreact_grad\n",
    "    w2_grad = h.data.T @ logits_grad\n",
    "    b2_grad = logits_grad.sum(dim=0)\n",
    "    bnw_grad = ((hpreact.data - bnmeani.data) / bnstdi.data * hpreact_bn_grad).sum(dim=0)\n",
    "    bnb_grad = hpreact_bn_grad.sum(dim=0)\n",
    "    param_grads = [C_grad, w1_grad, w2_grad, b2_grad, bnw_grad, bnb_grad]\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            emb = C[X_val].view(X_val.shape[0], -1)\n",
    "            hpreact = emb @ w1\n",
    "            hpreact = (hpreact - hpreact.mean(dim=0, keepdim=True)) / hpreact.std(dim=0, keepdim=True) * bnw + bnb\n",
    "            h = hpreact.tanh()\n",
    "            logits = h @ w2 + b2\n",
    "            val_loss = F.cross_entropy(logits, Y_val)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    for p, g in zip(params, param_grads):\n",
    "        p.data -= lr * g\n",
    "    \n",
    "    bnmean_running = bnmean_running * 0.99 + bnmeani * 0.01\n",
    "    bnstd_running = bnstd_running * 0.99 + bnstdi * 0.01\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1794, -0.1739,  0.0228,  ...,  0.0201, -0.1145,  0.1450],\n",
       "        [-0.3108,  0.1481, -0.2057,  ..., -0.3503,  0.2971,  0.1593],\n",
       "        [-0.0755,  0.0891, -0.0533,  ..., -0.1936, -0.2113, -0.3970],\n",
       "        ...,\n",
       "        [ 0.0905, -0.2106, -0.5778,  ..., -0.1088, -0.1970, -0.3650],\n",
       "        [-0.1003,  0.1913, -0.4233,  ...,  0.3125,  0.0998, -0.2520],\n",
       "        [ 0.2542,  0.2179, -0.3316,  ..., -0.4188, -0.2184, -0.4282]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17936445772647858"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[0,0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 3.3074488639831543, val loss: 3.3160440921783447\n",
      "step: 1000, train loss: 2.583611488342285, val loss: 2.4248502254486084\n",
      "step: 2000, train loss: 2.4143431186676025, val loss: 2.390408992767334\n",
      "step: 3000, train loss: 2.1222798824310303, val loss: 2.379809856414795\n",
      "step: 4000, train loss: 2.1513726711273193, val loss: 2.374274969100952\n",
      "step: 5000, train loss: 2.349586009979248, val loss: 2.386371374130249\n",
      "step: 6000, train loss: 2.2481563091278076, val loss: 2.2990663051605225\n",
      "step: 7000, train loss: 2.0973422527313232, val loss: 2.2958080768585205\n",
      "step: 8000, train loss: 2.5018584728240967, val loss: 2.2962090969085693\n",
      "step: 9000, train loss: 2.072721242904663, val loss: 2.288125991821289\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# model\n",
    "params = get_params()\n",
    "C, w1, w2, b2, bnw, bnb = params\n",
    "bnmean_running = torch.zeros(n_hidden)\n",
    "bnstd_running = torch.ones(n_hidden)\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 10000\n",
    "ini_lr = 1.0\n",
    "\n",
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # forward\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    hpreact = emb @ w1\n",
    "    bnmeani = mean_proj @ hpreact\n",
    "    bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "    hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "    h = hpreact_bn.tanh()\n",
    "    logits = h @ w2 + b2\n",
    "    # 2. loss\n",
    "    exp_l = logits.exp()\n",
    "    count = exp_l.sum(dim=-1, keepdim=True)\n",
    "    probs = exp_l / count\n",
    "    nlls = -probs.log()\n",
    "    loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    if step % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            emb = C[X_val].view(X_val.shape[0], -1)\n",
    "            hpreact = emb @ w1\n",
    "            hpreact = (hpreact - hpreact.mean(dim=0, keepdim=True)) / hpreact.std(dim=0, keepdim=True) * bnw + bnb\n",
    "            h = hpreact.tanh()\n",
    "            logits = h @ w2 + b2\n",
    "            val_loss = F.cross_entropy(logits, Y_val)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    for p in params:\n",
    "        p.data -= lr * p.grad\n",
    "        p.grad = None\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = bnmean_running * 0.99 + bnmeani * 0.01\n",
    "        bnstd_running = bnstd_running * 0.99 + bnstdi * 0.01\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1794, -0.1739,  0.0228,  ...,  0.0201, -0.1145,  0.1450],\n",
       "        [-0.3108,  0.1481, -0.2057,  ..., -0.3503,  0.2971,  0.1593],\n",
       "        [-0.0755,  0.0891, -0.0533,  ..., -0.1936, -0.2113, -0.3970],\n",
       "        ...,\n",
       "        [ 0.0905, -0.2106, -0.5778,  ..., -0.1088, -0.1970, -0.3650],\n",
       "        [-0.1003,  0.1913, -0.4233,  ...,  0.3125,  0.0998, -0.2520],\n",
       "        [ 0.2542,  0.2179, -0.3316,  ..., -0.4188, -0.2184, -0.4282]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17936447262763977"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[0,0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simpler BatchNorm Grad\n",
    "\n",
    "Let $x\\in\\mathbb{R}^{n\\times d}$, $w\\in\\mathbb{R}^d$, $b\\in\\mathbb{R}^d$, $P = I - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^T \\in \\mathbb{R}^{n\\times n}$, then\n",
    "\n",
    "$$\n",
    "    v = (\\frac{1}{n} x^T P x)~.\\text{diag}() \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "$$\n",
    "    o = \\frac{Px}{\\sqrt{v + \\epsilon}} w + b \\in \\mathbb{R}^{n \\times d}\n",
    "$$\n",
    "\n",
    "Denote $dx$ as grad from the end layer to current layer, $dy/dx$ as grad from next layer to current layer.\n",
    "\n",
    "$$\n",
    "    dw =  do \\cdot \\frac{do}{dw} = \\left((Px) * do\\right).~\\text{sum}(\\text{dim=0}) / \\sqrt{v + \\epsilon} \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "$$\n",
    "    db =  do \\cdot \\frac{do}{db} = \\left(do\\right).~\\text{sum}(\\text{dim=0}) \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "Grad $dx$ is more complex, but if we directly use computation graph(see micrograd), the result will be clear.\n",
    "\n",
    "$$\n",
    "    dx = do \\cdot \\frac{\\partial o}{\\partial x} + dv \\cdot \\frac{d v}{d x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    dv = do \\cdot \\frac{\\partial o}{\\partial v}\n",
    "$$\n",
    "\n",
    "After that, we can use matrix operations to simplify the calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass:\n",
      "o relative error 1.3214495959198304e-13\n",
      "backward pass:\n",
      "b grad relative error: 0.0\n",
      "w grad relative error: 2.9079137144240243e-15\n",
      "x grad relative error: 8.759191542421218e-14\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# use double precision to avoid numerical error\n",
    "n, d = 100, 10\n",
    "# params\n",
    "x = torch.randn(n, d, dtype=torch.float64)\n",
    "w = torch.randn(d, dtype=torch.float64)\n",
    "b = torch.randn(d, dtype=torch.float64)\n",
    "# grad backprop from next layer\n",
    "do = torch.randn(n, d, dtype=torch.float64)\n",
    "\n",
    "# -------- manual --------- \n",
    "# buffer\n",
    "P = torch.eye(n, dtype=torch.float64) - torch.ones(n, n, dtype=torch.float64) / n\n",
    "# forward\n",
    "v = (x.T @ P @ x).diag() / n  # (d,) # this may waste memory when d is large\n",
    "o = (P @ x) / v.sqrt().view(1, d) * w.view(1, d) + b.view(1, d)  # (n, d)\n",
    "# backward\n",
    "w_grad = ((P @ x) * do).sum(dim=0) / v.sqrt().view(1, d) # dw\n",
    "b_grad = do.sum(dim=0) # db\n",
    "o_to_v_grad = ((o - b) * do / (-2 * v)).sum(dim=0) # do * do/dv\n",
    "v_to_x_grad = o_to_v_grad * (P @ x) * 2 / n # dv * dv / dx\n",
    "o_to_x_grad = (P @ do) * w.view(1, d) / v.sqrt().view(1, d) # do * do/dx\n",
    "x_grad = v_to_x_grad + o_to_x_grad # dx\n",
    "\n",
    "# -------- torch --------- \n",
    "xt, wt, bt = x.clone(), w.clone(), b.clone()\n",
    "for p in [xt, wt, bt]:\n",
    "    p.requires_grad = True\n",
    "ot = (xt - xt.mean(dim=0, keepdim=True)) / xt.std(dim=0, keepdim=True, unbiased=False) * wt.view(1, d) + bt.view(1, d)\n",
    "out = ot * do\n",
    "out.sum().backward()\n",
    "\n",
    "# -------- compare -------- \n",
    "print('forward pass:')\n",
    "print(f'o relative error {((ot - o) / ot).abs().max()}')\n",
    "print('backward pass:')\n",
    "print(f'b grad relative error: {((bt.grad - b_grad) / bt.grad).abs().max()}')\n",
    "print(f'w grad relative error: {((wt.grad - w_grad) / wt.grad).abs().max()}')\n",
    "print(f'x grad relative error: {((xt.grad - x_grad) / xt.grad).abs().max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for each grad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative error: 2.4119488693758977e-15\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "n, d = 100, 10\n",
    "x = torch.randn(n, d, requires_grad=True, dtype=torch.float64)\n",
    "w = torch.randn(d, requires_grad=True, dtype=torch.float64)\n",
    "b = torch.randn(d, requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "# forward\n",
    "v = (x.T @ P @ x).diag() / n  # (d,) # this may waste memory\n",
    "\n",
    "# backward\n",
    "dv = torch.randn_like(v)\n",
    "(v * dv).sum().backward()\n",
    "v_to_x_grad = dv * (P @ x) * 2 / n\n",
    "\n",
    "err = ((x.grad - v_to_x_grad) / x.grad).abs().max().item()\n",
    "print(f'relative error: {err}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative error: 8.061833389428677e-15\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(45)\n",
    "n, d = 100, 10\n",
    "ones = torch.ones(n, dtype=torch.float64)\n",
    "x = torch.randn(n, d, requires_grad=True, dtype=torch.float64)\n",
    "w = torch.randn(d, requires_grad=True, dtype=torch.float64)\n",
    "b = torch.randn(d, requires_grad=True, dtype=torch.float64)\n",
    "v = torch.rand(d, requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "# forward\n",
    "o = (P @ x) / v.sqrt().view(1, d) * w.view(1, d) + b.view(1, d)  # (n, d)\n",
    "\n",
    "# backward\n",
    "do = torch.randn_like(o)\n",
    "(o * do).sum().backward()\n",
    "\n",
    "o_to_v_grad = ((o - b) * do / (-2 * v)).sum(dim=0)\n",
    "\n",
    "err = ((v.grad - o_to_v_grad) / v.grad).abs().max().item()\n",
    "print(f'relative error: {err}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative error: 3.606235033323989e-13\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(45)\n",
    "n, d = 100, 10\n",
    "ones = torch.ones(n, dtype=torch.float64)\n",
    "x = torch.randn(n, d, requires_grad=True, dtype=torch.float64)\n",
    "w = torch.randn(d, requires_grad=True, dtype=torch.float64)\n",
    "b = torch.randn(d, requires_grad=True, dtype=torch.float64)\n",
    "v = torch.rand(d, requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "# forward\n",
    "o = (P @ x) / v.sqrt().view(1, d) * w.view(1, d) + b.view(1, d)  # (n, d)\n",
    "\n",
    "# backward\n",
    "do = torch.randn_like(o)\n",
    "(o * do).sum().backward()\n",
    "\n",
    "o_to_x_grad = (P @ do) * w.view(1, d) / v.sqrt().view(1, d)\n",
    "\n",
    "err = ((x.grad - o_to_x_grad) / x.grad).abs().max().item()\n",
    "print(f'relative error: {err}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Linear:\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float64, generator=None):\n",
    "        self.weight = torch.randn(in_features, out_features, dtype=dtype, generator=generator) * (in_features)**-0.5\n",
    "        self.bias = torch.zeros(out_features, dtype=dtype) * 0 if bias else None\n",
    "        # grads\n",
    "        self.weight_grad = None\n",
    "        self.bias_grad = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'MyLinear(in_features={self.weight.shape[0]}, out_features={self.weight.shape[1]}, bias={self.bias is not None})'\n",
    "\n",
    "    def parameters(self):\n",
    "        if self.bias is not None:\n",
    "            return [self.weight, self.bias]\n",
    "        else:\n",
    "            return [self.weight]\n",
    "    \n",
    "    def grads(self):\n",
    "        if self.bias is not None:\n",
    "            return [self.weight_grad, self.bias_grad]\n",
    "        else:\n",
    "            return [self.weight_grad]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.bias is not None:\n",
    "            out = x @ self.weight + self.bias\n",
    "        else:\n",
    "            out = x @ self.weight\n",
    "        # backward buffer\n",
    "        self.x = x\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "                x: input of current layer\n",
    "                out: output of current layer\n",
    "                grad: grad from next layer\n",
    "            Output:\n",
    "                x_grad: grad back to previous layer\n",
    "        \"\"\"\n",
    "        x_grad = grad @ self.weight.T\n",
    "        self.weight_grad = self.x.T @ grad\n",
    "        if self.bias is not None:\n",
    "            self.bias_grad = grad.sum(dim=0)\n",
    "        return x_grad\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, in_features, eps=1e-5, momentum=0.001, dtype=torch.float64): # manual bn need fp64\n",
    "        self.weight = torch.ones(in_features, dtype=dtype)\n",
    "        self.bias = torch.zeros(in_features, dtype=dtype)\n",
    "        self.running_mean = torch.zeros(in_features, dtype=dtype)\n",
    "        self.running_var = torch.ones(in_features, dtype=dtype)\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self._training = True # internal flag\n",
    "        self.dtype = dtype\n",
    "        # grads\n",
    "        self.weight_grad = None\n",
    "        self.bias_grad = None\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'MyBatchNorm1d(in_features={self.weight.shape[0]}, eps={self.eps}, momentum={self.momentum})'\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight, self.bias]\n",
    "    \n",
    "    def grads(self):\n",
    "        return [self.weight_grad, self.bias_grad]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self._training:\n",
    "            b, d = x.shape\n",
    "            Pm = torch.ones(b, dtype=self.dtype) / b # mean projection matrix (b, )\n",
    "            Pv = (torch.eye(b, dtype=self.dtype) - Pm) # var projection matrix (b, b)\n",
    "            m = Pm @ x # (d,)\n",
    "            v = Pm @ (Pv @ x).square() # (d,)\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = self.running_mean * (1 - self.momentum) + m * self.momentum\n",
    "                self.running_var = self.running_var * (1 - self.momentum) + v * self.momentum\n",
    "        else:\n",
    "            m = self.running_mean\n",
    "            v = self.running_var\n",
    "        std = (v + self.eps).sqrt()\n",
    "        out = self.weight * (x - m) / std + self.bias\n",
    "        # backward buffer\n",
    "        self.x = x\n",
    "        self.out = out\n",
    "        self.std = std\n",
    "        self.Pv = Pv\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        assert self._training, 'BatchNorm1d is not in training mode'\n",
    "        x, out, std, Pv = self.x, self.out, self.std, self.Pv\n",
    "        b, d = x.shape\n",
    "        self.weight_grad = ((Pv @ x) * grad).sum(dim=0) / std # dw (d,)\n",
    "        self.bias_grad = grad.sum(dim=0) # db (d,)\n",
    "        o_to_v_grad = ((out - self.bias) * grad / (-2 * std.square())).sum(dim=0) # do * do/dv (d,)\n",
    "        v_to_x_grad = o_to_v_grad * (Pv @ x) * 2 / b # dv * dv / dx (b, d)\n",
    "        o_to_x_grad = (Pv @ grad) * self.weight / std # do * do/dx (b, d)\n",
    "        x_grad = v_to_x_grad + o_to_x_grad # dx (b, d)\n",
    "        return x_grad\n",
    "\n",
    "\n",
    "class Func:\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    def grads(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "class Tanh(Func):\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'MyTanh()'\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out = x.tanh()\n",
    "        # backward buffer\n",
    "        self.out = out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        x_grad = grad * (1 - self.out**2)\n",
    "        return x_grad\n",
    "\n",
    "\n",
    "class CrossEntropyLoss(Func):\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'MyCrossEntropyLoss()'\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        xmax = x.max(dim=-1, keepdim=True)[0]\n",
    "        exp_l = (x - xmax).exp()\n",
    "        count = exp_l.sum(dim=-1, keepdim=True)\n",
    "        probs = exp_l / count\n",
    "        loss = -probs[range(y.shape[0]), y].log().mean()\n",
    "        # backward buffer\n",
    "        self.probs = probs.clone()\n",
    "        self.y = y\n",
    "        return loss\n",
    "        \n",
    "    \n",
    "    def backward(self, grad):\n",
    "        b = self.y.shape[0]\n",
    "        x_grad = self.probs.data\n",
    "        x_grad[range(b), self.y] -= 1\n",
    "        x_grad = x_grad / b * grad\n",
    "        return x_grad\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x grad relative error: 1.6171961676681751e-15\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(41)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "x = torch.randn(100, 10, requires_grad=True, dtype=torch.float64)\n",
    "y = torch.randint(0, 10, (100,))\n",
    "loss = loss_fn(x, y)\n",
    "loss.backward()\n",
    "x_grad = loss_fn.backward(grad=1.0) # last layer, dloss=1.0\n",
    "\n",
    "print(f'x grad relative error: {((x.grad - x_grad) / x.grad).abs().max().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass:\n",
      "o relative error: 5.99848193077636e-15\n",
      "backward pass:\n",
      "db relative error: 0.0\n",
      "dw relative error: 0.0\n",
      "dx relative error: 2.5006944740962915e-14\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dtype = torch.float64\n",
    "eps = 1e-5\n",
    "# model\n",
    "bn = BatchNorm1d(10, dtype=dtype, eps=eps)\n",
    "# params\n",
    "x = torch.randn(100, 10, dtype=dtype)\n",
    "# ------- manual -------\n",
    "# forward\n",
    "o = bn(x)\n",
    "# backward\n",
    "do = torch.randn_like(o, dtype=dtype)\n",
    "dx = bn.backward(do)\n",
    "\n",
    "# ------- torch -------\n",
    "import copy\n",
    "bnt = copy.deepcopy(bn)\n",
    "xt = x.clone()\n",
    "xt.requires_grad = True\n",
    "for p in bnt.parameters():\n",
    "    p.requires_grad = True\n",
    "# forward\n",
    "ot = (xt - xt.mean(dim=0, keepdim=True)) /( xt.var(dim=0, keepdim=True, unbiased=False) + eps).sqrt() * bnt.weight + bnt.bias\n",
    "# backward\n",
    "(ot * do).sum().backward()\n",
    "\n",
    "# -------- compare -------- \n",
    "print('forward pass:')\n",
    "print(f'o relative error: {((ot - o) / ot).abs().max().item()}')\n",
    "print('backward pass:')\n",
    "print(f'db relative error: {((bnt.bias_grad - bn.bias_grad) / bnt.bias_grad).abs().max().item()}')\n",
    "print(f'dw relative error: {((bnt.weight_grad - bn.weight_grad) / bnt.weight_grad).abs().max().item()}')\n",
    "print(f'dx relative error: {((dx - xt.grad) / xt.grad).abs().max().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 0.72M\n",
      "check grad:\n",
      "[Layer 1] weight grad relative error: 1.0616496577638565e-11\n",
      "x_grad relative error: 8.573186037652634e-12\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "n_embd = 30\n",
    "n_hidden = 100\n",
    "bs = 32\n",
    "dtype = torch.float64\n",
    "# model\n",
    "layers = [Linear(n_embd, n_hidden, bias=False, dtype=dtype), BatchNorm1d(n_hidden, dtype=dtype), Tanh()]\n",
    "for _ in range(70):\n",
    "    layers.extend([Linear(n_hidden, n_hidden, bias=False, dtype=dtype), BatchNorm1d(n_hidden, dtype=dtype), Tanh()])\n",
    "params = [p for l in layers for p in l.parameters()]\n",
    "print(f'number of params: {sum(p.numel() for p in params) / 1e6:.2f}M')\n",
    "# input\n",
    "x = torch.randn(bs, n_embd, dtype=dtype, requires_grad=True)\n",
    "\n",
    "# --- manual ---\n",
    "# forward\n",
    "h = x\n",
    "for l in layers:\n",
    "    h = l(h)\n",
    "\n",
    "# backward\n",
    "grad = torch.ones(bs, n_hidden)\n",
    "for i in range(len(layers)-1, -1, -1):\n",
    "    grad = layers[i].backward(grad)\n",
    "\n",
    "\n",
    "# --- torch ---\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "h = x\n",
    "for l in layers:\n",
    "    h = l(h)\n",
    "h.sum().backward()\n",
    "\n",
    "# --- compare ---\n",
    "print('check grad:')\n",
    "print(f'[Layer 1] weight grad relative error: {((params[0].grad - layers[0].weight_grad) / params[0].grad).abs().max().item()}')\n",
    "print(f'x_grad relative error: {((x.grad - grad) / x.grad).abs().max().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 0.13M\n",
      "step: 0, train loss: 3.2882673916139513, val loss: 3.303725181609048\n",
      "step: 1000, train loss: 2.7890219515714154, val loss: 2.3904028412396934\n",
      "step: 2000, train loss: 2.5281986182836484, val loss: 2.3126000885979643\n",
      "step: 3000, train loss: 2.0280105382652405, val loss: 2.340651265202145\n",
      "step: 4000, train loss: 2.594937313936472, val loss: 2.307098736610323\n",
      "step: 5000, train loss: 2.2496904227728076, val loss: 2.3046101823543435\n",
      "step: 6000, train loss: 2.449963955274705, val loss: 2.334276222598324\n",
      "step: 7000, train loss: 2.255257099008805, val loss: 2.332755044574154\n",
      "step: 8000, train loss: 2.5833844582144003, val loss: 2.2991499577452026\n",
      "step: 9000, train loss: 1.9728677558938887, val loss: 2.2365087665394228\n",
      "step: 10000, train loss: 1.931203179280651, val loss: 2.2688682024489895\n",
      "step: 11000, train loss: 2.444830721886659, val loss: 2.2871470263842646\n",
      "step: 12000, train loss: 2.6856199103032634, val loss: 2.227653722602827\n",
      "step: 13000, train loss: 2.1739196381914985, val loss: 2.21136679365425\n",
      "step: 14000, train loss: 2.0703842757681, val loss: 2.3043125964881814\n",
      "step: 15000, train loss: 2.509908521320544, val loss: 2.156436231002042\n",
      "step: 16000, train loss: 2.1113831780868555, val loss: 2.249491088967034\n",
      "step: 17000, train loss: 2.0674913378271444, val loss: 2.231233479314878\n",
      "step: 18000, train loss: 2.267011817476008, val loss: 2.216650636069428\n",
      "step: 19000, train loss: 2.032363382702209, val loss: 2.218178398732692\n",
      "step: 20000, train loss: 2.083638550587054, val loss: 2.214992936187291\n",
      "step: 21000, train loss: 2.334929512101281, val loss: 2.1986976529515636\n",
      "step: 22000, train loss: 2.008685686774677, val loss: 2.2092640260236855\n",
      "step: 23000, train loss: 2.3876769747112636, val loss: 2.192961913327\n",
      "step: 24000, train loss: 2.059466186925001, val loss: 2.1539891510221096\n",
      "step: 25000, train loss: 2.3355224076138486, val loss: 2.1775985586447435\n",
      "step: 26000, train loss: 2.30626445096307, val loss: 2.2051519999106377\n",
      "step: 27000, train loss: 2.299323521095018, val loss: 2.1332238546004976\n",
      "step: 28000, train loss: 2.1950411829666683, val loss: 2.153903010357312\n",
      "step: 29000, train loss: 2.128165238442788, val loss: 2.2451468482635373\n",
      "step: 30000, train loss: 2.1146516855644193, val loss: 2.16422336360527\n",
      "step: 31000, train loss: 2.101250226538605, val loss: 2.1768911184429234\n",
      "step: 32000, train loss: 2.0872414945831244, val loss: 2.1775046427246902\n",
      "step: 33000, train loss: 1.732227107886077, val loss: 2.1670257090936302\n",
      "step: 34000, train loss: 2.3591155428866055, val loss: 2.1809305745558274\n",
      "step: 35000, train loss: 1.9425238880698188, val loss: 2.1620498429427393\n",
      "step: 36000, train loss: 1.8781963453417434, val loss: 2.1201377689880374\n",
      "step: 37000, train loss: 2.251907617387747, val loss: 2.2036190574594365\n",
      "step: 38000, train loss: 1.8313624684670744, val loss: 2.1554533927816264\n",
      "step: 39000, train loss: 2.375457940004985, val loss: 2.1190310777141947\n",
      "step: 40000, train loss: 2.3607780629352186, val loss: 2.0806589366531396\n",
      "step: 41000, train loss: 1.850053361486267, val loss: 2.1448153045871416\n",
      "step: 42000, train loss: 2.4646989974234184, val loss: 2.1101667998237024\n",
      "step: 43000, train loss: 1.8888450619692796, val loss: 2.1325493461857667\n",
      "step: 44000, train loss: 2.0797368092855346, val loss: 2.1361948798422885\n",
      "step: 45000, train loss: 2.1316438043955044, val loss: 2.1381420626034924\n",
      "step: 46000, train loss: 2.215294519001473, val loss: 2.119707682982619\n",
      "step: 47000, train loss: 2.2392009648222198, val loss: 2.1591082893527416\n",
      "step: 48000, train loss: 2.1766429117543167, val loss: 2.1985349695243777\n",
      "step: 49000, train loss: 2.206437348439784, val loss: 2.162760053085875\n",
      "step: 50000, train loss: 1.9245245241449545, val loss: 2.2055925339010733\n",
      "step: 51000, train loss: 2.265147321421893, val loss: 2.122747922749501\n",
      "step: 52000, train loss: 2.045466794544878, val loss: 2.1492874904054675\n",
      "step: 53000, train loss: 2.1728271857464296, val loss: 2.120314498137974\n",
      "step: 54000, train loss: 2.0657154436920853, val loss: 2.104541973721146\n",
      "step: 55000, train loss: 1.9184694616886548, val loss: 2.1670735699912633\n",
      "step: 56000, train loss: 2.397486538888157, val loss: 2.1182924165740915\n",
      "step: 57000, train loss: 2.3923158154918163, val loss: 2.1320297758656634\n",
      "step: 58000, train loss: 1.9489797166386917, val loss: 2.1211079167625684\n",
      "step: 59000, train loss: 2.3181540616594076, val loss: 2.1391235894749494\n",
      "step: 60000, train loss: 2.2924648327128283, val loss: 2.1394729504183463\n",
      "step: 61000, train loss: 2.1624289553381724, val loss: 2.1567399859551277\n",
      "step: 62000, train loss: 1.9341912871504623, val loss: 2.1312596276170126\n",
      "step: 63000, train loss: 1.989311832880643, val loss: 2.09760815280412\n",
      "step: 64000, train loss: 2.4815834502849725, val loss: 2.099397809846676\n",
      "step: 65000, train loss: 1.9818434075215596, val loss: 2.0544059102818353\n",
      "step: 66000, train loss: 2.2018678516401833, val loss: 2.1077946644297194\n",
      "step: 67000, train loss: 2.105185421601032, val loss: 2.102784825247674\n",
      "step: 68000, train loss: 2.286004430816684, val loss: 2.088420550641155\n",
      "step: 69000, train loss: 1.9977609719450053, val loss: 2.08875134595921\n",
      "step: 70000, train loss: 2.211849244406994, val loss: 2.119971963409553\n",
      "step: 71000, train loss: 2.1937168007913694, val loss: 2.131478342155513\n",
      "step: 72000, train loss: 2.1360283225150924, val loss: 2.068469077582702\n",
      "step: 73000, train loss: 2.0603944817309934, val loss: 2.0262853452028207\n",
      "step: 74000, train loss: 2.1734763822044636, val loss: 2.0220920872325197\n",
      "step: 75000, train loss: 2.079769294496929, val loss: 2.0660343608635663\n",
      "step: 76000, train loss: 1.8942926559042514, val loss: 2.0590475458355146\n",
      "step: 77000, train loss: 2.317684046678263, val loss: 2.048728300633721\n",
      "step: 78000, train loss: 2.081745128609115, val loss: 2.07581865921172\n",
      "step: 79000, train loss: 2.517709564232344, val loss: 2.0589972359246476\n",
      "step: 80000, train loss: 1.9560743342545233, val loss: 2.0734354199323626\n",
      "step: 81000, train loss: 1.7926050415331103, val loss: 2.087870707933049\n",
      "step: 82000, train loss: 2.1795689929996045, val loss: 2.073081844254911\n",
      "step: 83000, train loss: 1.9437479256311467, val loss: 2.082739016676678\n",
      "step: 84000, train loss: 2.3064619204732137, val loss: 2.092863503205763\n",
      "step: 85000, train loss: 1.9658387437400466, val loss: 2.0994824261862215\n",
      "step: 86000, train loss: 2.1107672179706656, val loss: 2.048100760807009\n",
      "step: 87000, train loss: 1.839676341624104, val loss: 2.0486501505059516\n",
      "step: 88000, train loss: 2.3995739560257223, val loss: 2.078286696270882\n",
      "step: 89000, train loss: 2.1297919776104064, val loss: 2.0761959230173823\n",
      "step: 90000, train loss: 2.1688117547319354, val loss: 2.095504670913101\n",
      "step: 91000, train loss: 2.058430949112132, val loss: 2.037778762993188\n",
      "step: 92000, train loss: 2.524810005773554, val loss: 2.076252049720017\n",
      "step: 93000, train loss: 2.0787830335327597, val loss: 2.093505440151385\n",
      "step: 94000, train loss: 2.1587901936225378, val loss: 2.0819981110460435\n",
      "step: 95000, train loss: 2.5917299944888685, val loss: 2.0844773411784727\n",
      "step: 96000, train loss: 2.263859846865017, val loss: 2.0366328284248487\n",
      "step: 97000, train loss: 2.3020408045612144, val loss: 2.04088621974237\n",
      "step: 98000, train loss: 2.00531316921782, val loss: 2.035563888975955\n",
      "step: 99000, train loss: 2.1425074288964456, val loss: 2.0580664928350654\n",
      "step: 100000, train loss: 2.321965141043496, val loss: 2.0560102340412945\n",
      "step: 101000, train loss: 2.4642641981121027, val loss: 2.027574549649788\n",
      "step: 102000, train loss: 2.078628221464322, val loss: 2.0248746013143712\n",
      "step: 103000, train loss: 2.4035592050347168, val loss: 2.0144820922318583\n",
      "step: 104000, train loss: 1.732599714653647, val loss: 2.0224209364195853\n",
      "step: 105000, train loss: 2.137699310298594, val loss: 2.0327958775484887\n",
      "step: 106000, train loss: 2.041223451241297, val loss: 2.0185000390412724\n",
      "step: 107000, train loss: 1.8671941326827473, val loss: 2.02793431767003\n",
      "step: 108000, train loss: 1.992266129131448, val loss: 2.023045633319967\n",
      "step: 109000, train loss: 2.382105347875596, val loss: 2.016093172609658\n",
      "step: 110000, train loss: 2.1815286572212145, val loss: 2.0197651190172112\n",
      "step: 111000, train loss: 1.8467375502097205, val loss: 2.0193425529040203\n",
      "step: 112000, train loss: 2.0883889207423, val loss: 2.0277716711842273\n",
      "step: 113000, train loss: 1.9157994872714652, val loss: 2.020723998842733\n",
      "step: 114000, train loss: 1.7056894354427048, val loss: 2.0215724294199355\n",
      "step: 115000, train loss: 2.1080709679212473, val loss: 2.028526305033075\n",
      "step: 116000, train loss: 2.024793063065699, val loss: 2.0242750370895872\n",
      "step: 117000, train loss: 1.8770546243296693, val loss: 2.025442494044715\n",
      "step: 118000, train loss: 2.0690712089392616, val loss: 2.0236078648331977\n",
      "step: 119000, train loss: 2.1254795300990326, val loss: 2.0170096944590328\n",
      "step: 120000, train loss: 1.761556925680362, val loss: 2.027613766065771\n",
      "step: 121000, train loss: 1.8819408832627431, val loss: 2.032020603665267\n",
      "step: 122000, train loss: 2.2007939194371473, val loss: 2.02179456217462\n",
      "step: 123000, train loss: 2.3556440090783033, val loss: 2.0214237413201563\n",
      "step: 124000, train loss: 1.8744798306117685, val loss: 2.011912119308409\n",
      "step: 125000, train loss: 1.6789863972463739, val loss: 2.004686347628645\n",
      "step: 126000, train loss: 1.9155809614915018, val loss: 2.010790474715534\n",
      "step: 127000, train loss: 2.758621014083379, val loss: 2.0146260101591973\n",
      "step: 128000, train loss: 2.0464322715604752, val loss: 2.0124971019495606\n",
      "step: 129000, train loss: 1.911446521954062, val loss: 2.004724151339522\n",
      "step: 130000, train loss: 2.038884504485643, val loss: 2.0031527713716555\n",
      "step: 131000, train loss: 2.261305716431301, val loss: 2.004727715362281\n",
      "step: 132000, train loss: 1.8012843349441923, val loss: 2.0039919255854968\n",
      "step: 133000, train loss: 1.900370221954439, val loss: 2.014118875722235\n",
      "step: 134000, train loss: 2.0089605630590315, val loss: 2.0102309354727117\n",
      "step: 135000, train loss: 1.9458853801039038, val loss: 2.010422113443375\n",
      "step: 136000, train loss: 1.6774145110045071, val loss: 1.9956766657844405\n",
      "step: 137000, train loss: 1.8027335791188626, val loss: 2.0009683799052027\n",
      "step: 138000, train loss: 2.407746006866774, val loss: 2.0035679831865347\n",
      "step: 139000, train loss: 2.329758774984846, val loss: 2.002162826417577\n",
      "step: 140000, train loss: 1.613198325654347, val loss: 2.009878847426115\n",
      "step: 141000, train loss: 2.0596323898546745, val loss: 2.0108986693243245\n",
      "step: 142000, train loss: 2.2374826119048823, val loss: 2.0110176569819083\n",
      "step: 143000, train loss: 2.237373139003973, val loss: 2.0136804849792282\n",
      "step: 144000, train loss: 1.980003325580575, val loss: 1.9920846521740345\n",
      "step: 145000, train loss: 2.394402252540457, val loss: 1.9954089440872078\n",
      "step: 146000, train loss: 1.8678106165798514, val loss: 1.9886133018405665\n",
      "step: 147000, train loss: 1.6980093177804938, val loss: 1.9972414330129478\n",
      "step: 148000, train loss: 1.9678560165348162, val loss: 1.996739695147098\n",
      "step: 149000, train loss: 1.554765667243946, val loss: 1.9917539569927225\n",
      "step: 150000, train loss: 1.9633798685672597, val loss: 2.0083746350896954\n",
      "step: 151000, train loss: 2.2099311590031894, val loss: 1.9897950640383042\n",
      "step: 152000, train loss: 2.040029397914461, val loss: 2.008826633220777\n",
      "step: 153000, train loss: 1.7267473528302708, val loss: 2.0037266201506103\n",
      "step: 154000, train loss: 1.7429184005251248, val loss: 1.9978919304473874\n",
      "step: 155000, train loss: 2.4081583922314413, val loss: 1.996888172372166\n",
      "step: 156000, train loss: 1.8752488316842255, val loss: 2.0008676121358033\n",
      "step: 157000, train loss: 2.088675779364492, val loss: 1.9960373422267081\n",
      "step: 158000, train loss: 1.9431402069222932, val loss: 2.0041485177384746\n",
      "step: 159000, train loss: 2.1136983063796446, val loss: 2.003322617817882\n",
      "step: 160000, train loss: 2.0087495976455196, val loss: 1.9957878992368647\n",
      "step: 161000, train loss: 2.005751307885927, val loss: 1.996865305484602\n",
      "step: 162000, train loss: 2.0226315266413337, val loss: 1.994098390470084\n",
      "step: 163000, train loss: 2.18460505788625, val loss: 2.002821010211037\n",
      "step: 164000, train loss: 2.260360308570381, val loss: 1.999347822133446\n",
      "step: 165000, train loss: 1.9785255099701473, val loss: 2.0000460660200496\n",
      "step: 166000, train loss: 1.7610971162343492, val loss: 2.000891487951823\n",
      "step: 167000, train loss: 2.540064498006888, val loss: 2.014600586260324\n",
      "step: 168000, train loss: 2.4115126921139596, val loss: 2.0072290711322087\n",
      "step: 169000, train loss: 1.808453818695305, val loss: 2.002460672063316\n",
      "step: 170000, train loss: 1.7020230452935519, val loss: 1.9984483877051509\n",
      "step: 171000, train loss: 1.587591270029454, val loss: 2.0112890262258936\n",
      "step: 172000, train loss: 2.2305767417051716, val loss: 1.9997977181882727\n",
      "step: 173000, train loss: 2.1044387754287532, val loss: 2.00556228108596\n",
      "step: 174000, train loss: 2.2163032694726876, val loss: 2.0084227317892602\n",
      "step: 175000, train loss: 1.7552792560343675, val loss: 2.0163374571170136\n",
      "step: 176000, train loss: 1.8920297833468604, val loss: 2.013781698430609\n",
      "step: 177000, train loss: 1.7562295112054729, val loss: 1.9949620220190207\n",
      "step: 178000, train loss: 2.3917071624186095, val loss: 2.00604884558755\n",
      "step: 179000, train loss: 1.781696441428847, val loss: 2.0018140643926845\n",
      "step: 180000, train loss: 2.188849241151674, val loss: 1.9929784378883615\n",
      "step: 181000, train loss: 1.4926544359327119, val loss: 2.000014268529456\n",
      "step: 182000, train loss: 2.360123501666617, val loss: 1.9870213250823048\n",
      "step: 183000, train loss: 1.7140200636216298, val loss: 1.991730264583519\n",
      "step: 184000, train loss: 1.7959180298671225, val loss: 2.0021272941526482\n",
      "step: 185000, train loss: 1.8120976573822487, val loss: 1.989093577216841\n",
      "step: 186000, train loss: 2.406647424255674, val loss: 1.9959885054831839\n",
      "step: 187000, train loss: 1.8601786618738307, val loss: 2.001479345949762\n",
      "step: 188000, train loss: 2.4548979399731365, val loss: 1.9942212907769294\n",
      "step: 189000, train loss: 1.8367299244449997, val loss: 1.9800718607429122\n",
      "step: 190000, train loss: 2.003601695844892, val loss: 1.9892550948756766\n",
      "step: 191000, train loss: 2.170484819737248, val loss: 1.9890664302541525\n",
      "step: 192000, train loss: 1.941830642180197, val loss: 1.9858082319627544\n",
      "step: 193000, train loss: 2.53418881182846, val loss: 1.9910386531945017\n",
      "step: 194000, train loss: 1.9571303315976516, val loss: 1.994984784638989\n",
      "step: 195000, train loss: 2.0402259550444963, val loss: 1.9920854077850154\n",
      "step: 196000, train loss: 2.2158219129635173, val loss: 1.9760332158436276\n",
      "step: 197000, train loss: 2.0043300688020786, val loss: 1.9955609009945654\n",
      "step: 198000, train loss: 1.972192838097954, val loss: 1.995355866575449\n",
      "step: 199000, train loss: 2.5780091684845257, val loss: 1.9867149528031116\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(42)\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "vocab_size = 27\n",
    "block_size = 3\n",
    "n_layer = 5\n",
    "dtype = torch.float64\n",
    "eval_interval = 1000\n",
    "\n",
    "# model\n",
    "C = torch.randn(vocab_size, n_embd, dtype=dtype, generator=g)\n",
    "layers = [Linear(n_embd * block_size, n_hidden, bias=False, dtype=dtype, generator=g), BatchNorm1d(n_hidden, dtype=dtype), Tanh()]\n",
    "for _ in range(n_layer-2):\n",
    "    layers.extend([Linear(n_hidden, n_hidden, bias=False, dtype=dtype, generator=g), BatchNorm1d(n_hidden, dtype=dtype), Tanh()])\n",
    "layers.extend([Linear(n_hidden, vocab_size, bias=False, dtype=dtype, generator=g), BatchNorm1d(vocab_size, dtype=dtype)])\n",
    "params = [C] + [p for l in layers for p in l.parameters()]\n",
    "print(f'number of params: {sum(p.numel() for p in params) / 1e6:.2f}M')\n",
    "layers[-1].weight.data *= 0.1 # less confident\n",
    "# loss\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 200000\n",
    "ini_lr = 1.0\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # forward\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    h = emb\n",
    "    for l in layers:\n",
    "        h = l(h)\n",
    "    logits = h\n",
    "    # 2. loss\n",
    "    loss = loss_fn(logits, y)\n",
    "\n",
    "    # backward\n",
    "    # 1. zero grad\n",
    "    emb_grad = torch.zeros(bs, n_embd * block_size, dtype=dtype)\n",
    "    C_grad = torch.zeros(vocab_size, n_embd, dtype=dtype)\n",
    "    # 2. backward\n",
    "    # loss\n",
    "    h_grad = loss_fn.backward(grad=1.0) # last layer, dloss=1.0\n",
    "    # layers\n",
    "    for i in range(len(layers)-1, -1, -1):\n",
    "        h_grad = layers[i].backward(h_grad)\n",
    "    # embedding\n",
    "    emb_grad = h_grad\n",
    "    C_grad.index_add_(dim=0, index=x.view(-1), source=emb_grad.view(-1, n_embd))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        x, y = X_val[:128], Y_val[:128] # TODO: large bs, batchnorm is slow\n",
    "        emb = C[x].view(x.shape[0], -1)\n",
    "        h = emb\n",
    "        for l in layers:\n",
    "            h = l(h)\n",
    "        val_loss = loss_fn(h, y)\n",
    "        print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    param_grads = [C_grad] + [p for l in layers for p in l.grads()]\n",
    "    for p, g in zip(params, param_grads):\n",
    "        p.data -= lr * g\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 2.0650913516589435\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "n_batches = X_test.shape[0] // 128\n",
    "for _ in range(n_batches):\n",
    "    x, y = X_test[:128], Y_test[:128] # TODO: large bs, batchnorm is slow\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    h = emb\n",
    "    for l in layers:\n",
    "        h = l(h)\n",
    "    test_loss += loss_fn(h, y).item()\n",
    "print(f'test loss: {test_loss / n_batches}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 0.13M\n",
      "step: 0, train loss: 3.2882673916139513, val loss: 3.3037251816090474\n",
      "step: 1000, train loss: 2.789021951571414, val loss: 2.3904028412396934\n",
      "step: 2000, train loss: 2.5281986182836484, val loss: 2.3126000885979647\n",
      "step: 3000, train loss: 2.028010538265241, val loss: 2.3406512652021454\n",
      "step: 4000, train loss: 2.594937313936474, val loss: 2.3070987366103233\n",
      "step: 5000, train loss: 2.2496904227728094, val loss: 2.3046101823543435\n",
      "step: 6000, train loss: 2.449963955274707, val loss: 2.3342762225983242\n",
      "step: 7000, train loss: 2.255257099008804, val loss: 2.3327550445741556\n",
      "step: 8000, train loss: 2.583384458214401, val loss: 2.299149957745203\n",
      "step: 9000, train loss: 1.9728677558938865, val loss: 2.236508766539423\n",
      "step: 10000, train loss: 1.9312031792806503, val loss: 2.268868202448988\n",
      "step: 11000, train loss: 2.444830721886661, val loss: 2.2871470263842646\n",
      "step: 12000, train loss: 2.685619910303268, val loss: 2.227653722602827\n",
      "step: 13000, train loss: 2.1739196381915042, val loss: 2.2113667936542516\n",
      "step: 14000, train loss: 2.0703842757681006, val loss: 2.3043125964881845\n",
      "step: 15000, train loss: 2.509908521320541, val loss: 2.1564362310020426\n",
      "step: 16000, train loss: 2.111383178086854, val loss: 2.24949108896703\n",
      "step: 17000, train loss: 2.0674913378271484, val loss: 2.2312334793148785\n",
      "step: 18000, train loss: 2.2670118174760088, val loss: 2.216650636069429\n",
      "step: 19000, train loss: 2.032363382702211, val loss: 2.2181783987326975\n",
      "step: 20000, train loss: 2.0836385505870463, val loss: 2.214992936187297\n",
      "step: 21000, train loss: 2.334929512101264, val loss: 2.1986976529515667\n",
      "step: 22000, train loss: 2.0086856867746667, val loss: 2.209264026023683\n",
      "step: 23000, train loss: 2.3876769747112703, val loss: 2.1929619133269953\n",
      "step: 24000, train loss: 2.0594661869249973, val loss: 2.153989151022101\n",
      "step: 25000, train loss: 2.3355224076138437, val loss: 2.177598558644735\n",
      "step: 26000, train loss: 2.3062644509630816, val loss: 2.2051519999106284\n",
      "step: 27000, train loss: 2.299323521095009, val loss: 2.1332238546004865\n",
      "step: 28000, train loss: 2.1950411829666625, val loss: 2.1539030103573067\n",
      "step: 29000, train loss: 2.128165238442781, val loss: 2.245146848263534\n",
      "step: 30000, train loss: 2.1146516855644277, val loss: 2.1642233636052577\n",
      "step: 31000, train loss: 2.10125022653858, val loss: 2.17689111844291\n",
      "step: 32000, train loss: 2.087241494583121, val loss: 2.1775046427246836\n",
      "step: 33000, train loss: 1.7322271078860751, val loss: 2.1670257090936227\n",
      "step: 34000, train loss: 2.359115542886606, val loss: 2.180930574555811\n",
      "step: 35000, train loss: 1.9425238880698312, val loss: 2.162049842942746\n",
      "step: 36000, train loss: 1.878196345341716, val loss: 2.1201377689880543\n",
      "step: 37000, train loss: 2.2519076173877557, val loss: 2.2036190574594268\n",
      "step: 38000, train loss: 1.8313624684670353, val loss: 2.1554533927816313\n",
      "step: 39000, train loss: 2.375457940005002, val loss: 2.1190310777141845\n",
      "step: 40000, train loss: 2.3607780629352275, val loss: 2.08065893665315\n",
      "step: 41000, train loss: 1.8500533614863024, val loss: 2.1448153045871425\n",
      "step: 42000, train loss: 2.4646989974234055, val loss: 2.1101667998237303\n",
      "step: 43000, train loss: 1.8888450619692867, val loss: 2.132549346185762\n",
      "step: 44000, train loss: 2.079736809285532, val loss: 2.1361948798423227\n",
      "step: 45000, train loss: 2.131643804395495, val loss: 2.1381420626035026\n",
      "step: 46000, train loss: 2.215294519001462, val loss: 2.1197076829826114\n",
      "step: 47000, train loss: 2.2392009648222535, val loss: 2.1591082893527114\n",
      "step: 48000, train loss: 2.1766429117542443, val loss: 2.1985349695243412\n",
      "step: 49000, train loss: 2.2064373484397715, val loss: 2.1627600530858775\n",
      "step: 50000, train loss: 1.9245245241448699, val loss: 2.2055925339010964\n",
      "step: 51000, train loss: 2.2651473214219364, val loss: 2.122747922749468\n",
      "step: 52000, train loss: 2.045466794544719, val loss: 2.149287490405449\n",
      "step: 53000, train loss: 2.1728271857462476, val loss: 2.1203144981379745\n",
      "step: 54000, train loss: 2.0657154436920817, val loss: 2.1045419737212083\n",
      "step: 55000, train loss: 1.9184694616887925, val loss: 2.167073569991178\n",
      "step: 56000, train loss: 2.397486538887932, val loss: 2.1182924165740338\n",
      "step: 57000, train loss: 2.3923158154917004, val loss: 2.1320297758655937\n",
      "step: 58000, train loss: 1.948979716638475, val loss: 2.1211079167626457\n",
      "step: 59000, train loss: 2.3181540616594107, val loss: 2.1391235894749463\n",
      "step: 60000, train loss: 2.2924648327128594, val loss: 2.1394729504182566\n",
      "step: 61000, train loss: 2.1624289553381377, val loss: 2.1567399859550185\n",
      "step: 62000, train loss: 1.934191287150592, val loss: 2.1312596276168043\n",
      "step: 63000, train loss: 1.989311832880738, val loss: 2.0976081528041295\n",
      "step: 64000, train loss: 2.4815834502850804, val loss: 2.0993978098467476\n",
      "step: 65000, train loss: 1.9818434075216678, val loss: 2.054405910281783\n",
      "step: 66000, train loss: 2.2018678516405137, val loss: 2.1077946644297056\n",
      "step: 67000, train loss: 2.1051854216010524, val loss: 2.1027848252477472\n",
      "step: 68000, train loss: 2.286004430816653, val loss: 2.0884205506413247\n",
      "step: 69000, train loss: 1.9977609719452827, val loss: 2.088751345959217\n",
      "step: 70000, train loss: 2.2118492444074236, val loss: 2.119971963409614\n",
      "step: 71000, train loss: 2.1937168007913876, val loss: 2.131478342155502\n",
      "step: 72000, train loss: 2.1360283225148056, val loss: 2.068469077582515\n",
      "step: 73000, train loss: 2.0603944817300697, val loss: 2.026285345202826\n",
      "step: 74000, train loss: 2.173476382204191, val loss: 2.0220920872324144\n",
      "step: 75000, train loss: 2.079769294497352, val loss: 2.0660343608635467\n",
      "step: 76000, train loss: 1.894292655904519, val loss: 2.0590475458354685\n",
      "step: 77000, train loss: 2.317684046678281, val loss: 2.048728300633545\n",
      "step: 78000, train loss: 2.081745128609368, val loss: 2.0758186592114187\n",
      "step: 79000, train loss: 2.517709564232862, val loss: 2.058997235924605\n",
      "step: 80000, train loss: 1.9560743342545268, val loss: 2.073435419932209\n",
      "step: 81000, train loss: 1.7926050415325125, val loss: 2.0878707079327334\n",
      "step: 82000, train loss: 2.1795689929994135, val loss: 2.073081844255019\n",
      "step: 83000, train loss: 1.9437479256315553, val loss: 2.082739016676972\n",
      "step: 84000, train loss: 2.306461920473762, val loss: 2.092863503205791\n",
      "step: 85000, train loss: 1.9658387437401723, val loss: 2.09948242618637\n",
      "step: 86000, train loss: 2.110767217971286, val loss: 2.048100760807112\n",
      "step: 87000, train loss: 1.839676341622884, val loss: 2.0486501505060213\n",
      "step: 88000, train loss: 2.3995739560263467, val loss: 2.0782866962701556\n",
      "step: 89000, train loss: 2.129791977610265, val loss: 2.076195923016984\n",
      "step: 90000, train loss: 2.1688117547321912, val loss: 2.095504670912933\n",
      "step: 91000, train loss: 2.05843094911077, val loss: 2.0377787629935087\n",
      "step: 92000, train loss: 2.5248100057754304, val loss: 2.0762520497203942\n",
      "step: 93000, train loss: 2.0787830335337953, val loss: 2.093505440152962\n",
      "step: 94000, train loss: 2.1587901936226177, val loss: 2.081998111047219\n",
      "step: 95000, train loss: 2.59172999448831, val loss: 2.0844773411799467\n",
      "step: 96000, train loss: 2.263859846864025, val loss: 2.0366328284260837\n",
      "step: 97000, train loss: 2.30204080455904, val loss: 2.0408862197426036\n",
      "step: 98000, train loss: 2.005313169218179, val loss: 2.0355638889765917\n",
      "step: 99000, train loss: 2.1425074288982016, val loss: 2.058066492833906\n",
      "step: 100000, train loss: 2.321965141042738, val loss: 2.0560102340420077\n",
      "step: 101000, train loss: 2.464264198111417, val loss: 2.0275745496503372\n",
      "step: 102000, train loss: 2.0786282214642853, val loss: 2.024874601315111\n",
      "step: 103000, train loss: 2.403559205035421, val loss: 2.0144820922327473\n",
      "step: 104000, train loss: 1.7325997146544996, val loss: 2.0224209364205032\n",
      "step: 105000, train loss: 2.1376993102992374, val loss: 2.0327958775492583\n",
      "step: 106000, train loss: 2.041223451242544, val loss: 2.018500039042172\n",
      "step: 107000, train loss: 1.8671941326843784, val loss: 2.027934317670743\n",
      "step: 108000, train loss: 1.9922661291312853, val loss: 2.0230456333205664\n",
      "step: 109000, train loss: 2.382105347874657, val loss: 2.0160931726103617\n",
      "step: 110000, train loss: 2.1815286572233283, val loss: 2.0197651190179298\n",
      "step: 111000, train loss: 1.8467375502081842, val loss: 2.0193425529051425\n",
      "step: 112000, train loss: 2.0883889207442987, val loss: 2.0277716711851017\n",
      "step: 113000, train loss: 1.9157994872708146, val loss: 2.0207239988439527\n",
      "step: 114000, train loss: 1.705689435444047, val loss: 2.0215724294211483\n",
      "step: 115000, train loss: 2.108070967922089, val loss: 2.0285263050341817\n",
      "step: 116000, train loss: 2.0247930630651996, val loss: 2.0242750370906464\n",
      "step: 117000, train loss: 1.8770546243289243, val loss: 2.0254424940458198\n",
      "step: 118000, train loss: 2.0690712089388903, val loss: 2.0236078648344695\n",
      "step: 119000, train loss: 2.1254795300987257, val loss: 2.0170096944601354\n",
      "step: 120000, train loss: 1.761556925682729, val loss: 2.027613766067062\n",
      "step: 121000, train loss: 1.8819408832612805, val loss: 2.0320206036663935\n",
      "step: 122000, train loss: 2.200793919437383, val loss: 2.021794562175837\n",
      "step: 123000, train loss: 2.3556440090784596, val loss: 2.0214237413211116\n",
      "step: 124000, train loss: 1.8744798306129118, val loss: 2.0119121193095695\n",
      "step: 125000, train loss: 1.6789863972483798, val loss: 2.0046863476300114\n",
      "step: 126000, train loss: 1.9155809614911894, val loss: 2.0107904747167633\n",
      "step: 127000, train loss: 2.7586210140870406, val loss: 2.0146260101605584\n",
      "step: 128000, train loss: 2.0464322715602177, val loss: 2.0124971019507893\n",
      "step: 129000, train loss: 1.911446521953701, val loss: 2.0047241513406475\n",
      "step: 130000, train loss: 2.038884504485962, val loss: 2.003152771372748\n",
      "step: 131000, train loss: 2.2613057164315853, val loss: 2.0047277153636314\n",
      "step: 132000, train loss: 1.8012843349454348, val loss: 2.0039919255865883\n",
      "step: 133000, train loss: 1.900370221954116, val loss: 2.014118875723369\n",
      "step: 134000, train loss: 2.0089605630589924, val loss: 2.0102309354740266\n",
      "step: 135000, train loss: 1.945885380103562, val loss: 2.010422113444659\n",
      "step: 136000, train loss: 1.6774145110044723, val loss: 1.9956766657857534\n",
      "step: 137000, train loss: 1.8027335791193952, val loss: 2.0009683799066864\n",
      "step: 138000, train loss: 2.4077460068664633, val loss: 2.003567983187805\n",
      "step: 139000, train loss: 2.3297587749856934, val loss: 2.0021628264187235\n",
      "step: 140000, train loss: 1.6131983256547437, val loss: 2.009878847427158\n",
      "step: 141000, train loss: 2.059632389854186, val loss: 2.0108986693253823\n",
      "step: 142000, train loss: 2.237482611905298, val loss: 2.011017656983267\n",
      "step: 143000, train loss: 2.2373731390044433, val loss: 2.01368048498054\n",
      "step: 144000, train loss: 1.9800033255798188, val loss: 1.9920846521752336\n",
      "step: 145000, train loss: 2.394402252541986, val loss: 1.9954089440886118\n",
      "step: 146000, train loss: 1.8678106165799204, val loss: 1.9886133018418009\n",
      "step: 147000, train loss: 1.6980093177808862, val loss: 1.997241433014295\n",
      "step: 148000, train loss: 1.9678560165341568, val loss: 1.9967396951483627\n",
      "step: 149000, train loss: 1.5547656672445902, val loss: 1.9917539569939144\n",
      "step: 150000, train loss: 1.9633798685662913, val loss: 2.0083746350908194\n",
      "step: 151000, train loss: 2.20993115900317, val loss: 1.9897950640397004\n",
      "step: 152000, train loss: 2.040029397913708, val loss: 2.0088266332220726\n",
      "step: 153000, train loss: 1.7267473528326376, val loss: 2.0037266201517663\n",
      "step: 154000, train loss: 1.7429184005251424, val loss: 1.9978919304485923\n",
      "step: 155000, train loss: 2.4081583922322247, val loss: 1.9968881723732177\n",
      "step: 156000, train loss: 1.875248831683599, val loss: 2.0008676121368962\n",
      "step: 157000, train loss: 2.088675779362692, val loss: 1.9960373422280038\n",
      "step: 158000, train loss: 1.9431402069237393, val loss: 2.004148517739307\n",
      "step: 159000, train loss: 2.113698306380189, val loss: 2.0033226178187946\n",
      "step: 160000, train loss: 2.008749597644478, val loss: 1.995787899237859\n",
      "step: 161000, train loss: 2.0057513078853817, val loss: 1.9968653054858072\n",
      "step: 162000, train loss: 2.022631526640886, val loss: 1.9940983904711256\n",
      "step: 163000, train loss: 2.184605057886594, val loss: 2.0028210102120267\n",
      "step: 164000, train loss: 2.26036030857083, val loss: 1.9993478221343173\n",
      "step: 165000, train loss: 1.9785255099690116, val loss: 2.000046066021042\n",
      "step: 166000, train loss: 1.7610971162333553, val loss: 2.000891487952923\n",
      "step: 167000, train loss: 2.540064498007443, val loss: 2.0146005862614373\n",
      "step: 168000, train loss: 2.4115126921159886, val loss: 2.0072290711332514\n",
      "step: 169000, train loss: 1.8084538186970582, val loss: 2.002460672064212\n",
      "step: 170000, train loss: 1.70202304529262, val loss: 1.9984483877060526\n",
      "step: 171000, train loss: 1.587591270029409, val loss: 2.0112890262267906\n",
      "step: 172000, train loss: 2.2305767417057334, val loss: 1.9997977181892108\n",
      "step: 173000, train loss: 2.104438775429064, val loss: 2.0055622810867897\n",
      "step: 174000, train loss: 2.216303269472455, val loss: 2.008422731790149\n",
      "step: 175000, train loss: 1.755279256034029, val loss: 2.0163374571179857\n",
      "step: 176000, train loss: 1.8920297833469513, val loss: 2.013781698431504\n",
      "step: 177000, train loss: 1.7562295112052886, val loss: 1.9949620220197553\n",
      "step: 178000, train loss: 2.3917071624175716, val loss: 2.0060488455885523\n",
      "step: 179000, train loss: 1.7816964414287815, val loss: 2.001814064393883\n",
      "step: 180000, train loss: 2.188849241152233, val loss: 1.9929784378893902\n",
      "step: 181000, train loss: 1.4926544359329919, val loss: 2.0000142685304954\n",
      "step: 182000, train loss: 2.3601235016643427, val loss: 1.9870213250832311\n",
      "step: 183000, train loss: 1.7140200636224416, val loss: 1.9917302645842632\n",
      "step: 184000, train loss: 1.7959180298682906, val loss: 2.0021272941532917\n",
      "step: 185000, train loss: 1.8120976573818757, val loss: 1.9890935772174854\n",
      "step: 186000, train loss: 2.406647424255117, val loss: 1.9959885054839364\n",
      "step: 187000, train loss: 1.8601786618741463, val loss: 2.001479345950418\n",
      "step: 188000, train loss: 2.454897939974361, val loss: 1.9942212907775938\n",
      "step: 189000, train loss: 1.8367299244435074, val loss: 1.9800718607438026\n",
      "step: 190000, train loss: 2.0036016958454175, val loss: 1.9892550948764252\n",
      "step: 191000, train loss: 2.1704848197379474, val loss: 1.989066430254791\n",
      "step: 192000, train loss: 1.9418306421802811, val loss: 1.985808231963449\n",
      "step: 193000, train loss: 2.5341888118308242, val loss: 1.991038653195265\n",
      "step: 194000, train loss: 1.957130331596941, val loss: 1.994984784639941\n",
      "step: 195000, train loss: 2.040225955043425, val loss: 1.9920854077858803\n",
      "step: 196000, train loss: 2.2158219129632597, val loss: 1.9760332158445406\n",
      "step: 197000, train loss: 2.00433006880226, val loss: 1.9955609009954098\n",
      "step: 198000, train loss: 1.9721928380972382, val loss: 1.9953558665761184\n",
      "step: 199000, train loss: 2.5780091684829705, val loss: 1.986714952804067\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "vocab_size = 27\n",
    "block_size = 3\n",
    "n_layer = 5\n",
    "dtype = torch.float64\n",
    "eval_interval = 1000\n",
    "\n",
    "# original model\n",
    "C = torch.randn(vocab_size, n_embd, dtype=dtype, generator=g)\n",
    "layers = [Linear(n_embd * block_size, n_hidden, bias=False, dtype=dtype, generator=g), BatchNorm1d(n_hidden, dtype=dtype), Tanh()]\n",
    "for _ in range(n_layer-2):\n",
    "    layers.extend([Linear(n_hidden, n_hidden, bias=False, dtype=dtype, generator=g), BatchNorm1d(n_hidden, dtype=dtype), Tanh()])\n",
    "layers.extend([Linear(n_hidden, vocab_size, bias=False, dtype=dtype, generator=g), BatchNorm1d(vocab_size, dtype=dtype)])\n",
    "# move same weight to torch model\n",
    "layers_t = [nn.Linear(n_embd * block_size, n_hidden, bias=False, dtype=dtype), nn.BatchNorm1d(n_hidden, dtype=dtype), nn.Tanh()]\n",
    "for _ in range(n_layer-2):\n",
    "    layers_t.extend([nn.Linear(n_hidden, n_hidden, bias=False, dtype=dtype), nn.BatchNorm1d(n_hidden, dtype=dtype), nn.Tanh()])\n",
    "layers_t.extend([nn.Linear(n_hidden, vocab_size, bias=False, dtype=dtype), nn.BatchNorm1d(vocab_size, dtype=dtype)])\n",
    "for l, lt in zip(layers, layers_t):\n",
    "    if isinstance(l, (Linear, BatchNorm1d)):\n",
    "        lt.weight.data = l.weight.data.T if isinstance(l, Linear) else l.weight.data\n",
    "        if l.bias is not None:\n",
    "            lt.bias.data = l.bias.data\n",
    "layers = layers_t\n",
    "params = [C] + [p for l in layers for p in l.parameters()]\n",
    "print(f'number of params: {sum(p.numel() for p in params) / 1e6:.2f}M')\n",
    "layers[-1].weight.data *= 0.1 # less confident\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 200000\n",
    "ini_lr = 1.0\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # forward\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    h = emb\n",
    "    for l in layers:\n",
    "        h = l(h)\n",
    "    logits = h\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            x, y = X_val[:128], Y_val[:128] # TODO: large bs, batchnorm is slow\n",
    "            emb = C[x].view(x.shape[0], -1)\n",
    "            h = emb\n",
    "            for l in layers:\n",
    "                h = l(h)\n",
    "            logits = h\n",
    "            val_loss = F.cross_entropy(logits, y)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    for p in params:\n",
    "        p.data -= lr * p.grad\n",
    "        p.grad = None\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 2.0650913516591505\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_loss = 0\n",
    "    n_batches = X_test.shape[0] // 128\n",
    "    for _ in range(n_batches):\n",
    "        x, y = X_test[:128], Y_test[:128] # TODO: large bs, batchnorm is slow\n",
    "        emb = C[x].view(x.shape[0], -1)\n",
    "        h = emb\n",
    "        for l in layers:\n",
    "            h = l(h)\n",
    "        logits = h\n",
    "        test_loss += F.cross_entropy(logits, y).item()\n",
    "    print(f'test loss: {test_loss / n_batches}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
