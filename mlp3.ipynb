{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([169062, 3]),\n",
       " torch.Size([21133, 3]),\n",
       " torch.Size([21133, 3]),\n",
       " torch.Size([169062]),\n",
       " torch.Size([21133]),\n",
       " torch.Size([21133]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words = list(set(words))\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "len(words)\n",
    "\n",
    "chs = list(set(''.join(words + ['.'])))\n",
    "chs = sorted(chs, reverse=False)\n",
    "stoi = {ch: i for i, ch in enumerate(chs)}\n",
    "itos = {i: ch for i, ch in enumerate(chs)}\n",
    "\n",
    "# predict next token use previous 3 tokens\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    context = '...'\n",
    "    for ch in w + '.':\n",
    "        x = [stoi[c] for c in context]\n",
    "        y = stoi[ch]\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        context = context[1:] + ch\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "n1, n2  = int(0.8 * len(X)), int(0.9 * len(X))\n",
    "\n",
    "X_train, X_val, X_test = X.tensor_split([n1, n2])\n",
    "Y_train, Y_val, Y_test = Y.tensor_split([n1, n2])\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implement backward from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "vocab_size = 27\n",
    "block_size = 3\n",
    "\n",
    "def get_params():\n",
    "    torch.manual_seed(42)\n",
    "    C = torch.randn(vocab_size, n_embd)\n",
    "    w1 = torch.randn(n_embd * block_size, n_hidden) * (n_embd * block_size)**-0.5\n",
    "    w2 = torch.randn(n_hidden, vocab_size) * (5/3) * (n_hidden)**-0.5 * 0.1 # 0.1 is for less confident at initialization\n",
    "    b2 = torch.randn(vocab_size) * 0\n",
    "    bnw = torch.ones(n_hidden)\n",
    "    bnb = torch.zeros(n_hidden)\n",
    "    params = [C, w1, w2, b2, bnw, bnb]\n",
    "    for p in params:\n",
    "        p.requires_grad = True\n",
    "    return params\n",
    "\n",
    "params = get_params()\n",
    "C, w1, w2, b2, bnw, bnb = params\n",
    "bs = 32\n",
    "idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "x, y = X_train[idx], Y_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward and torch backward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "# forward\n",
    "emb = C[x].view(x.shape[0], -1)\n",
    "emb.retain_grad()\n",
    "hpreact = emb @ w1\n",
    "hpreact.retain_grad()\n",
    "bnmeani = mean_proj @ hpreact\n",
    "bnmeani.retain_grad()\n",
    "bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "bnstdi.retain_grad()\n",
    "hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "hpreact_bn.retain_grad()\n",
    "h = hpreact_bn.tanh()\n",
    "h.retain_grad()\n",
    "logits = h @ w2 + b2\n",
    "logits.retain_grad()\n",
    "# 2. loss\n",
    "exp_l = logits.exp()\n",
    "exp_l.retain_grad()\n",
    "count = exp_l.sum(dim=-1, keepdim=True)\n",
    "count.retain_grad()\n",
    "probs = exp_l / count\n",
    "probs.retain_grad()\n",
    "nlls = -probs.log()\n",
    "nlls.retain_grad()\n",
    "loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "\n",
    "# backward\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manual backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer grad\n",
    "nlls_grad = torch.zeros(bs, vocab_size)\n",
    "probs_grad = torch.zeros(bs, vocab_size)\n",
    "count_grad = torch.zeros(bs, 1)\n",
    "exp_l_grad = torch.zeros(bs, vocab_size)\n",
    "logits_grad = torch.zeros(bs, vocab_size)\n",
    "h_grad = torch.zeros(bs, n_hidden)\n",
    "hpreact_bn_grad = torch.zeros(bs, n_hidden)\n",
    "bnmeani_grad = torch.zeros(1, n_hidden)\n",
    "bnstdi_grad = torch.zeros(1, n_hidden)\n",
    "bnvari_grad = torch.zeros(1, n_hidden)\n",
    "hpreact_grad = torch.zeros(bs, n_hidden)\n",
    "emb_grad = torch.zeros(bs, n_embd * block_size)\n",
    "# param grad\n",
    "C_grad = torch.zeros(vocab_size, n_embd)\n",
    "w1_grad = torch.zeros(n_embd * block_size, n_hidden)\n",
    "w2_grad = torch.zeros(n_hidden, vocab_size)\n",
    "b2_grad = torch.zeros(vocab_size)\n",
    "bnw_grad = torch.zeros(n_hidden)\n",
    "bnb_grad = torch.zeros(n_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same grad for loss calculation: [True, True, True, True, True]\n",
      "same grad for logits calculation: [True, True, True, True, True, True]\n",
      "same grad for params: [True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "# 1. loss\n",
    "nlls_grad[torch.arange(y.shape[0]), y] = 1 / bs\n",
    "probs_grad[torch.arange(y.shape[0]), y] = -1 / probs.data[torch.arange(y.shape[0]), y] * nlls_grad[torch.arange(y.shape[0]), y]\n",
    "count_grad = -(exp_l.data * probs_grad).sum(dim=-1, keepdim=True) / count.data**2\n",
    "exp_l_grad = probs_grad / count.data + count_grad  # one is from e/c to e, one is from c=\\sum e to e\n",
    "logits_grad = exp_l.data * exp_l_grad\n",
    "\n",
    "# 2. logits\n",
    "h_grad = logits_grad @ w2.data.T\n",
    "hpreact_bn_grad = h_grad * (1 - h.data**2)\n",
    "# bn\n",
    "bnmeani_grad = ((-bnw.data / bnstdi.data) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "bnstdi_grad = (-((hpreact.data - bnmeani.data) * bnw.data / bnstdi.data**2) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "# hpreact\n",
    "hpreact_grad_mean = bnmeani_grad * torch.ones_like(hpreact.data) / bs\n",
    "hpreact_grad_std = bnstdi_grad * (1 / 2 / bnstdi.data) * (1 / bs) * (2 * var_proj @ hpreact.data)\n",
    "hpreact_grad_direct = hpreact_bn_grad * (bnw.data / bnstdi.data)\n",
    "hpreact_grad = hpreact_grad_mean + hpreact_grad_std + hpreact_grad_direct\n",
    "# emb\n",
    "emb_grad = hpreact_grad @ w1.data.T\n",
    "\n",
    "# 3. params\n",
    "C_grad.index_add_(dim=0, index=x.view(-1), source=emb_grad.view(-1, n_embd)) # add emb_grad[i] to C[x[i]]\n",
    "w1_grad = emb.data.T @ hpreact_grad\n",
    "w2_grad = h.data.T @ logits_grad\n",
    "b2_grad = logits_grad.sum(dim=0)\n",
    "bnw_grad = ((hpreact.data - bnmeani.data) / bnstdi.data * hpreact_bn_grad).sum(dim=0)\n",
    "bnb_grad = hpreact_bn_grad.sum(dim=0)\n",
    "\n",
    "# check\n",
    "is_equal1 = [torch.allclose(nlls_grad, nlls.grad), torch.allclose(probs_grad, probs.grad), torch.allclose(count_grad, count.grad), torch.allclose(exp_l_grad, exp_l.grad), torch.allclose(logits_grad, logits.grad)]\n",
    "is_equal2 = [torch.allclose(h_grad, h.grad), torch.allclose(hpreact_bn_grad, hpreact_bn.grad), torch.allclose(bnmeani_grad, bnmeani.grad), torch.allclose(bnstdi_grad, bnstdi.grad), torch.allclose(hpreact_grad, hpreact.grad), torch.allclose(emb_grad, emb.grad)]\n",
    "is_equal3 = [torch.allclose(C_grad, C.grad), torch.allclose(w1_grad, w1.grad), torch.allclose(w2_grad, w2.grad), torch.allclose(b2_grad, b2.grad), torch.allclose(bnw_grad, bnw.grad), torch.allclose(bnb_grad, bnb.grad)]\n",
    "print('same grad for loss calculation:', is_equal1)\n",
    "print('same grad for logits calculation:', is_equal2)\n",
    "print('same grad for params:', is_equal3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 3.3074488639831543, val loss: 3.3160440921783447\n",
      "step: 1000, train loss: 2.583611488342285, val loss: 2.4248502254486084\n",
      "step: 2000, train loss: 2.4143431186676025, val loss: 2.390408992767334\n",
      "step: 3000, train loss: 2.1222798824310303, val loss: 2.379809856414795\n",
      "step: 4000, train loss: 2.1513724327087402, val loss: 2.374274969100952\n",
      "step: 5000, train loss: 2.349586009979248, val loss: 2.386371374130249\n",
      "step: 6000, train loss: 2.2481563091278076, val loss: 2.2990658283233643\n",
      "step: 7000, train loss: 2.0973422527313232, val loss: 2.2958080768585205\n",
      "step: 8000, train loss: 2.5018582344055176, val loss: 2.2962090969085693\n",
      "step: 9000, train loss: 2.072721242904663, val loss: 2.288125991821289\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# model\n",
    "torch.manual_seed(42)\n",
    "C = torch.randn(vocab_size, n_embd)\n",
    "w1 = torch.randn(n_embd * block_size, n_hidden) * (n_embd * block_size)**-0.5\n",
    "w2 = torch.randn(n_hidden, vocab_size) * (5/3) * (n_hidden)**-0.5 * 0.1 # 0.1 is for less confident at initialization\n",
    "b2 = torch.randn(vocab_size) * 0\n",
    "bnw = torch.ones(n_hidden)\n",
    "bnb = torch.zeros(n_hidden)\n",
    "params = [C, w1, w2, b2, bnw, bnb]\n",
    "bnmean_running = torch.zeros(n_hidden)\n",
    "bnstd_running = torch.ones(n_hidden)\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 10000\n",
    "ini_lr = 1.0\n",
    "\n",
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # ---------------- forward --------------------\n",
    "    # 1. logits\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    hpreact = emb @ w1\n",
    "    bnmeani = mean_proj @ hpreact\n",
    "    bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "    hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "    h = hpreact_bn.tanh()\n",
    "    logits = h @ w2 + b2\n",
    "    # 2. loss\n",
    "    exp_l = logits.exp()\n",
    "    count = exp_l.sum(dim=-1, keepdim=True)\n",
    "    probs = exp_l / count\n",
    "    nlls = -probs.log()\n",
    "    loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "    \n",
    "\n",
    "\n",
    "    # ---------------- backward --------------------\n",
    "    # 0. zero grad\n",
    "    # buffer grad\n",
    "    nlls_grad = torch.zeros(bs, vocab_size)\n",
    "    probs_grad = torch.zeros(bs, vocab_size)\n",
    "    count_grad = torch.zeros(bs, 1)\n",
    "    exp_l_grad = torch.zeros(bs, vocab_size)\n",
    "    logits_grad = torch.zeros(bs, vocab_size)\n",
    "    h_grad = torch.zeros(bs, n_hidden)\n",
    "    hpreact_bn_grad = torch.zeros(bs, n_hidden)\n",
    "    bnmeani_grad = torch.zeros(1, n_hidden)\n",
    "    bnstdi_grad = torch.zeros(1, n_hidden)\n",
    "    bnvari_grad = torch.zeros(1, n_hidden)\n",
    "    hpreact_grad = torch.zeros(bs, n_hidden)\n",
    "    emb_grad = torch.zeros(bs, n_embd * block_size)\n",
    "    # param grad\n",
    "    C_grad = torch.zeros(vocab_size, n_embd)\n",
    "    w1_grad = torch.zeros(n_embd * block_size, n_hidden)\n",
    "    w2_grad = torch.zeros(n_hidden, vocab_size)\n",
    "    b2_grad = torch.zeros(vocab_size)\n",
    "    bnw_grad = torch.zeros(n_hidden)\n",
    "    bnb_grad = torch.zeros(n_hidden)\n",
    "\n",
    "    # 1. loss\n",
    "    nlls_grad[torch.arange(y.shape[0]), y] = 1 / bs\n",
    "    probs_grad[torch.arange(y.shape[0]), y] = -1 / probs.data[torch.arange(y.shape[0]), y] * nlls_grad[torch.arange(y.shape[0]), y]\n",
    "    count_grad = -(exp_l.data * probs_grad).sum(dim=-1, keepdim=True) / count.data**2\n",
    "    exp_l_grad = probs_grad / count.data + count_grad  # one is from e/c to e, one is from c=\\sum e to e\n",
    "    logits_grad = exp_l.data * exp_l_grad\n",
    "\n",
    "    # 2. logits\n",
    "    h_grad = logits_grad @ w2.data.T\n",
    "    hpreact_bn_grad = h_grad * (1 - h.data**2)\n",
    "    # bn\n",
    "    bnmeani_grad = ((-bnw.data / bnstdi.data) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "    bnstdi_grad = (-((hpreact.data - bnmeani.data) * bnw.data / bnstdi.data**2) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "    # hpreact\n",
    "    hpreact_grad_mean = bnmeani_grad * torch.ones_like(hpreact.data) / bs\n",
    "    hpreact_grad_std = bnstdi_grad * (1 / 2 / bnstdi.data) * (1 / bs) * (2 * var_proj @ hpreact.data)\n",
    "    hpreact_grad_direct = hpreact_bn_grad * (bnw.data / bnstdi.data)\n",
    "    hpreact_grad = hpreact_grad_mean + hpreact_grad_std + hpreact_grad_direct\n",
    "    # emb\n",
    "    emb_grad = hpreact_grad @ w1.data.T\n",
    "    \n",
    "    # 3. params\n",
    "    C_grad.index_add_(dim=0, index=x.view(-1), source=emb_grad.view(-1, n_embd)) # add emb_grad[i] to C[x[i]]\n",
    "    w1_grad = emb.data.T @ hpreact_grad\n",
    "    w2_grad = h.data.T @ logits_grad\n",
    "    b2_grad = logits_grad.sum(dim=0)\n",
    "    bnw_grad = ((hpreact.data - bnmeani.data) / bnstdi.data * hpreact_bn_grad).sum(dim=0)\n",
    "    bnb_grad = hpreact_bn_grad.sum(dim=0)\n",
    "    param_grads = [C_grad, w1_grad, w2_grad, b2_grad, bnw_grad, bnb_grad]\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            emb = C[X_val].view(X_val.shape[0], -1)\n",
    "            hpreact = emb @ w1\n",
    "            hpreact = (hpreact - hpreact.mean(dim=0, keepdim=True)) / hpreact.std(dim=0, keepdim=True) * bnw + bnb\n",
    "            h = hpreact.tanh()\n",
    "            logits = h @ w2 + b2\n",
    "            val_loss = F.cross_entropy(logits, Y_val)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    for p, g in zip(params, param_grads):\n",
    "        p.data -= lr * g\n",
    "    \n",
    "    bnmean_running = bnmean_running * 0.99 + bnmeani * 0.01\n",
    "    bnstd_running = bnstd_running * 0.99 + bnstdi * 0.01\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1794, -0.1739,  0.0228,  ...,  0.0201, -0.1145,  0.1450],\n",
       "        [-0.3108,  0.1481, -0.2057,  ..., -0.3503,  0.2971,  0.1593],\n",
       "        [-0.0755,  0.0891, -0.0533,  ..., -0.1936, -0.2113, -0.3970],\n",
       "        ...,\n",
       "        [ 0.0905, -0.2106, -0.5778,  ..., -0.1088, -0.1970, -0.3650],\n",
       "        [-0.1003,  0.1913, -0.4233,  ...,  0.3125,  0.0998, -0.2520],\n",
       "        [ 0.2542,  0.2179, -0.3316,  ..., -0.4188, -0.2184, -0.4282]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17936445772647858"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[0,0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 3.3074488639831543, val loss: 3.3160440921783447\n",
      "step: 1000, train loss: 2.583611488342285, val loss: 2.4248502254486084\n",
      "step: 2000, train loss: 2.4143431186676025, val loss: 2.390408992767334\n",
      "step: 3000, train loss: 2.1222798824310303, val loss: 2.379809856414795\n",
      "step: 4000, train loss: 2.1513726711273193, val loss: 2.374274969100952\n",
      "step: 5000, train loss: 2.349586009979248, val loss: 2.386371374130249\n",
      "step: 6000, train loss: 2.2481563091278076, val loss: 2.2990663051605225\n",
      "step: 7000, train loss: 2.0973422527313232, val loss: 2.2958080768585205\n",
      "step: 8000, train loss: 2.5018584728240967, val loss: 2.2962090969085693\n",
      "step: 9000, train loss: 2.072721242904663, val loss: 2.288125991821289\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# model\n",
    "params = get_params()\n",
    "C, w1, w2, b2, bnw, bnb = params\n",
    "bnmean_running = torch.zeros(n_hidden)\n",
    "bnstd_running = torch.ones(n_hidden)\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 10000\n",
    "ini_lr = 1.0\n",
    "\n",
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # forward\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    hpreact = emb @ w1\n",
    "    bnmeani = mean_proj @ hpreact\n",
    "    bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "    hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "    h = hpreact_bn.tanh()\n",
    "    logits = h @ w2 + b2\n",
    "    # 2. loss\n",
    "    exp_l = logits.exp()\n",
    "    count = exp_l.sum(dim=-1, keepdim=True)\n",
    "    probs = exp_l / count\n",
    "    nlls = -probs.log()\n",
    "    loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    if step % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            emb = C[X_val].view(X_val.shape[0], -1)\n",
    "            hpreact = emb @ w1\n",
    "            hpreact = (hpreact - hpreact.mean(dim=0, keepdim=True)) / hpreact.std(dim=0, keepdim=True) * bnw + bnb\n",
    "            h = hpreact.tanh()\n",
    "            logits = h @ w2 + b2\n",
    "            val_loss = F.cross_entropy(logits, Y_val)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    for p in params:\n",
    "        p.data -= lr * p.grad\n",
    "        p.grad = None\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = bnmean_running * 0.99 + bnmeani * 0.01\n",
    "        bnstd_running = bnstd_running * 0.99 + bnstdi * 0.01\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1794, -0.1739,  0.0228,  ...,  0.0201, -0.1145,  0.1450],\n",
       "        [-0.3108,  0.1481, -0.2057,  ..., -0.3503,  0.2971,  0.1593],\n",
       "        [-0.0755,  0.0891, -0.0533,  ..., -0.1936, -0.2113, -0.3970],\n",
       "        ...,\n",
       "        [ 0.0905, -0.2106, -0.5778,  ..., -0.1088, -0.1970, -0.3650],\n",
       "        [-0.1003,  0.1913, -0.4233,  ...,  0.3125,  0.0998, -0.2520],\n",
       "        [ 0.2542,  0.2179, -0.3316,  ..., -0.4188, -0.2184, -0.4282]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17936447262763977"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[0,0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simpler grad\n",
    "## BatchNorm \n",
    "\n",
    "Let $x\\in\\mathbb{R}^{n\\times d}$, $w\\in\\mathbb{R}^d$, $b\\in\\mathbb{R}^d$, define $\\bar{x} = x\\text{.mean}(\\text{dim=0})$ then\n",
    "\n",
    "$$\n",
    "    o = \\frac{x - \\bar{x}}{\\sqrt{(x - \\bar{x})^2\\text{.mean(dim=0)} + \\epsilon}} w + b \\in \\mathbb{R}^{n \\times d}\n",
    "$$\n",
    "\n",
    "Note: as torch, we don't use Bessel correction\n",
    "\n",
    "Let $s = \\sqrt{(x - \\bar{x})^2\\text{.mean(dim=0)} + \\epsilon}$ and $x_{\\text{norm}} = \\frac{x - \\bar{x}}{s}$.\n",
    "\n",
    "Denote $dx$ as grad from the end layer to current layer, $dy/dx$ as grad from next layer to current layer.\n",
    "\n",
    "$$\n",
    "    dw =  do \\cdot \\frac{do}{dw} = \\left(x_{\\text{norm}} * do\\right).~\\text{sum}(\\text{dim=0}) \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "$$\n",
    "    db =  do \\cdot \\frac{do}{db} = \\left(do\\right).~\\text{sum}(\\text{dim=0}) \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "Grad $dx$ is more complex, but if we directly use computation graph to calculate grad in scalar level, and then simplify the computation with tensor operations and algebraic transformation. It's easy to see\n",
    "\n",
    "$$\n",
    "    dx = \\left(\n",
    "            s * do - s * do\\text{.mean(dim=0)} - \\frac{1}{s} * (x - \\bar{x}) * ((x - \\bar{x}) * do).~\\text{mean}(\\text{dim=0})\n",
    "    \\right) * w * \\frac{1}{s^2}\n",
    "$$\n",
    "\n",
    "Combine same terms and use quantities already calculated in forward pass, we get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    dx &= \\left(\n",
    "            (do - do\\text{.mean(dim=0)}) - \\frac{x - \\bar{x}}{s} * \\left(\\frac{x - \\bar{x}}{s} * do\\right).~\\text{mean}(\\text{dim=0})\n",
    "    \\right) * w * \\frac{1}{s} \\\\\n",
    "    &= \\left(\n",
    "            (do - \\frac{db}{n}) - x_\\text{norm} * \\left(\\frac{dw}{n}\\right)\n",
    "    \\right) * w * \\frac{1}{s}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## LayerNorm\n",
    "\n",
    "Almost the same as BatchNorm, but we need to consider the last dim.\n",
    "\n",
    "$$\n",
    "    dw =  do \\cdot \\frac{do}{dw} = \\left(x_{\\text{norm}} * do\\right).~\\text{sum}(\\text{dim=[0,1,ndim-1]}) \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "$$\n",
    "    db =  do \\cdot \\frac{do}{db} = \\left(do\\right).~\\text{sum}(\\text{dim=[0,1,ndim-1]}) \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "$$\n",
    "    dx = \\left(\n",
    "            (do - do\\text{.mean(dim=-1)}) - \\frac{x - \\bar{x}}{s} * \\left(\\frac{x - \\bar{x}}{s} * do\\right).~\\text{mean}(\\text{dim=-1})\n",
    "    \\right) * w * \\frac{1}{s}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x grad relative error: 6.827384472361946e-16\n"
     ]
    }
   ],
   "source": [
    "# --- manual ---\n",
    "loss_fn = CrossEntropyLoss()\n",
    "x = torch.randn(100, 10, dtype=torch.float64)\n",
    "y = torch.randint(0, 10, (100,))\n",
    "loss = loss_fn(x, y)\n",
    "x_grad = loss_fn.backward(grad=1.0) # last layer, dloss=1.0\n",
    "\n",
    "\n",
    "# --- torch ---\n",
    "import torch.nn as nn\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "xt, yt = x.clone(), y.clone()\n",
    "xt.requires_grad = True\n",
    "loss = loss_fn(xt, yt)\n",
    "loss.backward()\n",
    "print(f'x grad relative error: {((xt.grad - x_grad) / xt.grad).abs().max().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass:\n",
      "o relative error: 5.649903956929978e-15\n",
      "backward pass:\n",
      "db relative error: 4.587874512547105e-16\n",
      "dw relative error: 1.4906969562791864e-15\n",
      "dx relative error: 1.8066190659672137e-14\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dtype = torch.float64\n",
    "eps = 1e-5\n",
    "# model\n",
    "bn = BatchNorm1d(10, dtype=dtype, eps=eps)\n",
    "# params\n",
    "x = torch.randn(100, 10, dtype=dtype)\n",
    "# ------- manual -------\n",
    "# forward\n",
    "o = bn(x)\n",
    "# backward\n",
    "do = torch.randn_like(o, dtype=dtype)\n",
    "dx = bn.backward(do)\n",
    "\n",
    "# ------- torch -------\n",
    "import torch.nn as nn\n",
    "bnt = nn.BatchNorm1d(10, dtype=dtype, eps=eps)\n",
    "bnt.weight.data = bn.weight.data\n",
    "bnt.bias.data = bn.bias.data\n",
    "xt = x.clone()\n",
    "xt.requires_grad = True\n",
    "# forward\n",
    "ot = bnt(xt)\n",
    "# backward\n",
    "(ot * do).sum().backward()\n",
    "\n",
    "# -------- compare -------- \n",
    "print('forward pass:')\n",
    "print(f'o relative error: {((ot - o) / ot).abs().max().item()}')\n",
    "print('backward pass:')\n",
    "print(f'db relative error: {((bnt.bias.grad - bn.bias_grad) / bnt.bias.grad).abs().max().item()}')\n",
    "print(f'dw relative error: {((bnt.weight.grad - bn.weight_grad) / bnt.weight.grad).abs().max().item()}')\n",
    "print(f'dx relative error: {((dx - xt.grad) / xt.grad).abs().max().item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass:\n",
      "o relative error: 1.1410527296324882e-11\n",
      "backward pass:\n",
      "db relative error: 8.099100649135957e-15\n",
      "dw relative error: 6.349328555149738e-15\n",
      "dx relative error: 7.678927882277915e-12\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dtype = torch.float64\n",
    "eps = 1e-5\n",
    "# model\n",
    "ln = LayerNorm(10, dtype=dtype, eps=eps)\n",
    "# params\n",
    "x = torch.randn(3, 32, 100, 10, dtype=dtype)\n",
    "# ------- manual -------\n",
    "# forward\n",
    "o = ln(x)\n",
    "# backward\n",
    "do = torch.randn_like(o, dtype=dtype)\n",
    "dx = ln.backward(do)\n",
    "\n",
    "# ------- torch -------\n",
    "import torch.nn as nn\n",
    "lnt = nn.LayerNorm(10, dtype=dtype, eps=eps)\n",
    "lnt.weight.data = ln.weight.data\n",
    "lnt.bias.data = ln.bias.data\n",
    "xt = x.clone()\n",
    "xt.requires_grad = True\n",
    "# forward\n",
    "ot = lnt(xt)\n",
    "# backward\n",
    "ot.backward(do)\n",
    "\n",
    "# -------- compare -------- \n",
    "print('forward pass:')\n",
    "print(f'o relative error: {((ot - o) / ot).abs().max().item()}')\n",
    "print('backward pass:')\n",
    "print(f'db relative error: {((lnt.bias.grad - ln.bias_grad) / lnt.bias.grad).abs().max().item()}')\n",
    "print(f'dw relative error: {((lnt.weight.grad - ln.weight_grad) / lnt.weight.grad).abs().max().item()}')\n",
    "print(f'dx relative error: {((dx - xt.grad) / xt.grad).abs().max().item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 0.72M\n",
      "check grad:\n",
      "[Layer 1] weight grad relative error: 6.348547564942509e-12\n",
      "x_grad relative error: 1.2315982383522732e-11\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "n_embd = 30\n",
    "n_hidden = 100\n",
    "bs = 32\n",
    "dtype = torch.float64\n",
    "# model\n",
    "layers = [Linear(n_embd, n_hidden, bias=False, dtype=dtype), BatchNorm1d(n_hidden, dtype=dtype), Tanh()]\n",
    "for _ in range(70):\n",
    "    layers.extend([Linear(n_hidden, n_hidden, bias=False, dtype=dtype), BatchNorm1d(n_hidden, dtype=dtype), Tanh()])\n",
    "params = [p for l in layers for p in l.parameters()]\n",
    "print(f'number of params: {sum(p.numel() for p in params) / 1e6:.2f}M')\n",
    "# input\n",
    "x = torch.randn(bs, n_embd, dtype=dtype, requires_grad=True)\n",
    "\n",
    "# --- manual ---\n",
    "# forward\n",
    "h = x\n",
    "for l in layers:\n",
    "    h = l(h)\n",
    "\n",
    "# backward\n",
    "grad = torch.ones(bs, n_hidden)\n",
    "for i in range(len(layers)-1, -1, -1):\n",
    "    grad = layers[i].backward(grad)\n",
    "\n",
    "\n",
    "# --- torch ---\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "h = x\n",
    "for l in layers:\n",
    "    h = l(h)\n",
    "h.sum().backward()\n",
    "\n",
    "# --- compare ---\n",
    "print('check grad:')\n",
    "print(f'[Layer 1] weight grad relative error: {((params[0].grad - layers[0].weight_grad) / params[0].grad).abs().max().item()}')\n",
    "print(f'x_grad relative error: {((x.grad - grad) / x.grad).abs().max().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 0.13M\n",
      "step: 0, train loss: 3.3150274193753813, val loss: 3.3026809048796713\n",
      "step: 1000, train loss: 2.256254835855975, val loss: 2.3811463460462576\n",
      "step: 2000, train loss: 2.2965042794748753, val loss: 2.3415787219274264\n",
      "step: 3000, train loss: 2.201815659887002, val loss: 2.3182530825260605\n",
      "step: 4000, train loss: 2.3066741419208485, val loss: 2.2861301353404024\n",
      "step: 5000, train loss: 2.4205356550502026, val loss: 2.297186531872086\n",
      "step: 6000, train loss: 2.2023707457509905, val loss: 2.2699604253354098\n",
      "step: 7000, train loss: 2.135934226807466, val loss: 2.2608590802941806\n",
      "step: 8000, train loss: 2.088878249927551, val loss: 2.244110076318816\n",
      "step: 9000, train loss: 2.457943763633719, val loss: 2.2463789095252906\n",
      "step: 10000, train loss: 2.2692643875001157, val loss: 2.227891313638341\n",
      "step: 11000, train loss: 2.2677978424850176, val loss: 2.217541227198858\n",
      "step: 12000, train loss: 2.3120103750129637, val loss: 2.208017717364369\n",
      "step: 13000, train loss: 2.1416754727155762, val loss: 2.209440245252597\n",
      "step: 14000, train loss: 2.5517424179627555, val loss: 2.1995048758007476\n",
      "step: 15000, train loss: 2.708265876239643, val loss: 2.2132663357980276\n",
      "step: 16000, train loss: 2.287259692044209, val loss: 2.2025563157643746\n",
      "step: 17000, train loss: 2.710897691868504, val loss: 2.194725166715359\n",
      "step: 18000, train loss: 2.222965174842648, val loss: 2.2069611477421387\n",
      "step: 19000, train loss: 2.503911996158861, val loss: 2.185993979483732\n",
      "step: 20000, train loss: 2.55736028843021, val loss: 2.1837526502588878\n",
      "step: 21000, train loss: 1.9834255403586767, val loss: 2.189738262791262\n",
      "step: 22000, train loss: 2.225371464442664, val loss: 2.175073587499021\n",
      "step: 23000, train loss: 1.9586557964197961, val loss: 2.187872228781804\n",
      "step: 24000, train loss: 2.355399044695299, val loss: 2.1778690386385144\n",
      "step: 25000, train loss: 2.4635642066227623, val loss: 2.172737674206959\n",
      "step: 26000, train loss: 2.3670907170406834, val loss: 2.1802204841483954\n",
      "step: 27000, train loss: 2.0239613867043675, val loss: 2.175986746765005\n",
      "step: 28000, train loss: 2.252813928154761, val loss: 2.17470372655278\n",
      "step: 29000, train loss: 2.316518540471127, val loss: 2.1608682333087454\n",
      "step: 30000, train loss: 2.11181510621718, val loss: 2.164011374606921\n",
      "step: 31000, train loss: 1.9634852574689106, val loss: 2.1556053129858332\n",
      "step: 32000, train loss: 1.8691002815859323, val loss: 2.1650142917498365\n",
      "step: 33000, train loss: 2.3487631477842, val loss: 2.1684138709304612\n",
      "step: 34000, train loss: 1.7624954703397806, val loss: 2.1600415964084614\n",
      "step: 35000, train loss: 1.9445754860849462, val loss: 2.147445411570637\n",
      "step: 36000, train loss: 2.2301477664205915, val loss: 2.161743439208832\n",
      "step: 37000, train loss: 1.8813069522094068, val loss: 2.1553268693850884\n",
      "step: 38000, train loss: 2.4808137607866385, val loss: 2.1537724328123304\n",
      "step: 39000, train loss: 1.7895517134118735, val loss: 2.151402480992103\n",
      "step: 40000, train loss: 2.4140365209999413, val loss: 2.146670167860137\n",
      "step: 41000, train loss: 2.160819256997442, val loss: 2.141550533728278\n",
      "step: 42000, train loss: 2.095134929002756, val loss: 2.1538445057416986\n",
      "step: 43000, train loss: 2.420877221355649, val loss: 2.1574646770090142\n",
      "step: 44000, train loss: 1.9813109709182428, val loss: 2.1431721153270464\n",
      "step: 45000, train loss: 2.0191772627019047, val loss: 2.150308421575231\n",
      "step: 46000, train loss: 2.2534205976031974, val loss: 2.13680410717453\n",
      "step: 47000, train loss: 2.181884909197277, val loss: 2.1387394413617167\n",
      "step: 48000, train loss: 1.8738457021519028, val loss: 2.145740959110091\n",
      "step: 49000, train loss: 2.2024808706212915, val loss: 2.148837341204765\n",
      "step: 50000, train loss: 2.0656448687649567, val loss: 2.1453444097494136\n",
      "step: 51000, train loss: 1.986205489816918, val loss: 2.1449777282949083\n",
      "step: 52000, train loss: 2.168148307080806, val loss: 2.137132400722845\n",
      "step: 53000, train loss: 2.0600553785158198, val loss: 2.144451254355118\n",
      "step: 54000, train loss: 2.452719767301484, val loss: 2.1546032603591216\n",
      "step: 55000, train loss: 1.6953066009671762, val loss: 2.1281944304818152\n",
      "step: 56000, train loss: 2.083011061892436, val loss: 2.130973230478668\n",
      "step: 57000, train loss: 1.6226336587937267, val loss: 2.142070387579305\n",
      "step: 58000, train loss: 2.2939950789206125, val loss: 2.1465268018213752\n",
      "step: 59000, train loss: 1.87748568517232, val loss: 2.1377957318636125\n",
      "step: 60000, train loss: 2.3227271180457647, val loss: 2.1380964870801513\n",
      "step: 61000, train loss: 2.075333882745955, val loss: 2.136450157056532\n",
      "step: 62000, train loss: 2.152696395283445, val loss: 2.134456106159018\n",
      "step: 63000, train loss: 1.9696493855549482, val loss: 2.131692708111339\n",
      "step: 64000, train loss: 1.937021246737979, val loss: 2.15046371754873\n",
      "step: 65000, train loss: 1.9081801422584737, val loss: 2.132038914443466\n",
      "step: 66000, train loss: 1.685523597928876, val loss: 2.142568720163619\n",
      "step: 67000, train loss: 2.1747166987400206, val loss: 2.129573031955327\n",
      "step: 68000, train loss: 2.130512090471942, val loss: 2.1324481266781987\n",
      "step: 69000, train loss: 2.1255405564577607, val loss: 2.1498164006522766\n",
      "step: 70000, train loss: 2.4864765892685936, val loss: 2.1409309792193647\n",
      "step: 71000, train loss: 1.929377453394717, val loss: 2.1346069410121866\n",
      "step: 72000, train loss: 1.8446259969370455, val loss: 2.1331311733749683\n",
      "step: 73000, train loss: 2.539305050787018, val loss: 2.133797350826753\n",
      "step: 74000, train loss: 2.0273722989998135, val loss: 2.131807171969549\n",
      "step: 75000, train loss: 1.553009424541211, val loss: 2.132567490858572\n",
      "step: 76000, train loss: 1.8993969589987765, val loss: 2.1216053304951457\n",
      "step: 77000, train loss: 2.1527269729675056, val loss: 2.1271761970041205\n",
      "step: 78000, train loss: 1.6384379430926672, val loss: 2.121706217167713\n",
      "step: 79000, train loss: 2.157463852758124, val loss: 2.1259166309126556\n",
      "step: 80000, train loss: 1.9456370206774656, val loss: 2.1241681873526046\n",
      "step: 81000, train loss: 1.8792241254195061, val loss: 2.131035570791631\n",
      "step: 82000, train loss: 1.8100637155727393, val loss: 2.1335801810327353\n",
      "step: 83000, train loss: 2.1333067681051956, val loss: 2.128383021704952\n",
      "step: 84000, train loss: 2.0627209341613946, val loss: 2.138481149094848\n",
      "step: 85000, train loss: 1.950772889935943, val loss: 2.132007764257168\n",
      "step: 86000, train loss: 1.7689060080739587, val loss: 2.1393296901061425\n",
      "step: 87000, train loss: 1.8798964817039199, val loss: 2.120416581224549\n",
      "step: 88000, train loss: 1.6552190251133516, val loss: 2.1269123601121565\n",
      "step: 89000, train loss: 1.7834971633976004, val loss: 2.145726619733653\n",
      "step: 90000, train loss: 2.3017138794617367, val loss: 2.129793947431725\n",
      "step: 91000, train loss: 1.9928509096627307, val loss: 2.1307529674755044\n",
      "step: 92000, train loss: 2.307670147639962, val loss: 2.130355823984129\n",
      "step: 93000, train loss: 1.8364219607734469, val loss: 2.132818711355185\n",
      "step: 94000, train loss: 2.0690592939656893, val loss: 2.1404447371029813\n",
      "step: 95000, train loss: 1.7724129563503093, val loss: 2.1250720346706036\n",
      "step: 96000, train loss: 1.9841019745635395, val loss: 2.127851879196089\n",
      "step: 97000, train loss: 2.2298952782556287, val loss: 2.126633335621033\n",
      "step: 98000, train loss: 1.9419703441021006, val loss: 2.137429093934057\n",
      "step: 99000, train loss: 1.8188535345932508, val loss: 2.1285295894980942\n",
      "step: 100000, train loss: 2.275745276113179, val loss: 2.126695602292\n",
      "step: 101000, train loss: 2.1302487096271516, val loss: 2.106262282366762\n",
      "step: 102000, train loss: 2.0540237916973183, val loss: 2.104925199741079\n",
      "step: 103000, train loss: 2.1748556100460936, val loss: 2.1015476389609207\n",
      "step: 104000, train loss: 2.2078093927021465, val loss: 2.101947599754097\n",
      "step: 105000, train loss: 2.370819696323055, val loss: 2.1012625649341725\n",
      "step: 106000, train loss: 2.284515541381687, val loss: 2.1027740997814743\n",
      "step: 107000, train loss: 1.7140379845611524, val loss: 2.0998851661602393\n",
      "step: 108000, train loss: 1.8084263951281152, val loss: 2.1007189366475147\n",
      "step: 109000, train loss: 2.2047702809434826, val loss: 2.1008286886313097\n",
      "step: 110000, train loss: 2.449484071686463, val loss: 2.097908155615781\n",
      "step: 111000, train loss: 1.9019387764557234, val loss: 2.1004678998340887\n",
      "step: 112000, train loss: 2.1592488768964655, val loss: 2.100588909817113\n",
      "step: 113000, train loss: 1.6450427992199095, val loss: 2.0993276654628366\n",
      "step: 114000, train loss: 2.0522941165489605, val loss: 2.0977711719377203\n",
      "step: 115000, train loss: 2.550470505253629, val loss: 2.0984646373021314\n",
      "step: 116000, train loss: 1.738944160660628, val loss: 2.1002683680719585\n",
      "step: 117000, train loss: 1.9136458684388729, val loss: 2.0971086929915987\n",
      "step: 118000, train loss: 2.089003834839191, val loss: 2.0975532546504434\n",
      "step: 119000, train loss: 2.2888002049674236, val loss: 2.097206337823258\n",
      "step: 120000, train loss: 1.9110624618673238, val loss: 2.100364325801204\n",
      "step: 121000, train loss: 2.027640159957618, val loss: 2.0986595255579656\n",
      "step: 122000, train loss: 2.3552108227918827, val loss: 2.098115770649742\n",
      "step: 123000, train loss: 2.0098217262705265, val loss: 2.097245191628504\n",
      "step: 124000, train loss: 1.831700783468024, val loss: 2.0989269861024455\n",
      "step: 125000, train loss: 2.1407403202935935, val loss: 2.0973207571961106\n",
      "step: 126000, train loss: 2.144263454105143, val loss: 2.0951260328922854\n",
      "step: 127000, train loss: 1.8447034904437951, val loss: 2.1000130759937634\n",
      "step: 128000, train loss: 2.022102986668378, val loss: 2.0970914624309027\n",
      "step: 129000, train loss: 1.8674030404453192, val loss: 2.0986677616526483\n",
      "step: 130000, train loss: 2.2898480560823744, val loss: 2.1000422803188945\n",
      "step: 131000, train loss: 2.0887229602512494, val loss: 2.0987291748178465\n",
      "step: 132000, train loss: 2.2677293337423743, val loss: 2.098741317082573\n",
      "step: 133000, train loss: 1.9893517777774323, val loss: 2.099367004172752\n",
      "step: 134000, train loss: 2.0270593811038355, val loss: 2.0990359376269065\n",
      "step: 135000, train loss: 2.5743078191510365, val loss: 2.095367279102275\n",
      "step: 136000, train loss: 2.2312449581899676, val loss: 2.0982794456347196\n",
      "step: 137000, train loss: 2.2257625494840387, val loss: 2.09602131923242\n",
      "step: 138000, train loss: 1.8323405790013876, val loss: 2.0978886767187834\n",
      "step: 139000, train loss: 2.1768487122988316, val loss: 2.0989184171482034\n",
      "step: 140000, train loss: 2.263398245258883, val loss: 2.101518698223162\n",
      "step: 141000, train loss: 1.9530901322783918, val loss: 2.09679028727262\n",
      "step: 142000, train loss: 1.613647192020483, val loss: 2.097078293832891\n",
      "step: 143000, train loss: 1.9593186332687957, val loss: 2.100969268263511\n",
      "step: 144000, train loss: 1.5677355450988841, val loss: 2.099550167293762\n",
      "step: 145000, train loss: 2.208903674459961, val loss: 2.0961076364977886\n",
      "step: 146000, train loss: 2.1155198440890706, val loss: 2.0954573663364684\n",
      "step: 147000, train loss: 1.9213965042093837, val loss: 2.098696213756864\n",
      "step: 148000, train loss: 2.0675971254876897, val loss: 2.0998921445500462\n",
      "step: 149000, train loss: 1.9895737087248078, val loss: 2.0953904281587237\n",
      "step: 150000, train loss: 2.1676873399387153, val loss: 2.0961694418526835\n",
      "step: 151000, train loss: 2.0930055843563666, val loss: 2.0976593760853675\n",
      "step: 152000, train loss: 1.9433107298630232, val loss: 2.096707135465204\n",
      "step: 153000, train loss: 2.0287229224456067, val loss: 2.099341579087994\n",
      "step: 154000, train loss: 1.9182063468789003, val loss: 2.095139020486355\n",
      "step: 155000, train loss: 1.823191666670495, val loss: 2.0945490535026225\n",
      "step: 156000, train loss: 2.496980296658151, val loss: 2.0969020798399685\n",
      "step: 157000, train loss: 2.170888628524507, val loss: 2.097418862625271\n",
      "step: 158000, train loss: 2.09008484210763, val loss: 2.0961884819233645\n",
      "step: 159000, train loss: 1.7625077987788607, val loss: 2.0972900044680483\n",
      "step: 160000, train loss: 1.6210892496500102, val loss: 2.0967428154306957\n",
      "step: 161000, train loss: 1.8779087275229638, val loss: 2.095382722588112\n",
      "step: 162000, train loss: 2.2698847625403067, val loss: 2.1034106223968028\n",
      "step: 163000, train loss: 2.4558493029492166, val loss: 2.097303765798368\n",
      "step: 164000, train loss: 1.9781117064667495, val loss: 2.096828975133226\n",
      "step: 165000, train loss: 1.9603921879856485, val loss: 2.0979427740030716\n",
      "step: 166000, train loss: 1.910509466693079, val loss: 2.0973712416664325\n",
      "step: 167000, train loss: 2.0606393571767048, val loss: 2.096167614698051\n",
      "step: 168000, train loss: 2.009958056754915, val loss: 2.0976214520064094\n",
      "step: 169000, train loss: 1.804424373624236, val loss: 2.0964925219277535\n",
      "step: 170000, train loss: 1.9069005549967195, val loss: 2.095323830754219\n",
      "step: 171000, train loss: 2.010418463790387, val loss: 2.0975142318344098\n",
      "step: 172000, train loss: 2.095883443903577, val loss: 2.096862285474829\n",
      "step: 173000, train loss: 2.047059915523452, val loss: 2.09709622295618\n",
      "step: 174000, train loss: 1.9482201732754554, val loss: 2.096987050516019\n",
      "step: 175000, train loss: 2.1719556273567657, val loss: 2.0964880586283106\n",
      "step: 176000, train loss: 1.8633642557444223, val loss: 2.0955679433161127\n",
      "step: 177000, train loss: 1.9899675313130736, val loss: 2.097935978796188\n",
      "step: 178000, train loss: 1.9734498540346432, val loss: 2.0987564918446444\n",
      "step: 179000, train loss: 1.9355593639824797, val loss: 2.09671331331769\n",
      "step: 180000, train loss: 1.5240287467495466, val loss: 2.0987797207044103\n",
      "step: 181000, train loss: 2.111136373041427, val loss: 2.0975949738472144\n",
      "step: 182000, train loss: 2.3066278785380168, val loss: 2.0972253336585225\n",
      "step: 183000, train loss: 2.3387345900653225, val loss: 2.0993988009395768\n",
      "step: 184000, train loss: 2.135211177928393, val loss: 2.09579844381334\n",
      "step: 185000, train loss: 2.08566928294351, val loss: 2.0992647755058353\n",
      "step: 186000, train loss: 1.8952858639675108, val loss: 2.0983980475843027\n",
      "step: 187000, train loss: 1.8451796399006712, val loss: 2.1007782620844013\n",
      "step: 188000, train loss: 2.0200436300331273, val loss: 2.096048176346198\n",
      "step: 189000, train loss: 1.5530343529514474, val loss: 2.0979921817023546\n",
      "step: 190000, train loss: 2.3111062644162166, val loss: 2.1001265710182317\n",
      "step: 191000, train loss: 2.2224529938276425, val loss: 2.0971589124108214\n",
      "step: 192000, train loss: 1.8871430236950408, val loss: 2.095394333427864\n",
      "step: 193000, train loss: 2.090783274887627, val loss: 2.0971045827764527\n",
      "step: 194000, train loss: 1.7498919888674989, val loss: 2.098105401683428\n",
      "step: 195000, train loss: 1.8912041608972228, val loss: 2.098522166458878\n",
      "step: 196000, train loss: 2.407378646977118, val loss: 2.0986572784724964\n",
      "step: 197000, train loss: 2.165373359595704, val loss: 2.0962535349218947\n",
      "step: 198000, train loss: 2.128213980864027, val loss: 2.098531752954206\n",
      "step: 199000, train loss: 2.52108605650686, val loss: 2.0985495252426096\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(42)\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "vocab_size = 27\n",
    "block_size = 3\n",
    "n_layer = 5\n",
    "dtype = torch.float64\n",
    "eval_interval = 1000\n",
    "\n",
    "# model\n",
    "C = torch.randn(vocab_size, n_embd, dtype=dtype, generator=g)\n",
    "layers = [Linear(n_embd * block_size, n_hidden, bias=False, dtype=dtype, generator=g), BatchNorm1d(n_hidden, dtype=dtype), Tanh()]\n",
    "for _ in range(n_layer-2):\n",
    "    layers.extend([Linear(n_hidden, n_hidden, bias=False, dtype=dtype, generator=g), BatchNorm1d(n_hidden, dtype=dtype), Tanh()])\n",
    "layers.extend([Linear(n_hidden, vocab_size, bias=False, dtype=dtype, generator=g), BatchNorm1d(vocab_size, dtype=dtype)])\n",
    "params = [C] + [p for l in layers for p in l.parameters()]\n",
    "print(f'number of params: {sum(p.numel() for p in params) / 1e6:.2f}M')\n",
    "layers[-1].weight.data *= 0.1 # less confident\n",
    "# loss\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 200000\n",
    "ini_lr = 1.0\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # forward\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    h = emb\n",
    "    for l in layers:\n",
    "        h = l(h)\n",
    "    logits = h\n",
    "    # 2. loss\n",
    "    loss = loss_fn(logits, y)\n",
    "\n",
    "    # backward\n",
    "    # 1. zero grad\n",
    "    emb_grad = torch.zeros(bs, n_embd * block_size, dtype=dtype)\n",
    "    C_grad = torch.zeros(vocab_size, n_embd, dtype=dtype)\n",
    "    # 2. backward\n",
    "    # loss\n",
    "    h_grad = loss_fn.backward(grad=1.0) # last layer, dloss=1.0\n",
    "    # layers\n",
    "    for i in range(len(layers)-1, -1, -1):\n",
    "        h_grad = layers[i].backward(h_grad)\n",
    "    # embedding\n",
    "    emb_grad = h_grad\n",
    "    C_grad.index_add_(dim=0, index=x.view(-1), source=emb_grad.view(-1, n_embd))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        x, y = X_val, Y_val\n",
    "        emb = C[x].view(x.shape[0], -1)\n",
    "        h = emb\n",
    "        for l in layers:\n",
    "            h = l(h)\n",
    "        val_loss = loss_fn(h, y)\n",
    "        print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    param_grads = [C_grad] + [p for l in layers for p in l.grads()]\n",
    "    for p, g in zip(params, param_grads):\n",
    "        p.data -= lr * g\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 2.0902390506091812\n"
     ]
    }
   ],
   "source": [
    "x, y = X_test, Y_test\n",
    "emb = C[x].view(x.shape[0], -1)\n",
    "h = emb\n",
    "for l in layers:\n",
    "    h = l(h)\n",
    "test_loss = loss_fn(h, y).item()\n",
    "print(f'test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 0.13M\n",
      "step: 0, train loss: 3.315027419375381, val loss: 3.3026809048796704\n",
      "step: 1000, train loss: 2.2562548358559753, val loss: 2.3811463460462567\n",
      "step: 2000, train loss: 2.296504279474876, val loss: 2.3415787219274264\n",
      "step: 3000, train loss: 2.2018156598870022, val loss: 2.318253082526061\n",
      "step: 4000, train loss: 2.3066741419208556, val loss: 2.286130135340402\n",
      "step: 5000, train loss: 2.420535655050216, val loss: 2.2971865318720854\n",
      "step: 6000, train loss: 2.202370745750994, val loss: 2.269960425335409\n",
      "step: 7000, train loss: 2.1359342268074673, val loss: 2.26085908029418\n",
      "step: 8000, train loss: 2.0888782499275456, val loss: 2.244110076318816\n",
      "step: 9000, train loss: 2.4579437636337267, val loss: 2.2463789095252906\n",
      "step: 10000, train loss: 2.2692643875001126, val loss: 2.2278913136383403\n",
      "step: 11000, train loss: 2.2677978424850225, val loss: 2.2175412271988586\n",
      "step: 12000, train loss: 2.3120103750129735, val loss: 2.208017717364369\n",
      "step: 13000, train loss: 2.1416754727155722, val loss: 2.2094402452525994\n",
      "step: 14000, train loss: 2.5517424179627533, val loss: 2.199504875800747\n",
      "step: 15000, train loss: 2.708265876239625, val loss: 2.2132663357980285\n",
      "step: 16000, train loss: 2.287259692044207, val loss: 2.2025563157643746\n",
      "step: 17000, train loss: 2.7108976918685084, val loss: 2.1947251667153607\n",
      "step: 18000, train loss: 2.2229651748426433, val loss: 2.2069611477421383\n",
      "step: 19000, train loss: 2.5039119961588328, val loss: 2.185993979483732\n",
      "step: 20000, train loss: 2.5573602884302034, val loss: 2.1837526502588864\n",
      "step: 21000, train loss: 1.983425540358679, val loss: 2.1897382627912636\n",
      "step: 22000, train loss: 2.225371464442669, val loss: 2.1750735874990204\n",
      "step: 23000, train loss: 1.958655796419796, val loss: 2.1878722287818038\n",
      "step: 24000, train loss: 2.355399044695284, val loss: 2.1778690386385136\n",
      "step: 25000, train loss: 2.4635642066227517, val loss: 2.172737674206959\n",
      "step: 26000, train loss: 2.3670907170406883, val loss: 2.1802204841483985\n",
      "step: 27000, train loss: 2.0239613867042983, val loss: 2.175986746765009\n",
      "step: 28000, train loss: 2.2528139281547754, val loss: 2.1747037265527784\n",
      "step: 29000, train loss: 2.3165185404711344, val loss: 2.160868233308745\n",
      "step: 30000, train loss: 2.1118151062172488, val loss: 2.1640113746069223\n",
      "step: 31000, train loss: 1.9634852574689385, val loss: 2.155605312985835\n",
      "step: 32000, train loss: 1.8691002815859727, val loss: 2.165014291749829\n",
      "step: 33000, train loss: 2.34876314778418, val loss: 2.1684138709304515\n",
      "step: 34000, train loss: 1.7624954703398117, val loss: 2.1600415964084623\n",
      "step: 35000, train loss: 1.9445754860849187, val loss: 2.147445411570641\n",
      "step: 36000, train loss: 2.2301477664206173, val loss: 2.1617434392088413\n",
      "step: 37000, train loss: 1.8813069522094796, val loss: 2.1553268693850836\n",
      "step: 38000, train loss: 2.4808137607866882, val loss: 2.153772432812338\n",
      "step: 39000, train loss: 1.7895517134118146, val loss: 2.1514024809920933\n",
      "step: 40000, train loss: 2.4140365209999524, val loss: 2.1466701678601394\n",
      "step: 41000, train loss: 2.1608192569975118, val loss: 2.141550533728281\n",
      "step: 42000, train loss: 2.095134929002711, val loss: 2.1538445057416853\n",
      "step: 43000, train loss: 2.4208772213556675, val loss: 2.157464677009017\n",
      "step: 44000, train loss: 1.9813109709182872, val loss: 2.143172115327057\n",
      "step: 45000, train loss: 2.0191772627020095, val loss: 2.150308421575228\n",
      "step: 46000, train loss: 2.2534205976030965, val loss: 2.136804107174532\n",
      "step: 47000, train loss: 2.1818849091971715, val loss: 2.138739441361719\n",
      "step: 48000, train loss: 1.8738457021520627, val loss: 2.1457409591100816\n",
      "step: 49000, train loss: 2.202480870621184, val loss: 2.1488373412047745\n",
      "step: 50000, train loss: 2.065644868764968, val loss: 2.1453444097494154\n",
      "step: 51000, train loss: 1.9862054898170902, val loss: 2.1449777282949345\n",
      "step: 52000, train loss: 2.1681483070807803, val loss: 2.1371324007228676\n",
      "step: 53000, train loss: 2.060055378515999, val loss: 2.1444512543551024\n",
      "step: 54000, train loss: 2.452719767301541, val loss: 2.1546032603591243\n",
      "step: 55000, train loss: 1.6953066009671984, val loss: 2.1281944304818277\n",
      "step: 56000, train loss: 2.083011061892622, val loss: 2.130973230478662\n",
      "step: 57000, train loss: 1.6226336587939214, val loss: 2.1420703875793348\n",
      "step: 58000, train loss: 2.293995078920386, val loss: 2.146526801821386\n",
      "step: 59000, train loss: 1.8774856851722528, val loss: 2.137795731863656\n",
      "step: 60000, train loss: 2.322727118045388, val loss: 2.1380964870801473\n",
      "step: 61000, train loss: 2.0753338827457406, val loss: 2.136450157056558\n",
      "step: 62000, train loss: 2.1526963952838174, val loss: 2.134456106159012\n",
      "step: 63000, train loss: 1.9696493855547743, val loss: 2.1316927081113373\n",
      "step: 64000, train loss: 1.937021246737646, val loss: 2.1504637175487815\n",
      "step: 65000, train loss: 1.9081801422581912, val loss: 2.1320389144434766\n",
      "step: 66000, train loss: 1.6855235979291878, val loss: 2.14256872016363\n",
      "step: 67000, train loss: 2.1747166987390836, val loss: 2.1295730319553474\n",
      "step: 68000, train loss: 2.1305120904700265, val loss: 2.1324481266782285\n",
      "step: 69000, train loss: 2.1255405564578944, val loss: 2.149816400652374\n",
      "step: 70000, train loss: 2.486476589267964, val loss: 2.1409309792193687\n",
      "step: 71000, train loss: 1.9293774533959163, val loss: 2.1346069410121684\n",
      "step: 72000, train loss: 1.8446259969367826, val loss: 2.1331311733749017\n",
      "step: 73000, train loss: 2.539305050786432, val loss: 2.1337973508267387\n",
      "step: 74000, train loss: 2.0273722989989635, val loss: 2.131807171969661\n",
      "step: 75000, train loss: 1.5530094245413366, val loss: 2.1325674908586323\n",
      "step: 76000, train loss: 1.8993969589999151, val loss: 2.12160533049512\n",
      "step: 77000, train loss: 2.152726972967594, val loss: 2.127176197004053\n",
      "step: 78000, train loss: 1.6384379430919436, val loss: 2.1217062171677084\n",
      "step: 79000, train loss: 2.157463852756225, val loss: 2.1259166309126463\n",
      "step: 80000, train loss: 1.9456370206778162, val loss: 2.1241681873525997\n",
      "step: 81000, train loss: 1.8792241254185948, val loss: 2.131035570791649\n",
      "step: 82000, train loss: 1.8100637155736938, val loss: 2.133580181032647\n",
      "step: 83000, train loss: 2.133306768103363, val loss: 2.1283830217050075\n",
      "step: 84000, train loss: 2.062720934161363, val loss: 2.1384811490948255\n",
      "step: 85000, train loss: 1.9507728899362844, val loss: 2.132007764257237\n",
      "step: 86000, train loss: 1.7689060080726804, val loss: 2.1393296901062673\n",
      "step: 87000, train loss: 1.879896481703163, val loss: 2.120416581224518\n",
      "step: 88000, train loss: 1.6552190251126724, val loss: 2.126912360112266\n",
      "step: 89000, train loss: 1.7834971633955992, val loss: 2.145726619733348\n",
      "step: 90000, train loss: 2.3017138794576333, val loss: 2.12979394743172\n",
      "step: 91000, train loss: 1.9928509096647284, val loss: 2.1307529674753383\n",
      "step: 92000, train loss: 2.3076701476405117, val loss: 2.1303558239839697\n",
      "step: 93000, train loss: 1.836421960772382, val loss: 2.132818711355081\n",
      "step: 94000, train loss: 2.069059293960732, val loss: 2.1404447371028845\n",
      "step: 95000, train loss: 1.7724129563497568, val loss: 2.1250720346705947\n",
      "step: 96000, train loss: 1.9841019745626887, val loss: 2.127851879195572\n",
      "step: 97000, train loss: 2.229895278258486, val loss: 2.1266333356209506\n",
      "step: 98000, train loss: 1.941970344102397, val loss: 2.1374290939339486\n",
      "step: 99000, train loss: 1.8188535345971013, val loss: 2.1285295894981764\n",
      "step: 100000, train loss: 2.275745276103315, val loss: 2.1266956022921604\n",
      "step: 101000, train loss: 2.130248709633814, val loss: 2.1062622823669184\n",
      "step: 102000, train loss: 2.0540237916982966, val loss: 2.104925199741236\n",
      "step: 103000, train loss: 2.174855610039835, val loss: 2.1015476389610477\n",
      "step: 104000, train loss: 2.207809392698465, val loss: 2.1019475997542503\n",
      "step: 105000, train loss: 2.370819696322918, val loss: 2.101262564934381\n",
      "step: 106000, train loss: 2.2845155413832825, val loss: 2.10277409978167\n",
      "step: 107000, train loss: 1.714037984563707, val loss: 2.0998851661604028\n",
      "step: 108000, train loss: 1.808426395127676, val loss: 2.100718936647649\n",
      "step: 109000, train loss: 2.204770280949965, val loss: 2.100828688631478\n",
      "step: 110000, train loss: 2.4494840716847563, val loss: 2.0979081556158805\n",
      "step: 111000, train loss: 1.9019387764528286, val loss: 2.1004678998340705\n",
      "step: 112000, train loss: 2.1592488768982965, val loss: 2.1005889098171706\n",
      "step: 113000, train loss: 1.6450427992164474, val loss: 2.099327665462833\n",
      "step: 114000, train loss: 2.0522941165526416, val loss: 2.097771171937714\n",
      "step: 115000, train loss: 2.550470505250952, val loss: 2.0984646373020848\n",
      "step: 116000, train loss: 1.7389441606616123, val loss: 2.1002683680718808\n",
      "step: 117000, train loss: 1.9136458684363953, val loss: 2.097108692991532\n",
      "step: 118000, train loss: 2.089003834834577, val loss: 2.097553254650491\n",
      "step: 119000, train loss: 2.288800204966996, val loss: 2.0972063378232626\n",
      "step: 120000, train loss: 1.9110624618737977, val loss: 2.100364325801138\n",
      "step: 121000, train loss: 2.0276401599599003, val loss: 2.0986595255579976\n",
      "step: 122000, train loss: 2.3552108227958315, val loss: 2.0981157706496325\n",
      "step: 123000, train loss: 2.0098217262748093, val loss: 2.097245191628414\n",
      "step: 124000, train loss: 1.8317007834679484, val loss: 2.0989269861024984\n",
      "step: 125000, train loss: 2.1407403202924535, val loss: 2.097320757196039\n",
      "step: 126000, train loss: 2.144263454104632, val loss: 2.095126032892189\n",
      "step: 127000, train loss: 1.8447034904440889, val loss: 2.1000130759936653\n",
      "step: 128000, train loss: 2.0221029866680604, val loss: 2.0970914624307913\n",
      "step: 129000, train loss: 1.8674030404455806, val loss: 2.0986677616525604\n",
      "step: 130000, train loss: 2.2898480560792076, val loss: 2.1000422803188\n",
      "step: 131000, train loss: 2.0887229602523165, val loss: 2.0987291748178056\n",
      "step: 132000, train loss: 2.267729333742753, val loss: 2.098741317082484\n",
      "step: 133000, train loss: 1.9893517777788767, val loss: 2.0993670041726284\n",
      "step: 134000, train loss: 2.0270593811055146, val loss: 2.0990359376268155\n",
      "step: 135000, train loss: 2.5743078191544257, val loss: 2.095367279102224\n",
      "step: 136000, train loss: 2.231244958190287, val loss: 2.0982794456346054\n",
      "step: 137000, train loss: 2.2257625494840054, val loss: 2.0960213192322845\n",
      "step: 138000, train loss: 1.832340579002339, val loss: 2.0978886767186338\n",
      "step: 139000, train loss: 2.1768487123008367, val loss: 2.0989184171480173\n",
      "step: 140000, train loss: 2.2633982452593395, val loss: 2.1015186982229963\n",
      "step: 141000, train loss: 1.9530901322791985, val loss: 2.0967902872724693\n",
      "step: 142000, train loss: 1.6136471920185431, val loss: 2.097078293832811\n",
      "step: 143000, train loss: 1.9593186332685488, val loss: 2.1009692682634378\n",
      "step: 144000, train loss: 1.567735545099275, val loss: 2.0995501672936308\n",
      "step: 145000, train loss: 2.2089036744555752, val loss: 2.0961076364977638\n",
      "step: 146000, train loss: 2.115519844090169, val loss: 2.095457366336402\n",
      "step: 147000, train loss: 1.9213965042074594, val loss: 2.098696213756896\n",
      "step: 148000, train loss: 2.067597125488177, val loss: 2.0998921445500325\n",
      "step: 149000, train loss: 1.9895737087260916, val loss: 2.0953904281586677\n",
      "step: 150000, train loss: 2.1676873399407928, val loss: 2.0961694418526644\n",
      "step: 151000, train loss: 2.0930055843575284, val loss: 2.0976593760852262\n",
      "step: 152000, train loss: 1.9433107298661252, val loss: 2.0967071354651035\n",
      "step: 153000, train loss: 2.0287229224439587, val loss: 2.09934157908791\n",
      "step: 154000, train loss: 1.91820634687851, val loss: 2.0951390204862403\n",
      "step: 155000, train loss: 1.8231916666681784, val loss: 2.0945490535024933\n",
      "step: 156000, train loss: 2.4969802966577537, val loss: 2.0969020798399547\n",
      "step: 157000, train loss: 2.1708886285224915, val loss: 2.097418862625188\n",
      "step: 158000, train loss: 2.0900848421079243, val loss: 2.0961884819233\n",
      "step: 159000, train loss: 1.7625077987817408, val loss: 2.097290004467968\n",
      "step: 160000, train loss: 1.6210892496502745, val loss: 2.0967428154306074\n",
      "step: 161000, train loss: 1.8779087275215955, val loss: 2.0953827225880852\n",
      "step: 162000, train loss: 2.2698847625395286, val loss: 2.1034106223967357\n",
      "step: 163000, train loss: 2.4558493029516235, val loss: 2.0973037657982063\n",
      "step: 164000, train loss: 1.9781117064663538, val loss: 2.096828975133127\n",
      "step: 165000, train loss: 1.9603921879847246, val loss: 2.0979427740028225\n",
      "step: 166000, train loss: 1.910509466693928, val loss: 2.0973712416663863\n",
      "step: 167000, train loss: 2.060639357176435, val loss: 2.0961676146979666\n",
      "step: 168000, train loss: 2.0099580567525512, val loss: 2.0976214520063454\n",
      "step: 169000, train loss: 1.8044243736207741, val loss: 2.0964925219276958\n",
      "step: 170000, train loss: 1.9069005550008098, val loss: 2.0953238307540736\n",
      "step: 171000, train loss: 2.0104184637899065, val loss: 2.0975142318342286\n",
      "step: 172000, train loss: 2.0958834439059877, val loss: 2.0968622854747885\n",
      "step: 173000, train loss: 2.0470599155271914, val loss: 2.097096222956082\n",
      "step: 174000, train loss: 1.9482201732746196, val loss: 2.0969870505158412\n",
      "step: 175000, train loss: 2.171955627354083, val loss: 2.0964880586281436\n",
      "step: 176000, train loss: 1.8633642557406345, val loss: 2.0955679433160226\n",
      "step: 177000, train loss: 1.9899675313129, val loss: 2.0979359787961367\n",
      "step: 178000, train loss: 1.9734498540352605, val loss: 2.098756491844541\n",
      "step: 179000, train loss: 1.935559363981625, val loss: 2.0967133133176605\n",
      "step: 180000, train loss: 1.5240287467486224, val loss: 2.0987797207042895\n",
      "step: 181000, train loss: 2.111136373042216, val loss: 2.097594973847078\n",
      "step: 182000, train loss: 2.3066278785349126, val loss: 2.0972253336584523\n",
      "step: 183000, train loss: 2.338734590070889, val loss: 2.0993988009394315\n",
      "step: 184000, train loss: 2.135211177930985, val loss: 2.095798443813303\n",
      "step: 185000, train loss: 2.0856692829436705, val loss: 2.0992647755057967\n",
      "step: 186000, train loss: 1.8952858639681571, val loss: 2.098398047584131\n",
      "step: 187000, train loss: 1.845179639899436, val loss: 2.1007782620843707\n",
      "step: 188000, train loss: 2.0200436300328204, val loss: 2.0960481763460557\n",
      "step: 189000, train loss: 1.5530343529478567, val loss: 2.097992181702219\n",
      "step: 190000, train loss: 2.3111062644140965, val loss: 2.1001265710181034\n",
      "step: 191000, train loss: 2.22245299382801, val loss: 2.0971589124107846\n",
      "step: 192000, train loss: 1.8871430236921358, val loss: 2.095394333427821\n",
      "step: 193000, train loss: 2.090783274885674, val loss: 2.097104582776325\n",
      "step: 194000, train loss: 1.7498919888685232, val loss: 2.098105401683241\n",
      "step: 195000, train loss: 1.8912041608972352, val loss: 2.0985221664588907\n",
      "step: 196000, train loss: 2.407378646976565, val loss: 2.0986572784724085\n",
      "step: 197000, train loss: 2.1653733595977442, val loss: 2.09625353492177\n",
      "step: 198000, train loss: 2.1282139808683884, val loss: 2.0985317529541034\n",
      "step: 199000, train loss: 2.5210860565052142, val loss: 2.0985495252426283\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "vocab_size = 27\n",
    "block_size = 3\n",
    "n_layer = 5\n",
    "dtype = torch.float64\n",
    "eval_interval = 1000\n",
    "\n",
    "# original model\n",
    "C = torch.randn(vocab_size, n_embd, dtype=dtype, generator=g)\n",
    "layers = [Linear(n_embd * block_size, n_hidden, bias=False, dtype=dtype, generator=g), BatchNorm1d(n_hidden, dtype=dtype), Tanh()]\n",
    "for _ in range(n_layer-2):\n",
    "    layers.extend([Linear(n_hidden, n_hidden, bias=False, dtype=dtype, generator=g), BatchNorm1d(n_hidden, dtype=dtype), Tanh()])\n",
    "layers.extend([Linear(n_hidden, vocab_size, bias=False, dtype=dtype, generator=g), BatchNorm1d(vocab_size, dtype=dtype)])\n",
    "# move same weight to torch model\n",
    "layers_t = [nn.Linear(n_embd * block_size, n_hidden, bias=False, dtype=dtype), nn.BatchNorm1d(n_hidden, dtype=dtype), nn.Tanh()]\n",
    "for _ in range(n_layer-2):\n",
    "    layers_t.extend([nn.Linear(n_hidden, n_hidden, bias=False, dtype=dtype), nn.BatchNorm1d(n_hidden, dtype=dtype), nn.Tanh()])\n",
    "layers_t.extend([nn.Linear(n_hidden, vocab_size, bias=False, dtype=dtype), nn.BatchNorm1d(vocab_size, dtype=dtype)])\n",
    "for l, lt in zip(layers, layers_t):\n",
    "    if isinstance(l, (Linear, BatchNorm1d)):\n",
    "        lt.weight.data = l.weight.data.T if isinstance(l, Linear) else l.weight.data\n",
    "        if l.bias is not None:\n",
    "            lt.bias.data = l.bias.data\n",
    "layers = layers_t\n",
    "params = [C] + [p for l in layers for p in l.parameters()]\n",
    "print(f'number of params: {sum(p.numel() for p in params) / 1e6:.2f}M')\n",
    "layers[-1].weight.data *= 0.1 # less confident\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 200000\n",
    "ini_lr = 1.0\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # forward\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    h = emb\n",
    "    for l in layers:\n",
    "        h = l(h)\n",
    "    logits = h\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            x, y = X_val, Y_val\n",
    "            emb = C[x].view(x.shape[0], -1)\n",
    "            h = emb\n",
    "            for l in layers:\n",
    "                h = l(h)\n",
    "            logits = h\n",
    "            val_loss = F.cross_entropy(logits, y)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    for p in params:\n",
    "        p.data -= lr * p.grad\n",
    "        p.grad = None\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 2.0902390506093522\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    x, y = X_test, Y_test\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    h = emb\n",
    "    for l in layers:\n",
    "        h = l(h)\n",
    "    logits = h\n",
    "    test_loss = F.cross_entropy(logits, y).item()\n",
    "    print(f'test loss: {test_loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
