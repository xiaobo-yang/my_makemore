{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([169062, 3]),\n",
       " torch.Size([21133, 3]),\n",
       " torch.Size([21133, 3]),\n",
       " torch.Size([169062]),\n",
       " torch.Size([21133]),\n",
       " torch.Size([21133]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words = list(set(words))\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "len(words)\n",
    "\n",
    "chs = list(set(''.join(words + ['.'])))\n",
    "chs = sorted(chs, reverse=False)\n",
    "stoi = {ch: i for i, ch in enumerate(chs)}\n",
    "itos = {i: ch for i, ch in enumerate(chs)}\n",
    "\n",
    "# predict next token use previous 3 tokens\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    context = '...'\n",
    "    for ch in w + '.':\n",
    "        x = [stoi[c] for c in context]\n",
    "        y = stoi[ch]\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        context = context[1:] + ch\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "n1, n2  = int(0.8 * len(X)), int(0.9 * len(X))\n",
    "\n",
    "X_train, X_val, X_test = X.tensor_split([n1, n2])\n",
    "Y_train, Y_val, Y_test = Y.tensor_split([n1, n2])\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implement backward from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "vocab_size = 27\n",
    "block_size = 3\n",
    "\n",
    "def get_params():\n",
    "    torch.manual_seed(42)\n",
    "    C = torch.randn(vocab_size, n_embd)\n",
    "    w1 = torch.randn(n_embd * block_size, n_hidden) * (n_embd * block_size)**-0.5\n",
    "    w2 = torch.randn(n_hidden, vocab_size) * (5/3) * (n_hidden)**-0.5 * 0.1 # 0.1 is for less confident at initialization\n",
    "    b2 = torch.randn(vocab_size) * 0\n",
    "    bnw = torch.ones(n_hidden)\n",
    "    bnb = torch.zeros(n_hidden)\n",
    "    params = [C, w1, w2, b2, bnw, bnb]\n",
    "    for p in params:\n",
    "        p.requires_grad = True\n",
    "    return params\n",
    "\n",
    "params = get_params()\n",
    "C, w1, w2, b2, bnw, bnb = params\n",
    "bs = 32\n",
    "idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "x, y = X_train[idx], Y_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward and torch backward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "# forward\n",
    "emb = C[x].view(x.shape[0], -1)\n",
    "emb.retain_grad()\n",
    "hpreact = emb @ w1\n",
    "hpreact.retain_grad()\n",
    "bnmeani = mean_proj @ hpreact\n",
    "bnmeani.retain_grad()\n",
    "bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "bnstdi.retain_grad()\n",
    "hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "hpreact_bn.retain_grad()\n",
    "h = hpreact_bn.tanh()\n",
    "h.retain_grad()\n",
    "logits = h @ w2 + b2\n",
    "logits.retain_grad()\n",
    "# 2. loss\n",
    "exp_l = logits.exp()\n",
    "exp_l.retain_grad()\n",
    "count = exp_l.sum(dim=-1, keepdim=True)\n",
    "count.retain_grad()\n",
    "probs = exp_l / count\n",
    "probs.retain_grad()\n",
    "nlls = -probs.log()\n",
    "nlls.retain_grad()\n",
    "loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "\n",
    "# backward\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manual backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer grad\n",
    "nlls_grad = torch.zeros(bs, vocab_size)\n",
    "probs_grad = torch.zeros(bs, vocab_size)\n",
    "count_grad = torch.zeros(bs, 1)\n",
    "exp_l_grad = torch.zeros(bs, vocab_size)\n",
    "logits_grad = torch.zeros(bs, vocab_size)\n",
    "h_grad = torch.zeros(bs, n_hidden)\n",
    "hpreact_bn_grad = torch.zeros(bs, n_hidden)\n",
    "bnmeani_grad = torch.zeros(1, n_hidden)\n",
    "bnstdi_grad = torch.zeros(1, n_hidden)\n",
    "bnvari_grad = torch.zeros(1, n_hidden)\n",
    "hpreact_grad = torch.zeros(bs, n_hidden)\n",
    "emb_grad = torch.zeros(bs, n_embd * block_size)\n",
    "# param grad\n",
    "C_grad = torch.zeros(vocab_size, n_embd)\n",
    "w1_grad = torch.zeros(n_embd * block_size, n_hidden)\n",
    "w2_grad = torch.zeros(n_hidden, vocab_size)\n",
    "b2_grad = torch.zeros(vocab_size)\n",
    "bnw_grad = torch.zeros(n_hidden)\n",
    "bnb_grad = torch.zeros(n_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same grad for loss calculation: [True, True, True, True, True]\n",
      "same grad for logits calculation: [True, True, True, True, True, True]\n",
      "same grad for params: [True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "# 1. loss\n",
    "nlls_grad[torch.arange(y.shape[0]), y] = 1 / bs\n",
    "probs_grad[torch.arange(y.shape[0]), y] = -1 / probs.data[torch.arange(y.shape[0]), y] * nlls_grad[torch.arange(y.shape[0]), y]\n",
    "count_grad = -(exp_l.data * probs_grad).sum(dim=-1, keepdim=True) / count.data**2\n",
    "exp_l_grad = probs_grad / count.data + count_grad  # one is from e/c to e, one is from c=\\sum e to e\n",
    "logits_grad = exp_l.data * exp_l_grad\n",
    "\n",
    "# 2. logits\n",
    "h_grad = logits_grad @ w2.data.T\n",
    "hpreact_bn_grad = h_grad * (1 - h.data**2)\n",
    "# bn\n",
    "bnmeani_grad = ((-bnw.data / bnstdi.data) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "bnstdi_grad = (-((hpreact.data - bnmeani.data) * bnw.data / bnstdi.data**2) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "# hpreact\n",
    "hpreact_grad_mean = bnmeani_grad * torch.ones_like(hpreact.data) / bs\n",
    "hpreact_grad_std = bnstdi_grad * (1 / 2 / bnstdi.data) * (1 / bs) * (2 * var_proj @ hpreact.data)\n",
    "hpreact_grad_direct = hpreact_bn_grad * (bnw.data / bnstdi.data)\n",
    "hpreact_grad = hpreact_grad_mean + hpreact_grad_std + hpreact_grad_direct\n",
    "# emb\n",
    "emb_grad = hpreact_grad @ w1.data.T\n",
    "\n",
    "# 3. params\n",
    "C_grad.index_add_(dim=0, index=x.view(-1), source=emb_grad.view(-1, n_embd)) # add emb_grad[i] to C[x[i]]\n",
    "w1_grad = emb.data.T @ hpreact_grad\n",
    "w2_grad = h.data.T @ logits_grad\n",
    "b2_grad = logits_grad.sum(dim=0)\n",
    "bnw_grad = ((hpreact.data - bnmeani.data) / bnstdi.data * hpreact_bn_grad).sum(dim=0)\n",
    "bnb_grad = hpreact_bn_grad.sum(dim=0)\n",
    "\n",
    "# check\n",
    "is_equal1 = [torch.allclose(nlls_grad, nlls.grad), torch.allclose(probs_grad, probs.grad), torch.allclose(count_grad, count.grad), torch.allclose(exp_l_grad, exp_l.grad), torch.allclose(logits_grad, logits.grad)]\n",
    "is_equal2 = [torch.allclose(h_grad, h.grad), torch.allclose(hpreact_bn_grad, hpreact_bn.grad), torch.allclose(bnmeani_grad, bnmeani.grad), torch.allclose(bnstdi_grad, bnstdi.grad), torch.allclose(hpreact_grad, hpreact.grad), torch.allclose(emb_grad, emb.grad)]\n",
    "is_equal3 = [torch.allclose(C_grad, C.grad), torch.allclose(w1_grad, w1.grad), torch.allclose(w2_grad, w2.grad), torch.allclose(b2_grad, b2.grad), torch.allclose(bnw_grad, bnw.grad), torch.allclose(bnb_grad, bnb.grad)]\n",
    "print('same grad for loss calculation:', is_equal1)\n",
    "print('same grad for logits calculation:', is_equal2)\n",
    "print('same grad for params:', is_equal3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 3.3074488639831543, val loss: 3.3160440921783447\n",
      "step: 1000, train loss: 2.583611488342285, val loss: 2.4248502254486084\n",
      "step: 2000, train loss: 2.4143431186676025, val loss: 2.390408992767334\n",
      "step: 3000, train loss: 2.1222798824310303, val loss: 2.379809856414795\n",
      "step: 4000, train loss: 2.1513724327087402, val loss: 2.374274969100952\n",
      "step: 5000, train loss: 2.349586009979248, val loss: 2.386371374130249\n",
      "step: 6000, train loss: 2.2481563091278076, val loss: 2.2990658283233643\n",
      "step: 7000, train loss: 2.0973422527313232, val loss: 2.2958080768585205\n",
      "step: 8000, train loss: 2.5018582344055176, val loss: 2.2962090969085693\n",
      "step: 9000, train loss: 2.072721242904663, val loss: 2.288125991821289\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# model\n",
    "torch.manual_seed(42)\n",
    "C = torch.randn(vocab_size, n_embd)\n",
    "w1 = torch.randn(n_embd * block_size, n_hidden) * (n_embd * block_size)**-0.5\n",
    "w2 = torch.randn(n_hidden, vocab_size) * (5/3) * (n_hidden)**-0.5 * 0.1 # 0.1 is for less confident at initialization\n",
    "b2 = torch.randn(vocab_size) * 0\n",
    "bnw = torch.ones(n_hidden)\n",
    "bnb = torch.zeros(n_hidden)\n",
    "params = [C, w1, w2, b2, bnw, bnb]\n",
    "bnmean_running = torch.zeros(n_hidden)\n",
    "bnstd_running = torch.ones(n_hidden)\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 10000\n",
    "ini_lr = 1.0\n",
    "\n",
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # ---------------- forward --------------------\n",
    "    # 1. logits\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    hpreact = emb @ w1\n",
    "    bnmeani = mean_proj @ hpreact\n",
    "    bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "    hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "    h = hpreact_bn.tanh()\n",
    "    logits = h @ w2 + b2\n",
    "    # 2. loss\n",
    "    exp_l = logits.exp()\n",
    "    count = exp_l.sum(dim=-1, keepdim=True)\n",
    "    probs = exp_l / count\n",
    "    nlls = -probs.log()\n",
    "    loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "    \n",
    "\n",
    "\n",
    "    # ---------------- backward --------------------\n",
    "    # 0. zero grad\n",
    "    # buffer grad\n",
    "    nlls_grad = torch.zeros(bs, vocab_size)\n",
    "    probs_grad = torch.zeros(bs, vocab_size)\n",
    "    count_grad = torch.zeros(bs, 1)\n",
    "    exp_l_grad = torch.zeros(bs, vocab_size)\n",
    "    logits_grad = torch.zeros(bs, vocab_size)\n",
    "    h_grad = torch.zeros(bs, n_hidden)\n",
    "    hpreact_bn_grad = torch.zeros(bs, n_hidden)\n",
    "    bnmeani_grad = torch.zeros(1, n_hidden)\n",
    "    bnstdi_grad = torch.zeros(1, n_hidden)\n",
    "    bnvari_grad = torch.zeros(1, n_hidden)\n",
    "    hpreact_grad = torch.zeros(bs, n_hidden)\n",
    "    emb_grad = torch.zeros(bs, n_embd * block_size)\n",
    "    # param grad\n",
    "    C_grad = torch.zeros(vocab_size, n_embd)\n",
    "    w1_grad = torch.zeros(n_embd * block_size, n_hidden)\n",
    "    w2_grad = torch.zeros(n_hidden, vocab_size)\n",
    "    b2_grad = torch.zeros(vocab_size)\n",
    "    bnw_grad = torch.zeros(n_hidden)\n",
    "    bnb_grad = torch.zeros(n_hidden)\n",
    "\n",
    "    # 1. loss\n",
    "    nlls_grad[torch.arange(y.shape[0]), y] = 1 / bs\n",
    "    probs_grad[torch.arange(y.shape[0]), y] = -1 / probs.data[torch.arange(y.shape[0]), y] * nlls_grad[torch.arange(y.shape[0]), y]\n",
    "    count_grad = -(exp_l.data * probs_grad).sum(dim=-1, keepdim=True) / count.data**2\n",
    "    exp_l_grad = probs_grad / count.data + count_grad  # one is from e/c to e, one is from c=\\sum e to e\n",
    "    logits_grad = exp_l.data * exp_l_grad\n",
    "\n",
    "    # 2. logits\n",
    "    h_grad = logits_grad @ w2.data.T\n",
    "    hpreact_bn_grad = h_grad * (1 - h.data**2)\n",
    "    # bn\n",
    "    bnmeani_grad = ((-bnw.data / bnstdi.data) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "    bnstdi_grad = (-((hpreact.data - bnmeani.data) * bnw.data / bnstdi.data**2) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "    # hpreact\n",
    "    hpreact_grad_mean = bnmeani_grad * torch.ones_like(hpreact.data) / bs\n",
    "    hpreact_grad_std = bnstdi_grad * (1 / 2 / bnstdi.data) * (1 / bs) * (2 * var_proj @ hpreact.data)\n",
    "    hpreact_grad_direct = hpreact_bn_grad * (bnw.data / bnstdi.data)\n",
    "    hpreact_grad = hpreact_grad_mean + hpreact_grad_std + hpreact_grad_direct\n",
    "    # emb\n",
    "    emb_grad = hpreact_grad @ w1.data.T\n",
    "    \n",
    "    # 3. params\n",
    "    C_grad.index_add_(dim=0, index=x.view(-1), source=emb_grad.view(-1, n_embd)) # add emb_grad[i] to C[x[i]]\n",
    "    w1_grad = emb.data.T @ hpreact_grad\n",
    "    w2_grad = h.data.T @ logits_grad\n",
    "    b2_grad = logits_grad.sum(dim=0)\n",
    "    bnw_grad = ((hpreact.data - bnmeani.data) / bnstdi.data * hpreact_bn_grad).sum(dim=0)\n",
    "    bnb_grad = hpreact_bn_grad.sum(dim=0)\n",
    "    param_grads = [C_grad, w1_grad, w2_grad, b2_grad, bnw_grad, bnb_grad]\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            emb = C[X_val].view(X_val.shape[0], -1)\n",
    "            hpreact = emb @ w1\n",
    "            hpreact = (hpreact - hpreact.mean(dim=0, keepdim=True)) / hpreact.std(dim=0, keepdim=True) * bnw + bnb\n",
    "            h = hpreact.tanh()\n",
    "            logits = h @ w2 + b2\n",
    "            val_loss = F.cross_entropy(logits, Y_val)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    for p, g in zip(params, param_grads):\n",
    "        p.data -= lr * g\n",
    "    \n",
    "    bnmean_running = bnmean_running * 0.99 + bnmeani * 0.01\n",
    "    bnstd_running = bnstd_running * 0.99 + bnstdi * 0.01\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1794, -0.1739,  0.0228,  ...,  0.0201, -0.1145,  0.1450],\n",
       "        [-0.3108,  0.1481, -0.2057,  ..., -0.3503,  0.2971,  0.1593],\n",
       "        [-0.0755,  0.0891, -0.0533,  ..., -0.1936, -0.2113, -0.3970],\n",
       "        ...,\n",
       "        [ 0.0905, -0.2106, -0.5778,  ..., -0.1088, -0.1970, -0.3650],\n",
       "        [-0.1003,  0.1913, -0.4233,  ...,  0.3125,  0.0998, -0.2520],\n",
       "        [ 0.2542,  0.2179, -0.3316,  ..., -0.4188, -0.2184, -0.4282]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17936445772647858"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[0,0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 3.3074488639831543, val loss: 3.3160440921783447\n",
      "step: 1000, train loss: 2.583611488342285, val loss: 2.4248502254486084\n",
      "step: 2000, train loss: 2.4143431186676025, val loss: 2.390408992767334\n",
      "step: 3000, train loss: 2.1222798824310303, val loss: 2.379809856414795\n",
      "step: 4000, train loss: 2.1513726711273193, val loss: 2.374274969100952\n",
      "step: 5000, train loss: 2.349586009979248, val loss: 2.386371374130249\n",
      "step: 6000, train loss: 2.2481563091278076, val loss: 2.2990663051605225\n",
      "step: 7000, train loss: 2.0973422527313232, val loss: 2.2958080768585205\n",
      "step: 8000, train loss: 2.5018584728240967, val loss: 2.2962090969085693\n",
      "step: 9000, train loss: 2.072721242904663, val loss: 2.288125991821289\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# model\n",
    "params = get_params()\n",
    "C, w1, w2, b2, bnw, bnb = params\n",
    "bnmean_running = torch.zeros(n_hidden)\n",
    "bnstd_running = torch.ones(n_hidden)\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 10000\n",
    "ini_lr = 1.0\n",
    "\n",
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # forward\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    hpreact = emb @ w1\n",
    "    bnmeani = mean_proj @ hpreact\n",
    "    bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "    hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "    h = hpreact_bn.tanh()\n",
    "    logits = h @ w2 + b2\n",
    "    # 2. loss\n",
    "    exp_l = logits.exp()\n",
    "    count = exp_l.sum(dim=-1, keepdim=True)\n",
    "    probs = exp_l / count\n",
    "    nlls = -probs.log()\n",
    "    loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    if step % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            emb = C[X_val].view(X_val.shape[0], -1)\n",
    "            hpreact = emb @ w1\n",
    "            hpreact = (hpreact - hpreact.mean(dim=0, keepdim=True)) / hpreact.std(dim=0, keepdim=True) * bnw + bnb\n",
    "            h = hpreact.tanh()\n",
    "            logits = h @ w2 + b2\n",
    "            val_loss = F.cross_entropy(logits, Y_val)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    for p in params:\n",
    "        p.data -= lr * p.grad\n",
    "        p.grad = None\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = bnmean_running * 0.99 + bnmeani * 0.01\n",
    "        bnstd_running = bnstd_running * 0.99 + bnstdi * 0.01\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1794, -0.1739,  0.0228,  ...,  0.0201, -0.1145,  0.1450],\n",
       "        [-0.3108,  0.1481, -0.2057,  ..., -0.3503,  0.2971,  0.1593],\n",
       "        [-0.0755,  0.0891, -0.0533,  ..., -0.1936, -0.2113, -0.3970],\n",
       "        ...,\n",
       "        [ 0.0905, -0.2106, -0.5778,  ..., -0.1088, -0.1970, -0.3650],\n",
       "        [-0.1003,  0.1913, -0.4233,  ...,  0.3125,  0.0998, -0.2520],\n",
       "        [ 0.2542,  0.2179, -0.3316,  ..., -0.4188, -0.2184, -0.4282]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17936447262763977"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[0,0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simpler BatchNorm Grad\n",
    "\n",
    "Let $x\\in\\mathbb{R}^{n\\times d}$, $w\\in\\mathbb{R}^d$, $b\\in\\mathbb{R}^d$, $P = I - \\frac{1}{n} \\mathbf{1} \\mathbf{1}^T \\in \\mathbb{R}^{n\\times n}$, then\n",
    "\n",
    "$$\n",
    "    v = (\\frac{1}{n} x^T P x)~.\\text{diag}() \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "$$\n",
    "    o = \\frac{Px}{\\sqrt{v + \\epsilon}} w + b \\in \\mathbb{R}^{n \\times d}\n",
    "$$\n",
    "\n",
    "Denote $dx$ as grad from the end layer to current layer, $dy/dx$ as grad from next layer to current layer.\n",
    "\n",
    "$$\n",
    "    dw =  do \\cdot \\frac{do}{dw} = \\left((Px) * do\\right).~\\text{sum}(\\text{dim=0}) / \\sqrt{v + \\epsilon} \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "$$\n",
    "    db =  do \\cdot \\frac{do}{db} = \\left(do\\right).~\\text{sum}(\\text{dim=0}) \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "Grad $dx$ is more complex, but if we directly use computation graph(see micrograd), the result will be clear.\n",
    "\n",
    "$$\n",
    "    dx = do \\cdot \\frac{\\partial o}{\\partial x} + dv \\cdot \\frac{d v}{d x}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    dv = do \\cdot \\frac{\\partial o}{\\partial v}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass:\n",
      "o relative error 1.3214495959198304e-13\n",
      "backward pass:\n",
      "b grad relative error: 0.0\n",
      "w grad relative error: 2.9079137144240243e-15\n",
      "x grad relative error: 8.759191542421218e-14\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# use double precision to avoid numerical error\n",
    "n, d = 100, 10\n",
    "# params\n",
    "x = torch.randn(n, d, dtype=torch.float64)\n",
    "w = torch.randn(d, dtype=torch.float64)\n",
    "b = torch.randn(d, dtype=torch.float64)\n",
    "# grad backprop from next layer\n",
    "do = torch.randn(n, d, dtype=torch.float64)\n",
    "\n",
    "# -------- manual --------- \n",
    "# buffer\n",
    "P = torch.eye(n, dtype=torch.float64) - torch.ones(n, n, dtype=torch.float64) / n\n",
    "# forward\n",
    "v = (x.T @ P @ x).diag() / n  # (d,) # this may waste memory when d is large\n",
    "o = (P @ x) / v.sqrt().view(1, d) * w.view(1, d) + b.view(1, d)  # (n, d)\n",
    "# backward\n",
    "w_grad = ((P @ x) * do).sum(dim=0) / v.sqrt().view(1, d) # dw\n",
    "b_grad = do.sum(dim=0) # db\n",
    "o_to_v_grad = ((o - b) * do / (-2 * v)).sum(dim=0) # do * do/dv\n",
    "v_to_x_grad = o_to_v_grad * (P @ x) * 2 / n # dv * dv / dx\n",
    "o_to_x_grad = (P @ do) * w.view(1, d) / v.sqrt().view(1, d) # do * do/dx\n",
    "x_grad = v_to_x_grad + o_to_x_grad # dx\n",
    "\n",
    "# -------- torch --------- \n",
    "xt, wt, bt = x.clone(), w.clone(), b.clone()\n",
    "for p in [xt, wt, bt]:\n",
    "    p.requires_grad = True\n",
    "ot = (xt - xt.mean(dim=0, keepdim=True)) / xt.std(dim=0, keepdim=True, unbiased=False) * wt.view(1, d) + bt.view(1, d)\n",
    "out = ot * do\n",
    "out.sum().backward()\n",
    "\n",
    "# -------- compare -------- \n",
    "print('forward pass:')\n",
    "print(f'o relative error {((ot - o) / ot).abs().max()}')\n",
    "print('backward pass:')\n",
    "print(f'b grad relative error: {((bt.grad - b_grad) / bt.grad).abs().max()}')\n",
    "print(f'w grad relative error: {((wt.grad - w_grad) / wt.grad).abs().max()}')\n",
    "print(f'x grad relative error: {((xt.grad - x_grad) / xt.grad).abs().max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check for each grad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative error: 2.4119488693758977e-15\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "n, d = 100, 10\n",
    "x = torch.randn(n, d, requires_grad=True, dtype=torch.float64)\n",
    "w = torch.randn(d, requires_grad=True, dtype=torch.float64)\n",
    "b = torch.randn(d, requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "# forward\n",
    "v = (x.T @ P @ x).diag() / n  # (d,) # this may waste memory\n",
    "\n",
    "# backward\n",
    "dv = torch.randn_like(v)\n",
    "(v * dv).sum().backward()\n",
    "v_to_x_grad = dv * (P @ x) * 2 / n\n",
    "\n",
    "err = ((x.grad - v_to_x_grad) / x.grad).abs().max().item()\n",
    "print(f'relative error: {err}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative error: 8.061833389428677e-15\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(45)\n",
    "n, d = 100, 10\n",
    "ones = torch.ones(n, dtype=torch.float64)\n",
    "x = torch.randn(n, d, requires_grad=True, dtype=torch.float64)\n",
    "w = torch.randn(d, requires_grad=True, dtype=torch.float64)\n",
    "b = torch.randn(d, requires_grad=True, dtype=torch.float64)\n",
    "v = torch.rand(d, requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "# forward\n",
    "o = (P @ x) / v.sqrt().view(1, d) * w.view(1, d) + b.view(1, d)  # (n, d)\n",
    "\n",
    "# backward\n",
    "do = torch.randn_like(o)\n",
    "(o * do).sum().backward()\n",
    "\n",
    "o_to_v_grad = ((o - b) * do / (-2 * v)).sum(dim=0)\n",
    "\n",
    "err = ((v.grad - o_to_v_grad) / v.grad).abs().max().item()\n",
    "print(f'relative error: {err}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relative error: 3.606235033323989e-13\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(45)\n",
    "n, d = 100, 10\n",
    "ones = torch.ones(n, dtype=torch.float64)\n",
    "x = torch.randn(n, d, requires_grad=True, dtype=torch.float64)\n",
    "w = torch.randn(d, requires_grad=True, dtype=torch.float64)\n",
    "b = torch.randn(d, requires_grad=True, dtype=torch.float64)\n",
    "v = torch.rand(d, requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "# forward\n",
    "o = (P @ x) / v.sqrt().view(1, d) * w.view(1, d) + b.view(1, d)  # (n, d)\n",
    "\n",
    "# backward\n",
    "do = torch.randn_like(o)\n",
    "(o * do).sum().backward()\n",
    "\n",
    "o_to_x_grad = (P @ do) * w.view(1, d) / v.sqrt().view(1, d)\n",
    "\n",
    "err = ((x.grad - o_to_x_grad) / x.grad).abs().max().item()\n",
    "print(f'relative error: {err}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Linear:\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True, dtype=torch.float64, generator=None):\n",
    "        self.weight = torch.randn(in_features, out_features, dtype=dtype, generator=generator) * (in_features)**-0.5\n",
    "        self.bias = torch.zeros(out_features, dtype=dtype) * 0 if bias else None\n",
    "        self.weight_grad = None\n",
    "        self.bias_grad = None\n",
    "\n",
    "    def parameters(self):\n",
    "        if self.bias is not None:\n",
    "            return [self.weight, self.bias]\n",
    "        else:\n",
    "            return [self.weight]\n",
    "    \n",
    "    def grads(self):\n",
    "        if self.bias is not None:\n",
    "            return [self.weight_grad, self.bias_grad]\n",
    "        else:\n",
    "            return [self.weight_grad]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.bias is not None:\n",
    "            out = x @ self.weight + self.bias\n",
    "        else:\n",
    "            out = x @ self.weight\n",
    "        return out\n",
    "    \n",
    "    def backward(self, x, out, grad):\n",
    "        \"\"\"\n",
    "            Input:\n",
    "                x: input of current layer\n",
    "                out: output of current layer\n",
    "                grad: grad from next layer\n",
    "            Output:\n",
    "                x_grad: grad back to previous layer\n",
    "        \"\"\"\n",
    "        x_grad = grad @ self.weight.T\n",
    "        self.weight_grad = x.T @ grad\n",
    "        if self.bias is not None:\n",
    "            self.bias_grad = grad.sum(dim=0)\n",
    "        return x_grad\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, in_features, eps=1e-5, momentum=0.001, dtype=torch.float64): # manual bn need fp64\n",
    "        self.weight = torch.ones(in_features, dtype=dtype)\n",
    "        self.bias = torch.zeros(in_features, dtype=dtype)\n",
    "        self.running_mean = torch.zeros(in_features, dtype=dtype)\n",
    "        self.running_var = torch.ones(in_features, dtype=dtype)\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self._training = True # internal flag\n",
    "        self.dtype = dtype\n",
    "        # backward buffer\n",
    "        self.weight_grad = None\n",
    "        self.bias_grad = None\n",
    "        self.v = None\n",
    "        self.std = None\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight, self.bias]\n",
    "    \n",
    "    def grads(self):\n",
    "        return [self.weight_grad, self.bias_grad]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        b, d = x.shape\n",
    "        if self._training:\n",
    "            Pm = torch.ones(b, dtype=self.dtype) / b # mean projection matrix (b, )\n",
    "            Pv = (torch.eye(b, dtype=self.dtype) - Pm) # var projection matrix (b, b)\n",
    "            m = Pm @ x # (d,)\n",
    "            v = (x.T @ Pv @ x).diag() / b  # (d,) # this may waste memory when d is large # TODO: very slow\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = self.running_mean * (1 - self.momentum) + m * self.momentum\n",
    "                self.running_var = self.running_var * (1 - self.momentum) + v * self.momentum\n",
    "        else:\n",
    "            m = self.running_mean\n",
    "            v = self.running_var\n",
    "        std = (v + self.eps).sqrt()\n",
    "        out = self.weight * (x - m) / std + self.bias\n",
    "        if self._training:\n",
    "            # backward buffer\n",
    "            self.v = v\n",
    "            self.std = std\n",
    "        return out\n",
    "    \n",
    "    def backward(self, x, out, grad):\n",
    "        assert self._training, 'BatchNorm1d is not in training mode'\n",
    "        b, d = x.shape\n",
    "        Pm = torch.ones(b, dtype=self.dtype) / b \n",
    "        Pv = (torch.eye(b, dtype=self.dtype) - Pm)\n",
    "        self.weight_grad = ((Pv @ x) * grad).sum(dim=0) / self.std # dw (d,)\n",
    "        self.bias_grad = grad.sum(dim=0) # db (d,)\n",
    "        o_to_v_grad = ((out - self.bias) * grad / (-2 * self.std.square())).sum(dim=0) # do * do/dv (d,)\n",
    "        v_to_x_grad = o_to_v_grad * (Pv @ x) * 2 / b # dv * dv / dx (b, d)\n",
    "        o_to_x_grad = (Pv @ grad) * self.weight / self.std # do * do/dx (b, d)\n",
    "        x_grad = v_to_x_grad + o_to_x_grad # dx (b, d)\n",
    "        return x_grad\n",
    "\n",
    "\n",
    "class Tanh:\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    def grads(self):\n",
    "        return []\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = x.tanh()\n",
    "        return out\n",
    "    \n",
    "    def backward(self, x, out, grad):\n",
    "        x_grad = grad * (1 - out**2)\n",
    "        return x_grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass:\n",
      "o relative error: 5.99848193077636e-15\n",
      "backward pass:\n",
      "db relative error: 0.0\n",
      "dw relative error: 0.0\n",
      "dx relative error: 2.7864881282787247e-14\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dtype = torch.float64\n",
    "eps = 1e-5\n",
    "# model\n",
    "bn = BatchNorm1d(10, dtype=dtype, eps=eps)\n",
    "# params\n",
    "x = torch.randn(100, 10, dtype=dtype)\n",
    "# ------- manual -------\n",
    "# forward\n",
    "o = bn(x)\n",
    "# backward\n",
    "do = torch.randn_like(o, dtype=dtype)\n",
    "dx = bn.backward(x, o, do)\n",
    "\n",
    "# ------- torch -------\n",
    "import copy\n",
    "bnt = copy.deepcopy(bn)\n",
    "xt = x.clone()\n",
    "xt.requires_grad = True\n",
    "for p in bnt.parameters():\n",
    "    p.requires_grad = True\n",
    "# forward\n",
    "ot = (xt - xt.mean(dim=0, keepdim=True)) /( xt.var(dim=0, keepdim=True, unbiased=False) + eps).sqrt() * bnt.weight + bnt.bias\n",
    "# backward\n",
    "(ot * do).sum().backward()\n",
    "\n",
    "# -------- compare -------- \n",
    "print('forward pass:')\n",
    "print(f'o relative error: {((ot - o) / ot).abs().max().item()}')\n",
    "print('backward pass:')\n",
    "print(f'db relative error: {((bnt.bias_grad - bn.bias_grad) / bnt.bias_grad).abs().max().item()}')\n",
    "print(f'dw relative error: {((bnt.weight_grad - bn.weight_grad) / bnt.weight_grad).abs().max().item()}')\n",
    "print(f'dx relative error: {((dx - xt.grad) / xt.grad).abs().max().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 0.72M\n",
      "check grad:\n",
      "[Layer 1] weight grad relative error: 1.784616404524195e-11\n",
      "x_grad relative error: 9.523983785609139e-12\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "n_embd = 30\n",
    "n_hidden = 100\n",
    "bs = 32\n",
    "dtype = torch.float64\n",
    "# model\n",
    "layers = [Linear(n_embd, n_hidden, bias=False, dtype=dtype), BatchNorm1d(n_hidden, dtype=dtype), Tanh()]\n",
    "for _ in range(70):\n",
    "    layers.extend([Linear(n_hidden, n_hidden, bias=False, dtype=dtype), BatchNorm1d(n_hidden, dtype=dtype), Tanh()])\n",
    "params = [p for l in layers for p in l.parameters()]\n",
    "print(f'number of params: {sum(p.numel() for p in params) / 1e6:.2f}M')\n",
    "# input\n",
    "x = torch.randn(bs, n_embd, dtype=dtype, requires_grad=True)\n",
    "\n",
    "# --- manual ---\n",
    "# forward\n",
    "outs = [x]\n",
    "for l in layers:\n",
    "    out = l(outs[-1])\n",
    "    outs.append(out)\n",
    "\n",
    "# backward\n",
    "grad = torch.ones(bs, n_hidden)\n",
    "for i in range(len(layers)-1, -1, -1):\n",
    "    grad = layers[i].backward(outs[i], outs[i+1], grad)\n",
    "\n",
    "\n",
    "# --- torch ---\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "h = x\n",
    "for l in layers:\n",
    "    h = l(h)\n",
    "h.sum().backward()\n",
    "\n",
    "# --- compare ---\n",
    "print('check grad:')\n",
    "print(f'[Layer 1] weight grad relative error: {((params[0].grad - layers[0].weight_grad) / params[0].grad).abs().max().item()}')\n",
    "print(f'x_grad relative error: {((x.grad - grad) / x.grad).abs().max().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 0.13M\n",
      "step: 0, train loss: 3.3138560633196645, val loss: 3.2952505718703575\n",
      "step: 1000, train loss: 2.2650234162579297, val loss: 2.3929781293073993\n",
      "step: 2000, train loss: 2.383280954053651, val loss: 2.2941183192098977\n",
      "step: 3000, train loss: 2.0633511993347464, val loss: 2.366037332412258\n",
      "step: 4000, train loss: 2.386221372864641, val loss: 2.255767476565825\n",
      "step: 5000, train loss: 2.131021386443559, val loss: 2.338388504781813\n",
      "step: 6000, train loss: 2.5602508862779385, val loss: 2.2380155731180147\n",
      "step: 7000, train loss: 2.3065974647277168, val loss: 2.2459830960273215\n",
      "step: 8000, train loss: 2.190829772113247, val loss: 2.218119743575988\n",
      "step: 9000, train loss: 1.930482663268064, val loss: 2.228581415053821\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "vocab_size = 27\n",
    "block_size = 3\n",
    "n_layer = 5\n",
    "dtype = torch.float64\n",
    "eval_interval = 1000\n",
    "\n",
    "# model\n",
    "C = torch.randn(vocab_size, n_embd, dtype=dtype, generator=g)\n",
    "layers = [Linear(n_embd * block_size, n_hidden, bias=False, dtype=dtype, generator=g), BatchNorm1d(n_hidden, dtype=dtype), Tanh()]\n",
    "for _ in range(n_layer-2):\n",
    "    layers.extend([Linear(n_hidden, n_hidden, bias=False, dtype=dtype, generator=g), BatchNorm1d(n_hidden, dtype=dtype), Tanh()])\n",
    "layers.extend([Linear(n_hidden, vocab_size, bias=False, dtype=dtype, generator=g), BatchNorm1d(vocab_size, dtype=dtype)])\n",
    "params = [C] + [p for l in layers for p in l.parameters()]\n",
    "print(f'number of params: {sum(p.numel() for p in params) / 1e6:.2f}M')\n",
    "layers[-1].weight.data *= 0.1 # less confident\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 10000\n",
    "ini_lr = 1.0\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # forward\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    outs = [emb]\n",
    "    for l in layers:\n",
    "        out = l(outs[-1])\n",
    "        outs.append(out)\n",
    "    logits = outs[-1]\n",
    "    # 2. loss\n",
    "    exp_l = logits.exp()\n",
    "    count = exp_l.sum(dim=-1, keepdim=True)\n",
    "    probs = exp_l / count\n",
    "    nlls = -probs.log()\n",
    "    loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "\n",
    "    # backward\n",
    "    # 1. zero grad\n",
    "    nlls_grad = torch.zeros(bs, vocab_size, dtype=dtype)\n",
    "    probs_grad = torch.zeros(bs, vocab_size, dtype=dtype)\n",
    "    count_grad = torch.zeros(bs, 1, dtype=dtype)\n",
    "    exp_l_grad = torch.zeros(bs, vocab_size, dtype=dtype)\n",
    "    logits_grad = torch.zeros(bs, vocab_size, dtype=dtype)\n",
    "    emb_grad = torch.zeros(bs, n_embd * block_size, dtype=dtype)\n",
    "    # param grad\n",
    "    C_grad = torch.zeros(vocab_size, n_embd, dtype=dtype)\n",
    "    # 2. backward\n",
    "    # loss\n",
    "    nlls_grad[torch.arange(y.shape[0]), y] = 1 / bs\n",
    "    probs_grad[torch.arange(y.shape[0]), y] = -1 / probs.data[torch.arange(y.shape[0]), y] * nlls_grad[torch.arange(y.shape[0]), y]\n",
    "    count_grad = -(exp_l.data * probs_grad).sum(dim=-1, keepdim=True) / count.data**2\n",
    "    exp_l_grad = probs_grad / count.data + count_grad  # one is from e/c to e, one is from c=\\sum e to e\n",
    "    logits_grad = exp_l.data * exp_l_grad\n",
    "    # layers\n",
    "    h_grad = logits_grad\n",
    "    for i in range(len(layers)-1, -1, -1):\n",
    "        h_grad = layers[i].backward(outs[i], outs[i+1], h_grad)\n",
    "    # embedding\n",
    "    emb_grad = h_grad\n",
    "    C_grad.index_add_(dim=0, index=x.view(-1), source=emb_grad.view(-1, n_embd))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        x, y = X_val[:128], Y_val[:128] # TODO: large bs, batchnorm is slow\n",
    "        emb = C[x].view(x.shape[0], -1)\n",
    "        h = emb\n",
    "        for l in layers:\n",
    "            h = l(h)\n",
    "        logits = h\n",
    "        val_loss = F.cross_entropy(logits, y)\n",
    "        print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    param_grads = [C_grad] + [p for l in layers for p in l.grads()]\n",
    "    for p, g in zip(params, param_grads):\n",
    "        p.data -= lr * g\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 0.13M\n",
      "step: 0, train loss: 3.3138560633196645, val loss: 3.2952505718703575\n",
      "step: 1000, train loss: 2.2650234162579306, val loss: 2.3929781293073997\n",
      "step: 2000, train loss: 2.3832809540536495, val loss: 2.2941183192098977\n",
      "step: 3000, train loss: 2.0633511993347464, val loss: 2.3660373324122586\n",
      "step: 4000, train loss: 2.3862213728646404, val loss: 2.2557674765658247\n",
      "step: 5000, train loss: 2.1310213864435594, val loss: 2.338388504781811\n",
      "step: 6000, train loss: 2.560250886277938, val loss: 2.2380155731180142\n",
      "step: 7000, train loss: 2.306597464727716, val loss: 2.2459830960273215\n",
      "step: 8000, train loss: 2.190829772113249, val loss: 2.218119743575987\n",
      "step: 9000, train loss: 1.9304826632680636, val loss: 2.228581415053821\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "vocab_size = 27\n",
    "block_size = 3\n",
    "n_layer = 5\n",
    "dtype = torch.float64\n",
    "eval_interval = 1000\n",
    "\n",
    "# model\n",
    "C = torch.randn(vocab_size, n_embd, dtype=dtype, generator=g)\n",
    "layers = [Linear(n_embd * block_size, n_hidden, bias=False, dtype=dtype, generator=g), BatchNorm1d(n_hidden, dtype=dtype), Tanh()]\n",
    "for _ in range(n_layer-2):\n",
    "    layers.extend([Linear(n_hidden, n_hidden, bias=False, dtype=dtype, generator=g), BatchNorm1d(n_hidden, dtype=dtype), Tanh()])\n",
    "layers.extend([Linear(n_hidden, vocab_size, bias=False, dtype=dtype, generator=g), BatchNorm1d(vocab_size, dtype=dtype)])\n",
    "params = [C] + [p for l in layers for p in l.parameters()]\n",
    "print(f'number of params: {sum(p.numel() for p in params) / 1e6:.2f}M')\n",
    "layers[-1].weight.data *= 0.1 # less confident\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 10000\n",
    "ini_lr = 1.0\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # forward\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    outs = [emb]\n",
    "    for l in layers:\n",
    "        out = l(outs[-1])\n",
    "        outs.append(out)\n",
    "    logits = outs[-1]\n",
    "    # 2. loss\n",
    "    exp_l = logits.exp()\n",
    "    count = exp_l.sum(dim=-1, keepdim=True)\n",
    "    probs = exp_l / count\n",
    "    nlls = -probs.log()\n",
    "    loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            x, y = X_val[:128], Y_val[:128] # TODO: large bs, batchnorm is slow\n",
    "            emb = C[x].view(x.shape[0], -1)\n",
    "            h = emb\n",
    "            for l in layers:\n",
    "                h = l(h)\n",
    "            logits = h\n",
    "            val_loss = F.cross_entropy(logits, y)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    for p in params:\n",
    "        p.data -= lr * p.grad\n",
    "        p.grad = None\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
