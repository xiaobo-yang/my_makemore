{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([169062, 3]),\n",
       " torch.Size([21133, 3]),\n",
       " torch.Size([21133, 3]),\n",
       " torch.Size([169062]),\n",
       " torch.Size([21133]),\n",
       " torch.Size([21133]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words = list(set(words))\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "len(words)\n",
    "\n",
    "chs = list(set(''.join(words + ['.'])))\n",
    "chs = sorted(chs, reverse=False)\n",
    "stoi = {ch: i for i, ch in enumerate(chs)}\n",
    "itos = {i: ch for i, ch in enumerate(chs)}\n",
    "\n",
    "# predict next token use previous 3 tokens\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    context = '...'\n",
    "    for ch in w + '.':\n",
    "        x = [stoi[c] for c in context]\n",
    "        y = stoi[ch]\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        context = context[1:] + ch\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "n1, n2  = int(0.8 * len(X)), int(0.9 * len(X))\n",
    "\n",
    "X_train, X_val, X_test = X.tensor_split([n1, n2])\n",
    "Y_train, Y_val, Y_test = Y.tensor_split([n1, n2])\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implement backward from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "vocab_size = 27\n",
    "block_size = 3\n",
    "\n",
    "def get_params():\n",
    "    torch.manual_seed(42)\n",
    "    C = torch.randn(vocab_size, n_embd)\n",
    "    w1 = torch.randn(n_embd * block_size, n_hidden) * (n_embd * block_size)**-0.5\n",
    "    w2 = torch.randn(n_hidden, vocab_size) * (5/3) * (n_hidden)**-0.5 * 0.1 # 0.1 is for less confident at initialization\n",
    "    b2 = torch.randn(vocab_size) * 0\n",
    "    bnw = torch.ones(n_hidden)\n",
    "    bnb = torch.zeros(n_hidden)\n",
    "    params = [C, w1, w2, b2, bnw, bnb]\n",
    "    for p in params:\n",
    "        p.requires_grad = True\n",
    "    return params\n",
    "\n",
    "params = get_params()\n",
    "C, w1, w2, b2, bnw, bnb = params\n",
    "bs = 32\n",
    "idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "x, y = X_train[idx], Y_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward and torch backward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "# forward\n",
    "emb = C[x].view(x.shape[0], -1)\n",
    "emb.retain_grad()\n",
    "hpreact = emb @ w1\n",
    "hpreact.retain_grad()\n",
    "bnmeani = mean_proj @ hpreact\n",
    "bnmeani.retain_grad()\n",
    "bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "bnstdi.retain_grad()\n",
    "hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "hpreact_bn.retain_grad()\n",
    "h = hpreact_bn.tanh()\n",
    "h.retain_grad()\n",
    "logits = h @ w2 + b2\n",
    "logits.retain_grad()\n",
    "# 2. loss\n",
    "exp_l = logits.exp()\n",
    "exp_l.retain_grad()\n",
    "count = exp_l.sum(dim=-1, keepdim=True)\n",
    "count.retain_grad()\n",
    "probs = exp_l / count\n",
    "probs.retain_grad()\n",
    "nlls = -probs.log()\n",
    "nlls.retain_grad()\n",
    "loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "\n",
    "# backward\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manual backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer grad\n",
    "nlls_grad = torch.zeros(bs, vocab_size)\n",
    "probs_grad = torch.zeros(bs, vocab_size)\n",
    "count_grad = torch.zeros(bs, 1)\n",
    "exp_l_grad = torch.zeros(bs, vocab_size)\n",
    "logits_grad = torch.zeros(bs, vocab_size)\n",
    "h_grad = torch.zeros(bs, n_hidden)\n",
    "hpreact_bn_grad = torch.zeros(bs, n_hidden)\n",
    "bnmeani_grad = torch.zeros(1, n_hidden)\n",
    "bnstdi_grad = torch.zeros(1, n_hidden)\n",
    "bnvari_grad = torch.zeros(1, n_hidden)\n",
    "hpreact_grad = torch.zeros(bs, n_hidden)\n",
    "emb_grad = torch.zeros(bs, n_embd * block_size)\n",
    "# param grad\n",
    "C_grad = torch.zeros(vocab_size, n_embd)\n",
    "w1_grad = torch.zeros(n_embd * block_size, n_hidden)\n",
    "w2_grad = torch.zeros(n_hidden, vocab_size)\n",
    "b2_grad = torch.zeros(vocab_size)\n",
    "bnw_grad = torch.zeros(n_hidden)\n",
    "bnb_grad = torch.zeros(n_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same grad for loss calculation: [True, True, True, True, True]\n",
      "same grad for logits calculation: [True, True, True, True, True, True]\n",
      "same grad for params: [True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "# 1. loss\n",
    "nlls_grad[torch.arange(y.shape[0]), y] = 1 / bs\n",
    "probs_grad[torch.arange(y.shape[0]), y] = -1 / probs.data[torch.arange(y.shape[0]), y] * nlls_grad[torch.arange(y.shape[0]), y]\n",
    "count_grad = -(exp_l.data * probs_grad).sum(dim=-1, keepdim=True) / count.data**2\n",
    "exp_l_grad = probs_grad / count.data + count_grad  # one is from e/c to e, one is from c=\\sum e to e\n",
    "logits_grad = exp_l.data * exp_l_grad\n",
    "\n",
    "# 2. logits\n",
    "h_grad = logits_grad @ w2.data.T\n",
    "hpreact_bn_grad = h_grad * (1 - h.data**2)\n",
    "# bn\n",
    "bnmeani_grad = ((-bnw.data / bnstdi.data) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "bnstdi_grad = (-((hpreact.data - bnmeani.data) * bnw.data / bnstdi.data**2) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "# hpreact\n",
    "hpreact_grad_mean = bnmeani_grad * torch.ones_like(hpreact.data) / bs\n",
    "hpreact_grad_std = bnstdi_grad * (1 / 2 / bnstdi.data) * (1 / bs) * (2 * var_proj @ hpreact.data)\n",
    "hpreact_grad_direct = hpreact_bn_grad * (bnw.data / bnstdi.data)\n",
    "hpreact_grad = hpreact_grad_mean + hpreact_grad_std + hpreact_grad_direct\n",
    "# emb\n",
    "emb_grad = hpreact_grad @ w1.data.T\n",
    "\n",
    "# 3. params\n",
    "C_grad.index_add_(dim=0, index=x.view(-1), source=emb_grad.view(-1, n_embd)) # add emb_grad[i] to C[x[i]]\n",
    "w1_grad = emb.data.T @ hpreact_grad\n",
    "w2_grad = h.data.T @ logits_grad\n",
    "b2_grad = logits_grad.sum(dim=0)\n",
    "bnw_grad = ((hpreact.data - bnmeani.data) / bnstdi.data * hpreact_bn_grad).sum(dim=0)\n",
    "bnb_grad = hpreact_bn_grad.sum(dim=0)\n",
    "\n",
    "# check\n",
    "is_equal1 = [torch.allclose(nlls_grad, nlls.grad), torch.allclose(probs_grad, probs.grad), torch.allclose(count_grad, count.grad), torch.allclose(exp_l_grad, exp_l.grad), torch.allclose(logits_grad, logits.grad)]\n",
    "is_equal2 = [torch.allclose(h_grad, h.grad), torch.allclose(hpreact_bn_grad, hpreact_bn.grad), torch.allclose(bnmeani_grad, bnmeani.grad), torch.allclose(bnstdi_grad, bnstdi.grad), torch.allclose(hpreact_grad, hpreact.grad), torch.allclose(emb_grad, emb.grad)]\n",
    "is_equal3 = [torch.allclose(C_grad, C.grad), torch.allclose(w1_grad, w1.grad), torch.allclose(w2_grad, w2.grad), torch.allclose(b2_grad, b2.grad), torch.allclose(bnw_grad, bnw.grad), torch.allclose(bnb_grad, bnb.grad)]\n",
    "print('same grad for loss calculation:', is_equal1)\n",
    "print('same grad for logits calculation:', is_equal2)\n",
    "print('same grad for params:', is_equal3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 3.3074488639831543, val loss: 3.3160440921783447\n",
      "step: 1000, train loss: 2.583611488342285, val loss: 2.4248502254486084\n",
      "step: 2000, train loss: 2.4143431186676025, val loss: 2.390408992767334\n",
      "step: 3000, train loss: 2.1222798824310303, val loss: 2.379809856414795\n",
      "step: 4000, train loss: 2.1513724327087402, val loss: 2.374274969100952\n",
      "step: 5000, train loss: 2.349586009979248, val loss: 2.386371374130249\n",
      "step: 6000, train loss: 2.2481563091278076, val loss: 2.2990658283233643\n",
      "step: 7000, train loss: 2.0973422527313232, val loss: 2.2958080768585205\n",
      "step: 8000, train loss: 2.5018582344055176, val loss: 2.2962090969085693\n",
      "step: 9000, train loss: 2.072721242904663, val loss: 2.288125991821289\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# model\n",
    "torch.manual_seed(42)\n",
    "C = torch.randn(vocab_size, n_embd)\n",
    "w1 = torch.randn(n_embd * block_size, n_hidden) * (n_embd * block_size)**-0.5\n",
    "w2 = torch.randn(n_hidden, vocab_size) * (5/3) * (n_hidden)**-0.5 * 0.1 # 0.1 is for less confident at initialization\n",
    "b2 = torch.randn(vocab_size) * 0\n",
    "bnw = torch.ones(n_hidden)\n",
    "bnb = torch.zeros(n_hidden)\n",
    "params = [C, w1, w2, b2, bnw, bnb]\n",
    "bnmean_running = torch.zeros(n_hidden)\n",
    "bnstd_running = torch.ones(n_hidden)\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 10000\n",
    "ini_lr = 1.0\n",
    "\n",
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # ---------------- forward --------------------\n",
    "    # 1. logits\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    hpreact = emb @ w1\n",
    "    bnmeani = mean_proj @ hpreact\n",
    "    bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "    hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "    h = hpreact_bn.tanh()\n",
    "    logits = h @ w2 + b2\n",
    "    # 2. loss\n",
    "    exp_l = logits.exp()\n",
    "    count = exp_l.sum(dim=-1, keepdim=True)\n",
    "    probs = exp_l / count\n",
    "    nlls = -probs.log()\n",
    "    loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "    \n",
    "\n",
    "\n",
    "    # ---------------- backward --------------------\n",
    "    # 0. zero grad\n",
    "    # buffer grad\n",
    "    nlls_grad = torch.zeros(bs, vocab_size)\n",
    "    probs_grad = torch.zeros(bs, vocab_size)\n",
    "    count_grad = torch.zeros(bs, 1)\n",
    "    exp_l_grad = torch.zeros(bs, vocab_size)\n",
    "    logits_grad = torch.zeros(bs, vocab_size)\n",
    "    h_grad = torch.zeros(bs, n_hidden)\n",
    "    hpreact_bn_grad = torch.zeros(bs, n_hidden)\n",
    "    bnmeani_grad = torch.zeros(1, n_hidden)\n",
    "    bnstdi_grad = torch.zeros(1, n_hidden)\n",
    "    bnvari_grad = torch.zeros(1, n_hidden)\n",
    "    hpreact_grad = torch.zeros(bs, n_hidden)\n",
    "    emb_grad = torch.zeros(bs, n_embd * block_size)\n",
    "    # param grad\n",
    "    C_grad = torch.zeros(vocab_size, n_embd)\n",
    "    w1_grad = torch.zeros(n_embd * block_size, n_hidden)\n",
    "    w2_grad = torch.zeros(n_hidden, vocab_size)\n",
    "    b2_grad = torch.zeros(vocab_size)\n",
    "    bnw_grad = torch.zeros(n_hidden)\n",
    "    bnb_grad = torch.zeros(n_hidden)\n",
    "\n",
    "    # 1. loss\n",
    "    nlls_grad[torch.arange(y.shape[0]), y] = 1 / bs\n",
    "    probs_grad[torch.arange(y.shape[0]), y] = -1 / probs.data[torch.arange(y.shape[0]), y] * nlls_grad[torch.arange(y.shape[0]), y]\n",
    "    count_grad = -(exp_l.data * probs_grad).sum(dim=-1, keepdim=True) / count.data**2\n",
    "    exp_l_grad = probs_grad / count.data + count_grad  # one is from e/c to e, one is from c=\\sum e to e\n",
    "    logits_grad = exp_l.data * exp_l_grad\n",
    "\n",
    "    # 2. logits\n",
    "    h_grad = logits_grad @ w2.data.T\n",
    "    hpreact_bn_grad = h_grad * (1 - h.data**2)\n",
    "    # bn\n",
    "    bnmeani_grad = ((-bnw.data / bnstdi.data) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "    bnstdi_grad = (-((hpreact.data - bnmeani.data) * bnw.data / bnstdi.data**2) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "    # hpreact\n",
    "    hpreact_grad_mean = bnmeani_grad * torch.ones_like(hpreact.data) / bs\n",
    "    hpreact_grad_std = bnstdi_grad * (1 / 2 / bnstdi.data) * (1 / bs) * (2 * var_proj @ hpreact.data)\n",
    "    hpreact_grad_direct = hpreact_bn_grad * (bnw.data / bnstdi.data)\n",
    "    hpreact_grad = hpreact_grad_mean + hpreact_grad_std + hpreact_grad_direct\n",
    "    # emb\n",
    "    emb_grad = hpreact_grad @ w1.data.T\n",
    "    \n",
    "    # 3. params\n",
    "    C_grad.index_add_(dim=0, index=x.view(-1), source=emb_grad.view(-1, n_embd)) # add emb_grad[i] to C[x[i]]\n",
    "    w1_grad = emb.data.T @ hpreact_grad\n",
    "    w2_grad = h.data.T @ logits_grad\n",
    "    b2_grad = logits_grad.sum(dim=0)\n",
    "    bnw_grad = ((hpreact.data - bnmeani.data) / bnstdi.data * hpreact_bn_grad).sum(dim=0)\n",
    "    bnb_grad = hpreact_bn_grad.sum(dim=0)\n",
    "    param_grads = [C_grad, w1_grad, w2_grad, b2_grad, bnw_grad, bnb_grad]\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            emb = C[X_val].view(X_val.shape[0], -1)\n",
    "            hpreact = emb @ w1\n",
    "            hpreact = (hpreact - hpreact.mean(dim=0, keepdim=True)) / hpreact.std(dim=0, keepdim=True) * bnw + bnb\n",
    "            h = hpreact.tanh()\n",
    "            logits = h @ w2 + b2\n",
    "            val_loss = F.cross_entropy(logits, Y_val)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    for p, g in zip(params, param_grads):\n",
    "        p.data -= lr * g\n",
    "    \n",
    "    bnmean_running = bnmean_running * 0.99 + bnmeani * 0.01\n",
    "    bnstd_running = bnstd_running * 0.99 + bnstdi * 0.01\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1794, -0.1739,  0.0228,  ...,  0.0201, -0.1145,  0.1450],\n",
       "        [-0.3108,  0.1481, -0.2057,  ..., -0.3503,  0.2971,  0.1593],\n",
       "        [-0.0755,  0.0891, -0.0533,  ..., -0.1936, -0.2113, -0.3970],\n",
       "        ...,\n",
       "        [ 0.0905, -0.2106, -0.5778,  ..., -0.1088, -0.1970, -0.3650],\n",
       "        [-0.1003,  0.1913, -0.4233,  ...,  0.3125,  0.0998, -0.2520],\n",
       "        [ 0.2542,  0.2179, -0.3316,  ..., -0.4188, -0.2184, -0.4282]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17936445772647858"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[0,0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 3.3074488639831543, val loss: 3.3160440921783447\n",
      "step: 1000, train loss: 2.583611488342285, val loss: 2.4248502254486084\n",
      "step: 2000, train loss: 2.4143431186676025, val loss: 2.390408992767334\n",
      "step: 3000, train loss: 2.1222798824310303, val loss: 2.379809856414795\n",
      "step: 4000, train loss: 2.1513726711273193, val loss: 2.374274969100952\n",
      "step: 5000, train loss: 2.349586009979248, val loss: 2.386371374130249\n",
      "step: 6000, train loss: 2.2481563091278076, val loss: 2.2990663051605225\n",
      "step: 7000, train loss: 2.0973422527313232, val loss: 2.2958080768585205\n",
      "step: 8000, train loss: 2.5018584728240967, val loss: 2.2962090969085693\n",
      "step: 9000, train loss: 2.072721242904663, val loss: 2.288125991821289\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# model\n",
    "params = get_params()\n",
    "C, w1, w2, b2, bnw, bnb = params\n",
    "bnmean_running = torch.zeros(n_hidden)\n",
    "bnstd_running = torch.ones(n_hidden)\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 10000\n",
    "ini_lr = 1.0\n",
    "\n",
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # forward\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    hpreact = emb @ w1\n",
    "    bnmeani = mean_proj @ hpreact\n",
    "    bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "    hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "    h = hpreact_bn.tanh()\n",
    "    logits = h @ w2 + b2\n",
    "    # 2. loss\n",
    "    exp_l = logits.exp()\n",
    "    count = exp_l.sum(dim=-1, keepdim=True)\n",
    "    probs = exp_l / count\n",
    "    nlls = -probs.log()\n",
    "    loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    if step % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            emb = C[X_val].view(X_val.shape[0], -1)\n",
    "            hpreact = emb @ w1\n",
    "            hpreact = (hpreact - hpreact.mean(dim=0, keepdim=True)) / hpreact.std(dim=0, keepdim=True) * bnw + bnb\n",
    "            h = hpreact.tanh()\n",
    "            logits = h @ w2 + b2\n",
    "            val_loss = F.cross_entropy(logits, Y_val)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    for p in params:\n",
    "        p.data -= lr * p.grad\n",
    "        p.grad = None\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = bnmean_running * 0.99 + bnmeani * 0.01\n",
    "        bnstd_running = bnstd_running * 0.99 + bnstdi * 0.01\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1794, -0.1739,  0.0228,  ...,  0.0201, -0.1145,  0.1450],\n",
       "        [-0.3108,  0.1481, -0.2057,  ..., -0.3503,  0.2971,  0.1593],\n",
       "        [-0.0755,  0.0891, -0.0533,  ..., -0.1936, -0.2113, -0.3970],\n",
       "        ...,\n",
       "        [ 0.0905, -0.2106, -0.5778,  ..., -0.1088, -0.1970, -0.3650],\n",
       "        [-0.1003,  0.1913, -0.4233,  ...,  0.3125,  0.0998, -0.2520],\n",
       "        [ 0.2542,  0.2179, -0.3316,  ..., -0.4188, -0.2184, -0.4282]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17936447262763977"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[0,0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simpler grad\n",
    "## BatchNorm \n",
    "\n",
    "Let $x\\in\\mathbb{R}^{n\\times d}$, $w\\in\\mathbb{R}^d$, $b\\in\\mathbb{R}^d$, define $\\bar{x} = x\\text{.mean}(\\text{dim=0})$ then\n",
    "\n",
    "$$\n",
    "    o = \\frac{x - \\bar{x}}{\\sqrt{(x - \\bar{x})^2\\text{.mean(dim=0)} + \\epsilon}} w + b \\in \\mathbb{R}^{n \\times d}\n",
    "$$\n",
    "\n",
    "Note: as torch, we don't use Bessel correction\n",
    "\n",
    "Let $s = \\sqrt{(x - \\bar{x})^2\\text{.mean(dim=0)} + \\epsilon}$ and $x_{\\text{norm}} = \\frac{x - \\bar{x}}{s}$.\n",
    "\n",
    "Denote $dx$ as grad from the end layer to current layer, $dy/dx$ as grad from next layer to current layer.\n",
    "\n",
    "$$\n",
    "    dw =  do \\cdot \\frac{do}{dw} = \\left(x_{\\text{norm}} * do\\right).~\\text{sum}(\\text{dim=0}) \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "$$\n",
    "    db =  do \\cdot \\frac{do}{db} = \\left(do\\right).~\\text{sum}(\\text{dim=0}) \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "Grad $dx$ is more complex, but if we directly use computation graph to calculate grad in scalar level, and then simplify the computation with tensor operations and algebraic transformation. It's easy to see\n",
    "\n",
    "$$\n",
    "    dx = \\left(\n",
    "            s * do - s * do\\text{.mean(dim=0)} - \\frac{1}{s} * (x - \\bar{x}) * ((x - \\bar{x}) * do).~\\text{mean}(\\text{dim=0})\n",
    "    \\right) * w * \\frac{1}{s^2}\n",
    "$$\n",
    "\n",
    "Combine same terms and use quantities already calculated in forward pass, we get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    dx &= \\left(\n",
    "            (do - do\\text{.mean(dim=0)}) - \\frac{x - \\bar{x}}{s} * \\left(\\frac{x - \\bar{x}}{s} * do\\right).~\\text{mean}(\\text{dim=0})\n",
    "    \\right) * w * \\frac{1}{s} \\\\\n",
    "    &= \\left(\n",
    "            (do - \\frac{db}{n}) - x_\\text{norm} * \\left(\\frac{dw}{n}\\right)\n",
    "    \\right) * w * \\frac{1}{s}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## LayerNorm\n",
    "\n",
    "Almost the same as BatchNorm, but we need to consider the last dim.\n",
    "\n",
    "$$\n",
    "    dw =  do \\cdot \\frac{do}{dw} = \\left(x_{\\text{norm}} * do\\right).~\\text{sum}(\\text{dim=[0,1,ndim-1]}) \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "$$\n",
    "    db =  do \\cdot \\frac{do}{db} = \\left(do\\right).~\\text{sum}(\\text{dim=[0,1,ndim-1]}) \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "$$\n",
    "    dx = \\left(\n",
    "            (do - do\\text{.mean(dim=-1)}) - \\frac{x - \\bar{x}}{s} * \\left(\\frac{x - \\bar{x}}{s} * do\\right).~\\text{mean}(\\text{dim=-1})\n",
    "    \\right) * w * \\frac{1}{s}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiny_torch import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x grad relative error: 6.827384472361946e-16\n"
     ]
    }
   ],
   "source": [
    "# --- manual ---\n",
    "loss_fn = CrossEntropyLoss()\n",
    "x = torch.randn(100, 10, dtype=torch.float64)\n",
    "y = torch.randint(0, 10, (100,))\n",
    "loss = loss_fn(x, y)\n",
    "x_grad = loss_fn.backward(grad=1.0) # last layer, dloss=1.0\n",
    "\n",
    "\n",
    "# --- torch ---\n",
    "import torch.nn as nn\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "xt, yt = x.clone(), y.clone()\n",
    "xt.requires_grad = True\n",
    "loss = loss_fn(xt, yt)\n",
    "loss.backward()\n",
    "print(f'x grad relative error: {((xt.grad - x_grad) / xt.grad).abs().max().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass:\n",
      "o relative error: 5.649903956929978e-15\n",
      "backward pass:\n",
      "db relative error: 4.587874512547105e-16\n",
      "dw relative error: 1.4906969562791864e-15\n",
      "dx relative error: 1.8066190659672137e-14\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dtype = torch.float64\n",
    "eps = 1e-5\n",
    "# model\n",
    "bn = BatchNorm1d(10, dtype=dtype, eps=eps)\n",
    "# params\n",
    "x = torch.randn(100, 10, dtype=dtype)\n",
    "# ------- manual -------\n",
    "# forward\n",
    "o = bn(x)\n",
    "# backward\n",
    "do = torch.randn_like(o, dtype=dtype)\n",
    "dx = bn.backward(do)\n",
    "\n",
    "# ------- torch -------\n",
    "import torch.nn as nn\n",
    "bnt = nn.BatchNorm1d(10, dtype=dtype, eps=eps)\n",
    "bnt.weight.data = bn.weight.data\n",
    "bnt.bias.data = bn.bias.data\n",
    "xt = x.clone()\n",
    "xt.requires_grad = True\n",
    "# forward\n",
    "ot = bnt(xt)\n",
    "# backward\n",
    "(ot * do).sum().backward()\n",
    "\n",
    "# -------- compare -------- \n",
    "print('forward pass:')\n",
    "print(f'o relative error: {((ot - o) / ot).abs().max().item()}')\n",
    "print('backward pass:')\n",
    "print(f'db relative error: {((bnt.bias.grad - bn.bias_grad) / bnt.bias.grad).abs().max().item()}')\n",
    "print(f'dw relative error: {((bnt.weight.grad - bn.weight_grad) / bnt.weight.grad).abs().max().item()}')\n",
    "print(f'dx relative error: {((dx - xt.grad) / xt.grad).abs().max().item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass:\n",
      "o relative error: 1.1410527296324882e-11\n",
      "backward pass:\n",
      "db relative error: 8.099100649135957e-15\n",
      "dw relative error: 6.349328555149738e-15\n",
      "dx relative error: 7.678927882277915e-12\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dtype = torch.float64\n",
    "eps = 1e-5\n",
    "# model\n",
    "ln = LayerNorm(10, dtype=dtype, eps=eps)\n",
    "# params\n",
    "x = torch.randn(3, 32, 100, 10, dtype=dtype)\n",
    "# ------- manual -------\n",
    "# forward\n",
    "o = ln(x)\n",
    "# backward\n",
    "do = torch.randn_like(o, dtype=dtype)\n",
    "dx = ln.backward(do)\n",
    "\n",
    "# ------- torch -------\n",
    "import torch.nn as nn\n",
    "lnt = nn.LayerNorm(10, dtype=dtype, eps=eps)\n",
    "lnt.weight.data = ln.weight.data\n",
    "lnt.bias.data = ln.bias.data\n",
    "xt = x.clone()\n",
    "xt.requires_grad = True\n",
    "# forward\n",
    "ot = lnt(xt)\n",
    "# backward\n",
    "ot.backward(do)\n",
    "\n",
    "# -------- compare -------- \n",
    "print('forward pass:')\n",
    "print(f'o relative error: {((ot - o) / ot).abs().max().item()}')\n",
    "print('backward pass:')\n",
    "print(f'db relative error: {((lnt.bias.grad - ln.bias_grad) / lnt.bias.grad).abs().max().item()}')\n",
    "print(f'dw relative error: {((lnt.weight.grad - ln.weight_grad) / lnt.weight.grad).abs().max().item()}')\n",
    "print(f'dx relative error: {((dx - xt.grad) / xt.grad).abs().max().item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 0.72M\n",
      "check grad:\n",
      "[Layer 1] weight grad relative error: 6.348547564942509e-12\n",
      "x_grad relative error: 1.2315982383522732e-11\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "n_embd = 30\n",
    "n_hidden = 100\n",
    "bs = 32\n",
    "dtype = torch.float64\n",
    "# model\n",
    "layers = [Linear(n_embd, n_hidden, bias=False, dtype=dtype), BatchNorm1d(n_hidden, dtype=dtype), Tanh()]\n",
    "for _ in range(70):\n",
    "    layers.extend([Linear(n_hidden, n_hidden, bias=False, dtype=dtype), BatchNorm1d(n_hidden, dtype=dtype), Tanh()])\n",
    "params = [p for l in layers for p in l.parameters()]\n",
    "print(f'number of params: {sum(p.numel() for p in params) / 1e6:.2f}M')\n",
    "# input\n",
    "x = torch.randn(bs, n_embd, dtype=dtype, requires_grad=True)\n",
    "\n",
    "# --- manual ---\n",
    "# forward\n",
    "h = x\n",
    "for l in layers:\n",
    "    h = l(h)\n",
    "\n",
    "# backward\n",
    "grad = torch.ones(bs, n_hidden)\n",
    "for i in range(len(layers)-1, -1, -1):\n",
    "    grad = layers[i].backward(grad)\n",
    "\n",
    "\n",
    "# --- torch ---\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "h = x\n",
    "for l in layers:\n",
    "    h = l(h)\n",
    "h.sum().backward()\n",
    "\n",
    "# --- compare ---\n",
    "print('check grad:')\n",
    "print(f'[Layer 1] weight grad relative error: {((params[0].grad - layers[0].weight_grad) / params[0].grad).abs().max().item()}')\n",
    "print(f'x_grad relative error: {((x.grad - grad) / x.grad).abs().max().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 0.13M\n",
      "step: 0, train loss: 3.2867542075582366, val loss: 3.2950197703563147\n",
      "step: 1000, train loss: 2.1482904137515266, val loss: 2.640912602529584\n",
      "step: 2000, train loss: 2.5459965059326417, val loss: 2.4858058744686815\n",
      "step: 3000, train loss: 2.266767981848271, val loss: 2.3477051377936706\n",
      "step: 4000, train loss: 2.3878019681315785, val loss: 2.3204157819249396\n",
      "step: 5000, train loss: 2.3080888344416914, val loss: 2.3027045817113034\n",
      "step: 6000, train loss: 2.2412630574133634, val loss: 2.254448478673697\n",
      "step: 7000, train loss: 2.3546462824976766, val loss: 2.2395046216802745\n",
      "step: 8000, train loss: 1.8836993623611573, val loss: 2.2420983878352296\n",
      "step: 9000, train loss: 1.8356284958590918, val loss: 2.2322821326356497\n",
      "step: 10000, train loss: 2.249002134202489, val loss: 2.215477528956517\n",
      "step: 11000, train loss: 2.229033208184121, val loss: 2.171194241644135\n",
      "step: 12000, train loss: 2.166332077017777, val loss: 2.167437729786591\n",
      "step: 13000, train loss: 2.148181543744039, val loss: 2.1631709642844488\n",
      "step: 14000, train loss: 1.7330262480207443, val loss: 2.1611359951137414\n",
      "step: 15000, train loss: 2.266669078833099, val loss: 2.1575918902069207\n",
      "step: 16000, train loss: 2.2333063242411755, val loss: 2.155328552087092\n",
      "step: 17000, train loss: 2.201398160365801, val loss: 2.1535833366364123\n",
      "step: 18000, train loss: 2.2856527588616373, val loss: 2.153680490940715\n",
      "step: 19000, train loss: 1.8024840509590283, val loss: 2.150595253514059\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(42)\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "vocab_size = 27\n",
    "block_size = 3\n",
    "n_layer = 5\n",
    "dtype = torch.float64\n",
    "eval_interval = 1000\n",
    "\n",
    "# model\n",
    "C = torch.randn(vocab_size, n_embd, dtype=dtype, generator=g)\n",
    "layers = [Linear(n_embd * block_size, n_hidden, bias=False, dtype=dtype, generator=g), BatchNorm1d(n_hidden, dtype=dtype), Tanh()]\n",
    "for _ in range(n_layer-2):\n",
    "    layers.extend([Linear(n_hidden, n_hidden, bias=False, dtype=dtype, generator=g), BatchNorm1d(n_hidden, dtype=dtype), Tanh()])\n",
    "layers.extend([Linear(n_hidden, vocab_size, bias=False, dtype=dtype, generator=g), BatchNorm1d(vocab_size, dtype=dtype)])\n",
    "params = [C] + [p for l in layers for p in l.parameters()]\n",
    "print(f'number of params: {sum(p.numel() for p in params) / 1e6:.2f}M')\n",
    "layers[-1].weight.data *= 0.1 # less confident\n",
    "# loss\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 20000\n",
    "ini_lr = 1.0\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # forward\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    h = emb\n",
    "    for l in layers:\n",
    "        h = l(h)\n",
    "    logits = h\n",
    "    # 2. loss\n",
    "    loss = loss_fn(logits, y)\n",
    "\n",
    "    # backward\n",
    "    # 1. zero grad\n",
    "    emb_grad = torch.zeros(bs, n_embd * block_size, dtype=dtype)\n",
    "    C_grad = torch.zeros(vocab_size, n_embd, dtype=dtype)\n",
    "    # 2. backward\n",
    "    # loss\n",
    "    h_grad = loss_fn.backward(grad=1.0) # last layer, dloss=1.0\n",
    "    # layers\n",
    "    for i in range(len(layers)-1, -1, -1):\n",
    "        h_grad = layers[i].backward(h_grad)\n",
    "    # embedding\n",
    "    emb_grad = h_grad\n",
    "    C_grad.index_add_(dim=0, index=x.view(-1), source=emb_grad.view(-1, n_embd))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        for l in layers:\n",
    "            if isinstance(l, BatchNorm1d):\n",
    "                l._training = False\n",
    "        x, y = X_val, Y_val\n",
    "        emb = C[x].view(x.shape[0], -1)\n",
    "        h = emb\n",
    "        for l in layers:\n",
    "            h = l(h)\n",
    "        val_loss = loss_fn(h, y)\n",
    "        print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "        for l in layers:\n",
    "            if isinstance(l, BatchNorm1d):\n",
    "                l._training = True\n",
    "    \n",
    "    # update\n",
    "    param_grads = [C_grad] + [p for l in layers for p in l.grads()]\n",
    "    for p, g in zip(params, param_grads):\n",
    "        p.data -= lr * g\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 2.1454702675946895\n",
      ".anuelynn.\n",
      ".jamarbi.\n",
      ".nedyn.\n",
      ".shan.\n",
      ".silayley.\n",
      ".kemah.\n",
      ".lukan.\n",
      ".emiah.\n",
      ".nesilanzi.\n",
      ".kence.\n"
     ]
    }
   ],
   "source": [
    "for l in layers:\n",
    "    if isinstance(l, BatchNorm1d):\n",
    "        l._training = False\n",
    "x, y = X_test, Y_test\n",
    "emb = C[x].view(x.shape[0], -1)\n",
    "h = emb\n",
    "for l in layers:\n",
    "    h = l(h)\n",
    "test_loss = loss_fn(h, y).item()\n",
    "print(f'test loss: {test_loss}')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for _ in range(10):\n",
    "    cond = torch.tensor([[0, 0, 0]])\n",
    "    s = '.'\n",
    "    for i in range(10):\n",
    "        emb = C[cond].view(cond.shape[0], -1)\n",
    "        h = emb\n",
    "        for l in layers:\n",
    "            h = l(h)\n",
    "        logits = h\n",
    "        probs = logits.softmax(dim=-1)\n",
    "        next_ch = torch.multinomial(probs, num_samples=1)\n",
    "        s = s + itos[next_ch.item()]\n",
    "        cond = torch.cat([cond[:, 1:], next_ch], dim=-1)\n",
    "        if next_ch.item() == 0:\n",
    "            break\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1140,  1.8199,  0.0528, -0.6798, -1.4894], dtype=torch.float64),\n",
       " tensor([3.4967, 5.4446, 3.0848, 3.3779, 2.4840], dtype=torch.float64))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm, rv = layers[1].running_mean.clone(), layers[1].running_var.clone()\n",
    "rm[:5], rv[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.11395175664888935, 3.4967322385630992)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm[0].item(), rv[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare with torch\n",
    "train loss are almost the same, but in eval() model, val loss is slightly different, maybe because running var has different way to calculate(running mean is the same). \n",
    "\n",
    "If we do not use running mean and var(only for batch size > 1), the val loss is the same.\n",
    "\n",
    "The generation is also the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 0.13M\n",
      "step: 0, train loss: 3.286754207558237, val loss: 3.2950197726665835\n",
      "step: 1000, train loss: 2.148290413751526, val loss: 2.6227224585640125\n",
      "step: 2000, train loss: 2.545996505932642, val loss: 2.4732782507077267\n",
      "step: 3000, train loss: 2.266767981848271, val loss: 2.342057692096394\n",
      "step: 4000, train loss: 2.38780196813158, val loss: 2.3168131176634685\n",
      "step: 5000, train loss: 2.3080888344416923, val loss: 2.300169224251842\n",
      "step: 6000, train loss: 2.241263057413365, val loss: 2.2524295698011203\n",
      "step: 7000, train loss: 2.354646282497675, val loss: 2.238695372435957\n",
      "step: 8000, train loss: 1.8836993623611575, val loss: 2.2407968274018217\n",
      "step: 9000, train loss: 1.835628495859093, val loss: 2.2307235974638684\n",
      "step: 10000, train loss: 2.2490021342024873, val loss: 2.2138429877507324\n",
      "step: 11000, train loss: 2.2290332081841213, val loss: 2.1719617922691703\n",
      "step: 12000, train loss: 2.1663320770177763, val loss: 2.168218259457181\n",
      "step: 13000, train loss: 2.148181543744035, val loss: 2.164331181485238\n",
      "step: 14000, train loss: 1.7330262480207452, val loss: 2.162198000943528\n",
      "step: 15000, train loss: 2.266669078833097, val loss: 2.158250279994064\n",
      "step: 16000, train loss: 2.233306324241177, val loss: 2.1560679073774485\n",
      "step: 17000, train loss: 2.201398160365799, val loss: 2.1541978874279932\n",
      "step: 18000, train loss: 2.285652758861644, val loss: 2.1540324885771853\n",
      "step: 19000, train loss: 1.8024840509590265, val loss: 2.1512972168975564\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "vocab_size = 27\n",
    "block_size = 3\n",
    "n_layer = 5\n",
    "dtype = torch.float64\n",
    "eval_interval = 1000\n",
    "\n",
    "# original model\n",
    "C = torch.randn(vocab_size, n_embd, dtype=dtype, generator=g)\n",
    "layers = [Linear(n_embd * block_size, n_hidden, bias=False, dtype=dtype, generator=g), BatchNorm1d(n_hidden, dtype=dtype), Tanh()]\n",
    "for _ in range(n_layer-2):\n",
    "    layers.extend([Linear(n_hidden, n_hidden, bias=False, dtype=dtype, generator=g), BatchNorm1d(n_hidden, dtype=dtype), Tanh()])\n",
    "layers.extend([Linear(n_hidden, vocab_size, bias=False, dtype=dtype, generator=g), BatchNorm1d(vocab_size, dtype=dtype)])\n",
    "# move same weight to torch model\n",
    "layers_t = [nn.Linear(n_embd * block_size, n_hidden, bias=False, dtype=dtype), nn.BatchNorm1d(n_hidden, dtype=dtype, momentum=0.001), nn.Tanh()]\n",
    "for _ in range(n_layer-2):\n",
    "    layers_t.extend([nn.Linear(n_hidden, n_hidden, bias=False, dtype=dtype), nn.BatchNorm1d(n_hidden, dtype=dtype, momentum=0.001), nn.Tanh()])\n",
    "layers_t.extend([nn.Linear(n_hidden, vocab_size, bias=False, dtype=dtype), nn.BatchNorm1d(vocab_size, dtype=dtype, momentum=0.001)])\n",
    "for l, lt in zip(layers, layers_t):\n",
    "    if isinstance(l, (Linear, BatchNorm1d)):\n",
    "        lt.weight.data = l.weight.data.T if isinstance(l, Linear) else l.weight.data\n",
    "        if l.bias is not None:\n",
    "            lt.bias.data = l.bias.data\n",
    "layers = layers_t\n",
    "params = [C] + [p for l in layers for p in l.parameters()]\n",
    "print(f'number of params: {sum(p.numel() for p in params) / 1e6:.2f}M')\n",
    "layers[-1].weight.data *= 0.1 # less confident\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 20000\n",
    "ini_lr = 1.0\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # forward\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    h = emb\n",
    "    for l in layers:\n",
    "        h = l(h)\n",
    "    logits = h\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        for l in layers: # eval mode for batchnorm\n",
    "            if isinstance(l, nn.BatchNorm1d):\n",
    "                l.eval()\n",
    "        with torch.no_grad():\n",
    "            x, y = X_val, Y_val\n",
    "            emb = C[x].view(x.shape[0], -1)\n",
    "            h = emb\n",
    "            for l in layers:\n",
    "                h = l(h)\n",
    "            logits = h\n",
    "            val_loss = F.cross_entropy(logits, y)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "        for l in layers:\n",
    "            l.train()\n",
    "    \n",
    "    # update\n",
    "    for p in params:\n",
    "        p.data -= lr * p.grad\n",
    "        p.grad = None\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 2.146003713109895\n",
      ".anuelynn.\n",
      ".jamarbi.\n",
      ".nedyn.\n",
      ".shan.\n",
      ".silayley.\n",
      ".kemah.\n",
      ".lukan.\n",
      ".emiah.\n",
      ".nesilanzi.\n",
      ".kence.\n"
     ]
    }
   ],
   "source": [
    "for l in layers:\n",
    "    if isinstance(l, nn.BatchNorm1d):\n",
    "        l.eval()\n",
    "with torch.no_grad():\n",
    "    x, y = X_test, Y_test\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    h = emb\n",
    "    for l in layers:\n",
    "        h = l(h)\n",
    "    logits = h\n",
    "    test_loss = F.cross_entropy(logits, y).item()\n",
    "    print(f'test loss: {test_loss}')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        cond = torch.tensor([[0, 0, 0]])\n",
    "        s = '.'\n",
    "        for i in range(10):\n",
    "            emb = C[cond].view(cond.shape[0], -1)\n",
    "            h = emb\n",
    "            for l in layers:\n",
    "                h = l(h)\n",
    "            logits = h\n",
    "            probs = logits.softmax(dim=-1)\n",
    "            next_ch = torch.multinomial(probs, num_samples=1)\n",
    "            s = s + itos[next_ch.item()]\n",
    "            cond = torch.cat([cond[:, 1:], next_ch], dim=-1)\n",
    "            if next_ch.item() == 0:\n",
    "                break\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.1140,  1.8199,  0.0528, -0.6798, -1.4894], dtype=torch.float64),\n",
       " tensor([3.6095, 5.6202, 3.1843, 3.4869, 2.5642], dtype=torch.float64))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt, rvt = layers[1].running_mean.clone(), layers[1].running_var.clone()\n",
    "rmt[:5], rvt[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.11395175664893932, 3.609530052644503)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmt[0].item(), rvt[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
