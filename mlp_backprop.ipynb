{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([169062, 3]),\n",
       " torch.Size([21133, 3]),\n",
       " torch.Size([21133, 3]),\n",
       " torch.Size([169062]),\n",
       " torch.Size([21133]),\n",
       " torch.Size([21133]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words = list(set(words))\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "len(words)\n",
    "\n",
    "chs = list(set(''.join(words + ['.'])))\n",
    "chs = sorted(chs, reverse=False)\n",
    "stoi = {ch: i for i, ch in enumerate(chs)}\n",
    "itos = {i: ch for i, ch in enumerate(chs)}\n",
    "\n",
    "# predict next token use previous 3 tokens\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    context = '...'\n",
    "    for ch in w + '.':\n",
    "        x = [stoi[c] for c in context]\n",
    "        y = stoi[ch]\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "        context = context[1:] + ch\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "n1, n2  = int(0.8 * len(X)), int(0.9 * len(X))\n",
    "\n",
    "X_train, X_val, X_test = X.tensor_split([n1, n2])\n",
    "Y_train, Y_val, Y_test = Y.tensor_split([n1, n2])\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implement backward from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "vocab_size = 27\n",
    "block_size = 3\n",
    "\n",
    "def get_params():\n",
    "    torch.manual_seed(42)\n",
    "    C = torch.randn(vocab_size, n_embd)\n",
    "    w1 = torch.randn(n_embd * block_size, n_hidden) * (n_embd * block_size)**-0.5\n",
    "    w2 = torch.randn(n_hidden, vocab_size) * (5/3) * (n_hidden)**-0.5 * 0.1 # 0.1 is for less confident at initialization\n",
    "    b2 = torch.randn(vocab_size) * 0\n",
    "    bnw = torch.ones(n_hidden)\n",
    "    bnb = torch.zeros(n_hidden)\n",
    "    params = [C, w1, w2, b2, bnw, bnb]\n",
    "    for p in params:\n",
    "        p.requires_grad = True\n",
    "    return params\n",
    "\n",
    "params = get_params()\n",
    "C, w1, w2, b2, bnw, bnb = params\n",
    "bs = 32\n",
    "idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "x, y = X_train[idx], Y_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward and torch backward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "# forward\n",
    "emb = C[x].view(x.shape[0], -1)\n",
    "emb.retain_grad()\n",
    "hpreact = emb @ w1\n",
    "hpreact.retain_grad()\n",
    "bnmeani = mean_proj @ hpreact\n",
    "bnmeani.retain_grad()\n",
    "bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "bnstdi.retain_grad()\n",
    "hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "hpreact_bn.retain_grad()\n",
    "h = hpreact_bn.tanh()\n",
    "h.retain_grad()\n",
    "logits = h @ w2 + b2\n",
    "logits.retain_grad()\n",
    "# 2. loss\n",
    "exp_l = logits.exp()\n",
    "exp_l.retain_grad()\n",
    "count = exp_l.sum(dim=-1, keepdim=True)\n",
    "count.retain_grad()\n",
    "probs = exp_l / count\n",
    "probs.retain_grad()\n",
    "nlls = -probs.log()\n",
    "nlls.retain_grad()\n",
    "loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "\n",
    "# backward\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## manual backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buffer grad\n",
    "nlls_grad = torch.zeros(bs, vocab_size)\n",
    "probs_grad = torch.zeros(bs, vocab_size)\n",
    "count_grad = torch.zeros(bs, 1)\n",
    "exp_l_grad = torch.zeros(bs, vocab_size)\n",
    "logits_grad = torch.zeros(bs, vocab_size)\n",
    "h_grad = torch.zeros(bs, n_hidden)\n",
    "hpreact_bn_grad = torch.zeros(bs, n_hidden)\n",
    "bnmeani_grad = torch.zeros(1, n_hidden)\n",
    "bnstdi_grad = torch.zeros(1, n_hidden)\n",
    "bnvari_grad = torch.zeros(1, n_hidden)\n",
    "hpreact_grad = torch.zeros(bs, n_hidden)\n",
    "emb_grad = torch.zeros(bs, n_embd * block_size)\n",
    "# param grad\n",
    "C_grad = torch.zeros(vocab_size, n_embd)\n",
    "w1_grad = torch.zeros(n_embd * block_size, n_hidden)\n",
    "w2_grad = torch.zeros(n_hidden, vocab_size)\n",
    "b2_grad = torch.zeros(vocab_size)\n",
    "bnw_grad = torch.zeros(n_hidden)\n",
    "bnb_grad = torch.zeros(n_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "same grad for loss calculation: [True, True, True, True, True]\n",
      "same grad for logits calculation: [True, True, True, True, True, True]\n",
      "same grad for params: [True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "# 1. loss\n",
    "nlls_grad[torch.arange(y.shape[0]), y] = 1 / bs\n",
    "probs_grad[torch.arange(y.shape[0]), y] = -1 / probs.data[torch.arange(y.shape[0]), y] * nlls_grad[torch.arange(y.shape[0]), y]\n",
    "count_grad = -(exp_l.data * probs_grad).sum(dim=-1, keepdim=True) / count.data**2\n",
    "exp_l_grad = probs_grad / count.data + count_grad  # one is from e/c to e, one is from c=\\sum e to e\n",
    "logits_grad = exp_l.data * exp_l_grad\n",
    "\n",
    "# 2. logits\n",
    "h_grad = logits_grad @ w2.data.T\n",
    "hpreact_bn_grad = h_grad * (1 - h.data**2)\n",
    "# bn\n",
    "bnmeani_grad = ((-bnw.data / bnstdi.data) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "bnstdi_grad = (-((hpreact.data - bnmeani.data) * bnw.data / bnstdi.data**2) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "# hpreact\n",
    "hpreact_grad_mean = bnmeani_grad * torch.ones_like(hpreact.data) / bs\n",
    "hpreact_grad_std = bnstdi_grad * (1 / 2 / bnstdi.data) * (1 / bs) * (2 * var_proj @ hpreact.data)\n",
    "hpreact_grad_direct = hpreact_bn_grad * (bnw.data / bnstdi.data)\n",
    "hpreact_grad = hpreact_grad_mean + hpreact_grad_std + hpreact_grad_direct\n",
    "# emb\n",
    "emb_grad = hpreact_grad @ w1.data.T\n",
    "\n",
    "# 3. params\n",
    "C_grad.index_add_(dim=0, index=x.view(-1), source=emb_grad.view(-1, n_embd)) # add emb_grad[i] to C[x[i]]\n",
    "w1_grad = emb.data.T @ hpreact_grad\n",
    "w2_grad = h.data.T @ logits_grad\n",
    "b2_grad = logits_grad.sum(dim=0)\n",
    "bnw_grad = ((hpreact.data - bnmeani.data) / bnstdi.data * hpreact_bn_grad).sum(dim=0)\n",
    "bnb_grad = hpreact_bn_grad.sum(dim=0)\n",
    "\n",
    "# check\n",
    "is_equal1 = [torch.allclose(nlls_grad, nlls.grad), torch.allclose(probs_grad, probs.grad), torch.allclose(count_grad, count.grad), torch.allclose(exp_l_grad, exp_l.grad), torch.allclose(logits_grad, logits.grad)]\n",
    "is_equal2 = [torch.allclose(h_grad, h.grad), torch.allclose(hpreact_bn_grad, hpreact_bn.grad), torch.allclose(bnmeani_grad, bnmeani.grad), torch.allclose(bnstdi_grad, bnstdi.grad), torch.allclose(hpreact_grad, hpreact.grad), torch.allclose(emb_grad, emb.grad)]\n",
    "is_equal3 = [torch.allclose(C_grad, C.grad), torch.allclose(w1_grad, w1.grad), torch.allclose(w2_grad, w2.grad), torch.allclose(b2_grad, b2.grad), torch.allclose(bnw_grad, bnw.grad), torch.allclose(bnb_grad, bnb.grad)]\n",
    "print('same grad for loss calculation:', is_equal1)\n",
    "print('same grad for logits calculation:', is_equal2)\n",
    "print('same grad for params:', is_equal3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 3.3074488639831543, val loss: 3.3160440921783447\n",
      "step: 1000, train loss: 2.583611488342285, val loss: 2.4248502254486084\n",
      "step: 2000, train loss: 2.4143431186676025, val loss: 2.390408992767334\n",
      "step: 3000, train loss: 2.1222798824310303, val loss: 2.379809856414795\n",
      "step: 4000, train loss: 2.1513724327087402, val loss: 2.374274969100952\n",
      "step: 5000, train loss: 2.349586009979248, val loss: 2.386371374130249\n",
      "step: 6000, train loss: 2.2481563091278076, val loss: 2.2990658283233643\n",
      "step: 7000, train loss: 2.0973422527313232, val loss: 2.2958080768585205\n",
      "step: 8000, train loss: 2.5018582344055176, val loss: 2.2962090969085693\n",
      "step: 9000, train loss: 2.072721242904663, val loss: 2.288125991821289\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# model\n",
    "torch.manual_seed(42)\n",
    "C = torch.randn(vocab_size, n_embd)\n",
    "w1 = torch.randn(n_embd * block_size, n_hidden) * (n_embd * block_size)**-0.5\n",
    "w2 = torch.randn(n_hidden, vocab_size) * (5/3) * (n_hidden)**-0.5 * 0.1 # 0.1 is for less confident at initialization\n",
    "b2 = torch.randn(vocab_size) * 0\n",
    "bnw = torch.ones(n_hidden)\n",
    "bnb = torch.zeros(n_hidden)\n",
    "params = [C, w1, w2, b2, bnw, bnb]\n",
    "bnmean_running = torch.zeros(n_hidden)\n",
    "bnstd_running = torch.ones(n_hidden)\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 10000\n",
    "ini_lr = 1.0\n",
    "\n",
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # ---------------- forward --------------------\n",
    "    # 1. logits\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    hpreact = emb @ w1\n",
    "    bnmeani = mean_proj @ hpreact\n",
    "    bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "    hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "    h = hpreact_bn.tanh()\n",
    "    logits = h @ w2 + b2\n",
    "    # 2. loss\n",
    "    exp_l = logits.exp()\n",
    "    count = exp_l.sum(dim=-1, keepdim=True)\n",
    "    probs = exp_l / count\n",
    "    nlls = -probs.log()\n",
    "    loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "    \n",
    "\n",
    "\n",
    "    # ---------------- backward --------------------\n",
    "    # 0. zero grad\n",
    "    # buffer grad\n",
    "    nlls_grad = torch.zeros(bs, vocab_size)\n",
    "    probs_grad = torch.zeros(bs, vocab_size)\n",
    "    count_grad = torch.zeros(bs, 1)\n",
    "    exp_l_grad = torch.zeros(bs, vocab_size)\n",
    "    logits_grad = torch.zeros(bs, vocab_size)\n",
    "    h_grad = torch.zeros(bs, n_hidden)\n",
    "    hpreact_bn_grad = torch.zeros(bs, n_hidden)\n",
    "    bnmeani_grad = torch.zeros(1, n_hidden)\n",
    "    bnstdi_grad = torch.zeros(1, n_hidden)\n",
    "    bnvari_grad = torch.zeros(1, n_hidden)\n",
    "    hpreact_grad = torch.zeros(bs, n_hidden)\n",
    "    emb_grad = torch.zeros(bs, n_embd * block_size)\n",
    "    # param grad\n",
    "    C_grad = torch.zeros(vocab_size, n_embd)\n",
    "    w1_grad = torch.zeros(n_embd * block_size, n_hidden)\n",
    "    w2_grad = torch.zeros(n_hidden, vocab_size)\n",
    "    b2_grad = torch.zeros(vocab_size)\n",
    "    bnw_grad = torch.zeros(n_hidden)\n",
    "    bnb_grad = torch.zeros(n_hidden)\n",
    "\n",
    "    # 1. loss\n",
    "    nlls_grad[torch.arange(y.shape[0]), y] = 1 / bs\n",
    "    probs_grad[torch.arange(y.shape[0]), y] = -1 / probs.data[torch.arange(y.shape[0]), y] * nlls_grad[torch.arange(y.shape[0]), y]\n",
    "    count_grad = -(exp_l.data * probs_grad).sum(dim=-1, keepdim=True) / count.data**2\n",
    "    exp_l_grad = probs_grad / count.data + count_grad  # one is from e/c to e, one is from c=\\sum e to e\n",
    "    logits_grad = exp_l.data * exp_l_grad\n",
    "\n",
    "    # 2. logits\n",
    "    h_grad = logits_grad @ w2.data.T\n",
    "    hpreact_bn_grad = h_grad * (1 - h.data**2)\n",
    "    # bn\n",
    "    bnmeani_grad = ((-bnw.data / bnstdi.data) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "    bnstdi_grad = (-((hpreact.data - bnmeani.data) * bnw.data / bnstdi.data**2) * hpreact_bn_grad).sum(dim=0, keepdim=True)\n",
    "    # hpreact\n",
    "    hpreact_grad_mean = bnmeani_grad * torch.ones_like(hpreact.data) / bs\n",
    "    hpreact_grad_std = bnstdi_grad * (1 / 2 / bnstdi.data) * (1 / bs) * (2 * var_proj @ hpreact.data)\n",
    "    hpreact_grad_direct = hpreact_bn_grad * (bnw.data / bnstdi.data)\n",
    "    hpreact_grad = hpreact_grad_mean + hpreact_grad_std + hpreact_grad_direct\n",
    "    # emb\n",
    "    emb_grad = hpreact_grad @ w1.data.T\n",
    "    \n",
    "    # 3. params\n",
    "    C_grad.index_add_(dim=0, index=x.view(-1), source=emb_grad.view(-1, n_embd)) # add emb_grad[i] to C[x[i]]\n",
    "    w1_grad = emb.data.T @ hpreact_grad\n",
    "    w2_grad = h.data.T @ logits_grad\n",
    "    b2_grad = logits_grad.sum(dim=0)\n",
    "    bnw_grad = ((hpreact.data - bnmeani.data) / bnstdi.data * hpreact_bn_grad).sum(dim=0)\n",
    "    bnb_grad = hpreact_bn_grad.sum(dim=0)\n",
    "    param_grads = [C_grad, w1_grad, w2_grad, b2_grad, bnw_grad, bnb_grad]\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            emb = C[X_val].view(X_val.shape[0], -1)\n",
    "            hpreact = emb @ w1\n",
    "            hpreact = (hpreact - hpreact.mean(dim=0, keepdim=True)) / hpreact.std(dim=0, keepdim=True) * bnw + bnb\n",
    "            h = hpreact.tanh()\n",
    "            logits = h @ w2 + b2\n",
    "            val_loss = F.cross_entropy(logits, Y_val)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    for p, g in zip(params, param_grads):\n",
    "        p.data -= lr * g\n",
    "    \n",
    "    bnmean_running = bnmean_running * 0.99 + bnmeani * 0.01\n",
    "    bnstd_running = bnstd_running * 0.99 + bnstdi * 0.01\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1794, -0.1739,  0.0228,  ...,  0.0201, -0.1145,  0.1450],\n",
       "        [-0.3108,  0.1481, -0.2057,  ..., -0.3503,  0.2971,  0.1593],\n",
       "        [-0.0755,  0.0891, -0.0533,  ..., -0.1936, -0.2113, -0.3970],\n",
       "        ...,\n",
       "        [ 0.0905, -0.2106, -0.5778,  ..., -0.1088, -0.1970, -0.3650],\n",
       "        [-0.1003,  0.1913, -0.4233,  ...,  0.3125,  0.0998, -0.2520],\n",
       "        [ 0.2542,  0.2179, -0.3316,  ..., -0.4188, -0.2184, -0.4282]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17936445772647858"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[0,0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, train loss: 3.3074488639831543, val loss: 3.3160440921783447\n",
      "step: 1000, train loss: 2.583611488342285, val loss: 2.4248502254486084\n",
      "step: 2000, train loss: 2.4143431186676025, val loss: 2.390408992767334\n",
      "step: 3000, train loss: 2.1222798824310303, val loss: 2.379809856414795\n",
      "step: 4000, train loss: 2.1513726711273193, val loss: 2.374274969100952\n",
      "step: 5000, train loss: 2.349586009979248, val loss: 2.386371374130249\n",
      "step: 6000, train loss: 2.2481563091278076, val loss: 2.2990663051605225\n",
      "step: 7000, train loss: 2.0973422527313232, val loss: 2.2958080768585205\n",
      "step: 8000, train loss: 2.5018584728240967, val loss: 2.2962090969085693\n",
      "step: 9000, train loss: 2.072721242904663, val loss: 2.288125991821289\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# model\n",
    "params = get_params()\n",
    "C, w1, w2, b2, bnw, bnb = params\n",
    "bnmean_running = torch.zeros(n_hidden)\n",
    "bnstd_running = torch.ones(n_hidden)\n",
    "\n",
    "# args\n",
    "bs = 32\n",
    "n_steps = 10000\n",
    "ini_lr = 1.0\n",
    "\n",
    "# buffer\n",
    "mean_proj = torch.ones(1, bs) / bs\n",
    "var_proj = (torch.eye(bs) - mean_proj)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < n_steps // 2 else ini_lr / 10\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # forward\n",
    "    emb = C[x].view(x.shape[0], -1)\n",
    "    hpreact = emb @ w1\n",
    "    bnmeani = mean_proj @ hpreact\n",
    "    bnstdi = (var_proj @ hpreact).square().mean(dim=0, keepdim=True).sqrt()\n",
    "    hpreact_bn = (hpreact - bnmeani) / bnstdi * bnw + bnb\n",
    "    h = hpreact_bn.tanh()\n",
    "    logits = h @ w2 + b2\n",
    "    # 2. loss\n",
    "    exp_l = logits.exp()\n",
    "    count = exp_l.sum(dim=-1, keepdim=True)\n",
    "    probs = exp_l / count\n",
    "    nlls = -probs.log()\n",
    "    loss = nlls[torch.arange(y.shape[0]), y].mean()\n",
    "\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    if step % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            emb = C[X_val].view(X_val.shape[0], -1)\n",
    "            hpreact = emb @ w1\n",
    "            hpreact = (hpreact - hpreact.mean(dim=0, keepdim=True)) / hpreact.std(dim=0, keepdim=True) * bnw + bnb\n",
    "            h = hpreact.tanh()\n",
    "            logits = h @ w2 + b2\n",
    "            val_loss = F.cross_entropy(logits, Y_val)\n",
    "            print(f'step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "    \n",
    "    # update\n",
    "    for p in params:\n",
    "        p.data -= lr * p.grad\n",
    "        p.grad = None\n",
    "    with torch.no_grad():\n",
    "        bnmean_running = bnmean_running * 0.99 + bnmeani * 0.01\n",
    "        bnstd_running = bnstd_running * 0.99 + bnstdi * 0.01\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1794, -0.1739,  0.0228,  ...,  0.0201, -0.1145,  0.1450],\n",
       "        [-0.3108,  0.1481, -0.2057,  ..., -0.3503,  0.2971,  0.1593],\n",
       "        [-0.0755,  0.0891, -0.0533,  ..., -0.1936, -0.2113, -0.3970],\n",
       "        ...,\n",
       "        [ 0.0905, -0.2106, -0.5778,  ..., -0.1088, -0.1970, -0.3650],\n",
       "        [-0.1003,  0.1913, -0.4233,  ...,  0.3125,  0.0998, -0.2520],\n",
       "        [ 0.2542,  0.2179, -0.3316,  ..., -0.4188, -0.2184, -0.4282]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17936447262763977"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1[0,0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simpler grad\n",
    "## BatchNorm \n",
    "\n",
    "Let $x\\in\\mathbb{R}^{n\\times d}$, $w\\in\\mathbb{R}^d$, $b\\in\\mathbb{R}^d$, define $\\bar{x} = x\\text{.mean}(\\text{dim=0})$ then\n",
    "\n",
    "$$\n",
    "    o = \\frac{x - \\bar{x}}{\\sqrt{(x - \\bar{x})^2\\text{.mean(dim=0)} + \\epsilon}} w + b \\in \\mathbb{R}^{n \\times d}\n",
    "$$\n",
    "\n",
    "Note: as torch, we don't use Bessel correction\n",
    "\n",
    "Let $s = \\sqrt{(x - \\bar{x})^2\\text{.mean(dim=0)} + \\epsilon}$ and $x_{\\text{norm}} = \\frac{x - \\bar{x}}{s}$.\n",
    "\n",
    "Denote $dx$ as grad from the end layer to current layer, $dy/dx$ as grad from next layer to current layer.\n",
    "\n",
    "$$\n",
    "    dw =  do \\cdot \\frac{do}{dw} = \\left(x_{\\text{norm}} * do\\right).~\\text{sum}(\\text{dim=0}) \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "$$\n",
    "    db =  do \\cdot \\frac{do}{db} = \\left(do\\right).~\\text{sum}(\\text{dim=0}) \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "Grad $dx$ is more complex, but if we directly use computation graph to calculate grad in scalar level, and then simplify the computation with tensor operations and algebraic transformation. It's easy to see\n",
    "\n",
    "$$\n",
    "    dx = \\left(\n",
    "            s * do - s * do\\text{.mean(dim=0)} - \\frac{1}{s} * (x - \\bar{x}) * ((x - \\bar{x}) * do).~\\text{mean}(\\text{dim=0})\n",
    "    \\right) * w * \\frac{1}{s^2}\n",
    "$$\n",
    "\n",
    "Combine same terms and use quantities already calculated in forward pass, we get\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    dx &= \\left(\n",
    "            (do - do\\text{.mean(dim=0)}) - \\frac{x - \\bar{x}}{s} * \\left(\\frac{x - \\bar{x}}{s} * do\\right).~\\text{mean}(\\text{dim=0})\n",
    "    \\right) * w * \\frac{1}{s} \\\\\n",
    "    &= \\left(\n",
    "            (do - \\frac{db}{n}) - x_\\text{norm} * \\left(\\frac{dw}{n}\\right)\n",
    "    \\right) * w * \\frac{1}{s}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## LayerNorm\n",
    "\n",
    "Almost the same as BatchNorm, but we need to consider the last dim.\n",
    "\n",
    "$$\n",
    "    dw =  do \\cdot \\frac{do}{dw} = \\left(x_{\\text{norm}} * do\\right).~\\text{sum}(\\text{dim=[0,1,ndim-1]}) \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "$$\n",
    "    db =  do \\cdot \\frac{do}{db} = \\left(do\\right).~\\text{sum}(\\text{dim=[0,1,ndim-1]}) \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "$$\n",
    "    dx = \\left(\n",
    "            (do - do\\text{.mean(dim=-1)}) - \\frac{x - \\bar{x}}{s} * \\left(\\frac{x - \\bar{x}}{s} * do\\right).~\\text{mean}(\\text{dim=-1})\n",
    "    \\right) * w * \\frac{1}{s}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiny_torch import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x grad relative error: 6.827384472361946e-16\n"
     ]
    }
   ],
   "source": [
    "# --- manual ---\n",
    "loss_fn = CrossEntropyLoss()\n",
    "x = torch.randn(100, 10, dtype=torch.float64)\n",
    "y = torch.randint(0, 10, (100,))\n",
    "loss = loss_fn(x, y)\n",
    "x_grad = loss_fn.backward(grad=1.0) # last layer, dloss=1.0\n",
    "\n",
    "\n",
    "# --- torch ---\n",
    "import torch.nn as nn\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "xt, yt = x.clone(), y.clone()\n",
    "xt.requires_grad = True\n",
    "loss = loss_fn(xt, yt)\n",
    "loss.backward()\n",
    "print(f'x grad relative error: {((xt.grad - x_grad) / xt.grad).abs().max().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass:\n",
      "o relative error: 5.649903956929978e-15\n",
      "backward pass:\n",
      "db relative error: 4.587874512547105e-16\n",
      "dw relative error: 1.4906969562791864e-15\n",
      "dx relative error: 1.8066190659672137e-14\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dtype = torch.float64\n",
    "eps = 1e-5\n",
    "# model\n",
    "bn = BatchNorm1d(10, dtype=dtype, eps=eps)\n",
    "# params\n",
    "x = torch.randn(100, 10, dtype=dtype)\n",
    "# ------- manual -------\n",
    "# forward\n",
    "o = bn(x)\n",
    "# backward\n",
    "do = torch.randn_like(o, dtype=dtype)\n",
    "dx = bn.backward(do)\n",
    "\n",
    "# ------- torch -------\n",
    "import torch.nn as nn\n",
    "bnt = nn.BatchNorm1d(10, dtype=dtype, eps=eps)\n",
    "bnt.weight.data = bn.weight.data\n",
    "bnt.bias.data = bn.bias.data\n",
    "xt = x.clone()\n",
    "xt.requires_grad = True\n",
    "# forward\n",
    "ot = bnt(xt)\n",
    "# backward\n",
    "(ot * do).sum().backward()\n",
    "\n",
    "# -------- compare -------- \n",
    "print('forward pass:')\n",
    "print(f'o relative error: {((ot - o) / ot).abs().max().item()}')\n",
    "print('backward pass:')\n",
    "print(f'db relative error: {((bnt.bias.grad - bn.bias_grad) / bnt.bias.grad).abs().max().item()}')\n",
    "print(f'dw relative error: {((bnt.weight.grad - bn.weight_grad) / bnt.weight.grad).abs().max().item()}')\n",
    "print(f'dx relative error: {((dx - xt.grad) / xt.grad).abs().max().item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward pass:\n",
      "o relative error: 1.1410527296324882e-11\n",
      "backward pass:\n",
      "db relative error: 8.099100649135957e-15\n",
      "dw relative error: 6.349328555149738e-15\n",
      "dx relative error: 7.678927882277915e-12\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "dtype = torch.float64\n",
    "eps = 1e-5\n",
    "# model\n",
    "ln = LayerNorm(10, dtype=dtype, eps=eps)\n",
    "# params\n",
    "x = torch.randn(3, 32, 100, 10, dtype=dtype)\n",
    "# ------- manual -------\n",
    "# forward\n",
    "o = ln(x)\n",
    "# backward\n",
    "do = torch.randn_like(o, dtype=dtype)\n",
    "dx = ln.backward(do)\n",
    "\n",
    "# ------- torch -------\n",
    "import torch.nn as nn\n",
    "lnt = nn.LayerNorm(10, dtype=dtype, eps=eps)\n",
    "lnt.weight.data = ln.weight.data\n",
    "lnt.bias.data = ln.bias.data\n",
    "xt = x.clone()\n",
    "xt.requires_grad = True\n",
    "# forward\n",
    "ot = lnt(xt)\n",
    "# backward\n",
    "ot.backward(do)\n",
    "\n",
    "# -------- compare -------- \n",
    "print('forward pass:')\n",
    "print(f'o relative error: {((ot - o) / ot).abs().max().item()}')\n",
    "print('backward pass:')\n",
    "print(f'db relative error: {((lnt.bias.grad - ln.bias_grad) / lnt.bias.grad).abs().max().item()}')\n",
    "print(f'dw relative error: {((lnt.weight.grad - ln.weight_grad) / lnt.weight.grad).abs().max().item()}')\n",
    "print(f'dx relative error: {((dx - xt.grad) / xt.grad).abs().max().item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of params: 0.72M\n",
      "check grad:\n",
      "[Layer 1] weight grad relative error: 6.348547564942509e-12\n",
      "x_grad relative error: 1.2315982383522732e-11\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "n_embd = 30\n",
    "n_hidden = 100\n",
    "bs = 32\n",
    "dtype = torch.float64\n",
    "# model\n",
    "layers = [Linear(n_embd, n_hidden, bias=False, dtype=dtype), BatchNorm1d(n_hidden, dtype=dtype), Tanh()]\n",
    "for _ in range(70):\n",
    "    layers.extend([Linear(n_hidden, n_hidden, bias=False, dtype=dtype), BatchNorm1d(n_hidden, dtype=dtype), Tanh()])\n",
    "params = [p for l in layers for p in l.parameters()]\n",
    "print(f'number of params: {sum(p.numel() for p in params) / 1e6:.2f}M')\n",
    "# input\n",
    "x = torch.randn(bs, n_embd, dtype=dtype, requires_grad=True)\n",
    "\n",
    "# --- manual ---\n",
    "# forward\n",
    "h = x\n",
    "for l in layers:\n",
    "    h = l(h)\n",
    "\n",
    "# backward\n",
    "grad = torch.ones(bs, n_hidden)\n",
    "for i in range(len(layers)-1, -1, -1):\n",
    "    grad = layers[i].backward(grad)\n",
    "\n",
    "\n",
    "# --- torch ---\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "h = x\n",
    "for l in layers:\n",
    "    h = l(h)\n",
    "h.sum().backward()\n",
    "\n",
    "# --- compare ---\n",
    "print('check grad:')\n",
    "print(f'[Layer 1] weight grad relative error: {((params[0].grad - layers[0].weight_grad) / params[0].grad).abs().max().item()}')\n",
    "print(f'x_grad relative error: {((x.grad - grad) / x.grad).abs().max().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train mlp and compare with torch\n",
    "Exactly the same(regardless of tiny float error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiny_torch import *\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self, vocab_size, block_size, n_embd, n_hidden, n_layer, dtype=torch.float64, generator=None):\n",
    "        layers = [Embedding(vocab_size, n_embd, dtype=dtype, generator=generator), Flatten(), Linear(n_embd * block_size, n_hidden, bias=False, dtype=dtype, generator=generator), BatchNorm1d(n_hidden, dtype=dtype), Tanh()]\n",
    "        for _ in range(n_layer-2):\n",
    "            layers.extend([Linear(n_hidden, n_hidden, bias=False, dtype=dtype, generator=generator), BatchNorm1d(n_hidden, dtype=dtype), Tanh()])\n",
    "        layers.extend([Linear(n_hidden, vocab_size, bias=False, dtype=dtype, generator=generator), BatchNorm1d(vocab_size, dtype=dtype)])\n",
    "        layers[-1].weight.data *= 0.1\n",
    "        self.net = Sequential(layers)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.net.parameters()\n",
    "    \n",
    "    def grads(self):\n",
    "        return self.net.grads()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        grad = self.net.backward(grad)\n",
    "        return grad # None\n",
    "    \n",
    "    def eval(self):\n",
    "        for l in self.net.layers:\n",
    "            l._training = False\n",
    "\n",
    "    def train(self):\n",
    "        for l in self.net.layers:\n",
    "            l._training = True\n",
    "\n",
    "    def generate(self, s, max_new_tokens, do_sample=True, temperature=1.0):\n",
    "        assert isinstance(s, str), 'str in, str out'\n",
    "        assert len(s) == self.block_size, 'input string length must be equal to block size'\n",
    "        x = torch.tensor([[stoi[ch] for ch in s]])\n",
    "        for _ in range(max_new_tokens):\n",
    "            cond = x[:, -self.block_size:]\n",
    "            logits = self(cond) * (1 / temperature)\n",
    "            probs = logits.softmax(dim=-1)\n",
    "            if do_sample:\n",
    "                next_x = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                next_x = probs.argmax(dim=-1, keepdim=True)\n",
    "            x = torch.cat([x, next_x], dim=-1)\n",
    "            if next_x.item() == 0:\n",
    "                break\n",
    "        s = ''.join([itos[idx.item()] for idx in x[0]])\n",
    "        return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_t step: 0, train loss: 3.306549425058978, val loss: 3.26912796100787\n",
      "model   step: 0, train loss: 3.3065494250589778, val loss: 3.2691279610078707\n",
      "eval model forward diff: 1.0408340855860843e-16\n",
      "eval model running mean diff: 4.0657581468206416e-20\n",
      "eval model running var diff: 0.0\n",
      "train model forward diff: 5.551115123125783e-16\n",
      "\n",
      "model_t step: 1000, train loss: 3.0082452560929775, val loss: 2.6382328068110885\n",
      "model   step: 1000, train loss: 3.008245256092977, val loss: 2.6382328068110703\n",
      "eval model forward diff: 9.00790553259867e-12\n",
      "eval model running mean diff: 2.8245461525244764e-13\n",
      "eval model running var diff: 1.120215031846783e-13\n",
      "train model forward diff: 3.3674868449296014e-12\n",
      "\n",
      "model_t step: 2000, train loss: 2.4426375385086634, val loss: 2.405136117561469\n",
      "model   step: 2000, train loss: 2.4426375385087455, val loss: 2.4051361175614887\n",
      "eval model forward diff: 8.146372465489549e-12\n",
      "eval model running mean diff: 6.818989817247711e-13\n",
      "eval model running var diff: 5.601075159233915e-13\n",
      "train model forward diff: 3.5769165407373293e-12\n",
      "\n",
      "model_t step: 3000, train loss: 2.1951096446047855, val loss: 2.3346047081023253\n",
      "model   step: 3000, train loss: 2.195109644604814, val loss: 2.334604708102333\n",
      "eval model forward diff: 1.3872236692691331e-11\n",
      "eval model running mean diff: 1.198818821990244e-12\n",
      "eval model running var diff: 1.6415757642107565e-12\n",
      "train model forward diff: 7.353451181302262e-12\n",
      "\n",
      "model_t step: 4000, train loss: 2.5171445013012748, val loss: 2.3194163721369825\n",
      "model   step: 4000, train loss: 2.5171445013009395, val loss: 2.3194163721369607\n",
      "eval model forward diff: 1.5495937866205622e-11\n",
      "eval model running mean diff: 2.1916218839734825e-12\n",
      "eval model running var diff: 2.404743071338089e-12\n",
      "train model forward diff: 7.687628311714434e-12\n",
      "\n",
      "model_t step: 5000, train loss: 2.321277507404877, val loss: 2.3007085082741985\n",
      "model   step: 5000, train loss: 2.3212775074051026, val loss: 2.3007085082742202\n",
      "eval model forward diff: 2.0904611375271998e-11\n",
      "eval model running mean diff: 3.089639655229348e-12\n",
      "eval model running var diff: 5.905054223376283e-12\n",
      "train model forward diff: 1.1253442622205512e-11\n",
      "\n",
      "model_t step: 6000, train loss: 2.1492628186358163, val loss: 2.294943976800826\n",
      "model   step: 6000, train loss: 2.149262818635909, val loss: 2.2949439768008104\n",
      "eval model forward diff: 2.750777383653258e-11\n",
      "eval model running mean diff: 4.2269521216553585e-12\n",
      "eval model running var diff: 7.708500504577387e-12\n",
      "train model forward diff: 1.1649126108181918e-11\n",
      "\n",
      "model_t step: 7000, train loss: 2.0379589962767453, val loss: 2.3000847654252508\n",
      "model   step: 7000, train loss: 2.0379589962773226, val loss: 2.300084765425375\n",
      "eval model forward diff: 3.346034560536282e-11\n",
      "eval model running mean diff: 6.323830348264892e-12\n",
      "eval model running var diff: 9.180212146020494e-12\n",
      "train model forward diff: 1.5718815138399123e-11\n",
      "\n",
      "model_t step: 8000, train loss: 2.2961075089866956, val loss: 2.277819244008203\n",
      "model   step: 8000, train loss: 2.2961075089858936, val loss: 2.277819244008084\n",
      "eval model forward diff: 3.066369380633205e-11\n",
      "eval model running mean diff: 8.509970506054287e-12\n",
      "eval model running var diff: 1.0128786698260228e-11\n",
      "train model forward diff: 2.50028886483733e-11\n",
      "\n",
      "model_t step: 9000, train loss: 2.2735662929819997, val loss: 2.268529928285768\n",
      "model   step: 9000, train loss: 2.273566292983051, val loss: 2.2685299282857816\n",
      "eval model forward diff: 4.0172754012246514e-11\n",
      "eval model running mean diff: 5.240266554018547e-12\n",
      "eval model running var diff: 1.1580958414469933e-11\n",
      "train model forward diff: 2.2826407430898144e-11\n",
      "\n",
      "model_t step: 10000, train loss: 2.167743793019498, val loss: 2.2771758839304628\n",
      "model   step: 10000, train loss: 2.1677437930190813, val loss: 2.27717588393055\n",
      "eval model forward diff: 5.411093795260058e-11\n",
      "eval model running mean diff: 8.13316081149651e-12\n",
      "eval model running var diff: 1.8246737454319373e-11\n",
      "train model forward diff: 2.2282176104226892e-11\n",
      "\n",
      "model_t step: 11000, train loss: 2.4588574929408256, val loss: 2.263740171812021\n",
      "model   step: 11000, train loss: 2.458857492941886, val loss: 2.2637401718119623\n",
      "eval model forward diff: 5.289901849891976e-11\n",
      "eval model running mean diff: 1.004762939516013e-11\n",
      "eval model running var diff: 2.106581575844757e-11\n",
      "train model forward diff: 4.23832080542752e-11\n",
      "\n",
      "model_t step: 12000, train loss: 1.8322467581031268, val loss: 2.2394553778779476\n",
      "model   step: 12000, train loss: 1.8322467581031234, val loss: 2.2394553778780706\n",
      "eval model forward diff: 5.994293950095653e-11\n",
      "eval model running mean diff: 1.2174150576527154e-11\n",
      "eval model running var diff: 2.589217729109805e-11\n",
      "train model forward diff: 3.066891185454779e-11\n",
      "\n",
      "model_t step: 13000, train loss: 2.1038304320043446, val loss: 2.2548559170956923\n",
      "model   step: 13000, train loss: 2.1038304320046053, val loss: 2.2548559170956834\n",
      "eval model forward diff: 7.90687515461741e-11\n",
      "eval model running mean diff: 1.7391976747660465e-11\n",
      "eval model running var diff: 3.284661431735003e-11\n",
      "train model forward diff: 4.526912178448583e-11\n",
      "\n",
      "model_t step: 14000, train loss: 1.9138588250109607, val loss: 2.2300098428852344\n",
      "model   step: 14000, train loss: 1.913858825011408, val loss: 2.230009842885303\n",
      "eval model forward diff: 9.451106564029033e-11\n",
      "eval model running mean diff: 1.9561277944668554e-11\n",
      "eval model running var diff: 4.502531680827815e-11\n",
      "train model forward diff: 3.7633895999533706e-11\n",
      "\n",
      "model_t step: 15000, train loss: 2.0507760794224588, val loss: 2.2326865428920044\n",
      "model   step: 15000, train loss: 2.0507760794207313, val loss: 2.232686542892104\n",
      "eval model forward diff: 1.2140999317011847e-10\n",
      "eval model running mean diff: 2.8143709585037868e-11\n",
      "eval model running var diff: 4.605027470461209e-11\n",
      "train model forward diff: 5.823475035526826e-11\n",
      "\n",
      "model_t step: 16000, train loss: 2.3455457997011946, val loss: 2.237146548652529\n",
      "model   step: 16000, train loss: 2.3455457996993583, val loss: 2.2371465486524373\n",
      "eval model forward diff: 1.2067102872492796e-10\n",
      "eval model running mean diff: 3.447192531425003e-11\n",
      "eval model running var diff: 5.5926818731677486e-11\n",
      "train model forward diff: 6.370814986667028e-11\n",
      "\n",
      "model_t step: 17000, train loss: 2.5878914973479414, val loss: 2.2232535595481333\n",
      "model   step: 17000, train loss: 2.58789149735178, val loss: 2.223253559548043\n",
      "eval model forward diff: 1.2760481560292192e-10\n",
      "eval model running mean diff: 3.54525298007502e-11\n",
      "eval model running var diff: 9.207035134295438e-11\n",
      "train model forward diff: 9.039502479879502e-11\n",
      "\n",
      "model_t step: 18000, train loss: 1.9701596222175144, val loss: 2.223776104749694\n",
      "model   step: 18000, train loss: 1.9701596222180078, val loss: 2.223776104749712\n",
      "eval model forward diff: 1.5181944590381136e-10\n",
      "eval model running mean diff: 3.948197324632474e-11\n",
      "eval model running var diff: 9.142198109657329e-11\n",
      "train model forward diff: 1.0654099824591867e-10\n",
      "\n",
      "model_t step: 19000, train loss: 2.206823393640559, val loss: 2.2256468181888356\n",
      "model   step: 19000, train loss: 2.2068233936362858, val loss: 2.2256468181884914\n",
      "eval model forward diff: 1.8523849121265812e-10\n",
      "eval model running mean diff: 4.2835734959112415e-11\n",
      "eval model running var diff: 6.931344387339777e-11\n",
      "train model forward diff: 8.77904415830244e-11\n",
      "\n",
      "model_t step: 20000, train loss: 2.3375407477002734, val loss: 2.228398439363761\n",
      "model   step: 20000, train loss: 2.3375407477020516, val loss: 2.2283984393637852\n",
      "eval model forward diff: 2.020614786601982e-10\n",
      "eval model running mean diff: 4.932598773876862e-11\n",
      "eval model running var diff: 8.254019689957204e-11\n",
      "train model forward diff: 1.6820156378827278e-10\n",
      "\n",
      "model_t step: 21000, train loss: 2.2513798826973086, val loss: 2.2264474377841657\n",
      "model   step: 21000, train loss: 2.2513798826954825, val loss: 2.2264474377836954\n",
      "eval model forward diff: 2.5407331794014e-10\n",
      "eval model running mean diff: 6.761392834508939e-11\n",
      "eval model running var diff: 1.0704503949909849e-10\n",
      "train model forward diff: 1.7017157127874327e-10\n",
      "\n",
      "model_t step: 22000, train loss: 2.0475963271582116, val loss: 2.209954252349779\n",
      "model   step: 22000, train loss: 2.0475963271485345, val loss: 2.209954252349993\n",
      "eval model forward diff: 3.9573100352185975e-10\n",
      "eval model running mean diff: 9.899599195950515e-11\n",
      "eval model running var diff: 1.2116885272916988e-10\n",
      "train model forward diff: 1.8476242757969885e-10\n",
      "\n",
      "model_t step: 23000, train loss: 2.3662585645431613, val loss: 2.205125352238873\n",
      "model   step: 23000, train loss: 2.3662585645427945, val loss: 2.205125352239101\n",
      "eval model forward diff: 3.264524206514352e-10\n",
      "eval model running mean diff: 1.115482428648562e-10\n",
      "eval model running var diff: 1.7124079931818414e-10\n",
      "train model forward diff: 1.5007728393356956e-10\n",
      "\n",
      "model_t step: 24000, train loss: 2.1689444107611755, val loss: 2.2146849692112145\n",
      "model   step: 24000, train loss: 2.1689444107562945, val loss: 2.214684969210931\n",
      "eval model forward diff: 3.758824362876112e-10\n",
      "eval model running mean diff: 1.167841726101848e-10\n",
      "eval model running var diff: 2.6653523832465e-10\n",
      "train model forward diff: 3.388136438076117e-10\n",
      "\n",
      "model_t step: 25000, train loss: 1.796977997042812, val loss: 2.209609461748128\n",
      "model   step: 25000, train loss: 1.796977997040006, val loss: 2.209609461747827\n",
      "eval model forward diff: 3.828697359153921e-10\n",
      "eval model running mean diff: 1.4809997672671216e-10\n",
      "eval model running var diff: 2.3102586510503897e-10\n",
      "train model forward diff: 2.7001512137303507e-10\n",
      "\n",
      "model_t step: 26000, train loss: 2.130376809684315, val loss: 2.234489297107575\n",
      "model   step: 26000, train loss: 2.1303768096859117, val loss: 2.2344892971069203\n",
      "eval model forward diff: 4.2752001938595185e-10\n",
      "eval model running mean diff: 1.5274782827479072e-10\n",
      "eval model running var diff: 3.4861358244597795e-10\n",
      "train model forward diff: 2.539068955087487e-10\n",
      "\n",
      "model_t step: 27000, train loss: 2.598251198142548, val loss: 2.1946056822267375\n",
      "model   step: 27000, train loss: 2.598251198150355, val loss: 2.194605682227374\n",
      "eval model forward diff: 5.412963410833527e-10\n",
      "eval model running mean diff: 1.3025371059516289e-10\n",
      "eval model running var diff: 3.7135805541765876e-10\n",
      "train model forward diff: 4.1536463157854087e-10\n",
      "\n",
      "model_t step: 28000, train loss: 2.154319589888448, val loss: 2.1999763635683727\n",
      "model   step: 28000, train loss: 2.1543195898877823, val loss: 2.1999763635691405\n",
      "eval model forward diff: 5.664344548961253e-10\n",
      "eval model running mean diff: 1.355274781289495e-10\n",
      "eval model running var diff: 5.838955985382199e-10\n",
      "train model forward diff: 2.831335166320059e-10\n",
      "\n",
      "model_t step: 29000, train loss: 1.9572462045537138, val loss: 2.212181323078714\n",
      "model   step: 29000, train loss: 1.9572462045524825, val loss: 2.2121813230781937\n",
      "eval model forward diff: 6.41082298358242e-10\n",
      "eval model running mean diff: 1.310392350184486e-10\n",
      "eval model running var diff: 5.159037641533359e-10\n",
      "train model forward diff: 2.8328361878493524e-10\n",
      "\n",
      "model_t step: 30000, train loss: 2.2866762662137967, val loss: 2.1940292787368425\n",
      "model   step: 30000, train loss: 2.286676266233117, val loss: 2.194029278736206\n",
      "eval model forward diff: 6.439586641704409e-10\n",
      "eval model running mean diff: 1.6573031835775964e-10\n",
      "eval model running var diff: 5.231939326222346e-10\n",
      "train model forward diff: 3.972955298081615e-10\n",
      "\n",
      "model_t step: 31000, train loss: 2.4501600581562033, val loss: 2.191262836526995\n",
      "model   step: 31000, train loss: 2.450160058153566, val loss: 2.1912628365272138\n",
      "eval model forward diff: 7.493388132218115e-10\n",
      "eval model running mean diff: 2.139066701545289e-10\n",
      "eval model running var diff: 4.5998405084901606e-10\n",
      "train model forward diff: 7.1237948873204e-10\n",
      "\n",
      "model_t step: 32000, train loss: 1.9678737890839924, val loss: 2.2034131549932585\n",
      "model   step: 32000, train loss: 1.9678737890832045, val loss: 2.2034131549928864\n",
      "eval model forward diff: 9.06148489576708e-10\n",
      "eval model running mean diff: 1.9393375794152234e-10\n",
      "eval model running var diff: 6.887219683449075e-10\n",
      "train model forward diff: 9.590539473691706e-10\n",
      "\n",
      "model_t step: 33000, train loss: 1.9597684716843664, val loss: 2.2088576594758145\n",
      "model   step: 33000, train loss: 1.9597684716927883, val loss: 2.2088576594790443\n",
      "eval model forward diff: 9.021132729714054e-10\n",
      "eval model running mean diff: 2.720699221470113e-10\n",
      "eval model running var diff: 7.918110611626616e-10\n",
      "train model forward diff: 5.686335846633028e-10\n",
      "\n",
      "model_t step: 34000, train loss: 2.1358133536680057, val loss: 2.205257531598912\n",
      "model   step: 34000, train loss: 2.135813353664432, val loss: 2.2052575315979417\n",
      "eval model forward diff: 1.1659668786023758e-09\n",
      "eval model running mean diff: 2.814406485640575e-10\n",
      "eval model running var diff: 8.208829171962861e-10\n",
      "train model forward diff: 4.867031222488549e-10\n",
      "\n",
      "model_t step: 35000, train loss: 2.209092121380594, val loss: 2.194259173859637\n",
      "model   step: 35000, train loss: 2.2090921213780232, val loss: 2.1942591738611616\n",
      "eval model forward diff: 1.6267571734118746e-09\n",
      "eval model running mean diff: 3.028379352842947e-10\n",
      "eval model running var diff: 1.0418403917356045e-09\n",
      "train model forward diff: 5.568994154714346e-10\n",
      "\n",
      "model_t step: 36000, train loss: 2.1968310192920786, val loss: 2.1898322503002188\n",
      "model   step: 36000, train loss: 2.1968310193172518, val loss: 2.189832250298869\n",
      "eval model forward diff: 1.7600065849165958e-09\n",
      "eval model running mean diff: 4.256721086726145e-10\n",
      "eval model running var diff: 1.2381562441987626e-09\n",
      "train model forward diff: 5.041980166708981e-10\n",
      "\n",
      "model_t step: 37000, train loss: 1.9970478450392024, val loss: 2.1812038048380047\n",
      "model   step: 37000, train loss: 1.9970478450332132, val loss: 2.1812038048347913\n",
      "eval model forward diff: 1.5589076696187476e-09\n",
      "eval model running mean diff: 4.07967826188127e-10\n",
      "eval model running var diff: 1.3618546290672384e-09\n",
      "train model forward diff: 1.6999914809190386e-09\n",
      "\n",
      "model_t step: 38000, train loss: 2.1657080091397827, val loss: 2.1759943673517346\n",
      "model   step: 38000, train loss: 2.165708009172352, val loss: 2.17599436735164\n",
      "eval model forward diff: 1.8741213025919023e-09\n",
      "eval model running mean diff: 4.1320985522119713e-10\n",
      "eval model running var diff: 1.54575019450931e-09\n",
      "train model forward diff: 2.003551102802703e-09\n",
      "\n",
      "model_t step: 39000, train loss: 2.2024294259324155, val loss: 2.1910150893726943\n",
      "model   step: 39000, train loss: 2.2024294259105806, val loss: 2.19101508937301\n",
      "eval model forward diff: 2.0415029666764895e-09\n",
      "eval model running mean diff: 4.131319730760197e-10\n",
      "eval model running var diff: 1.975635655071528e-09\n",
      "train model forward diff: 1.0372751546583459e-09\n",
      "\n",
      "model_t step: 40000, train loss: 2.27798269490166, val loss: 2.1858466249606447\n",
      "model   step: 40000, train loss: 2.2779826948459077, val loss: 2.185846624959099\n",
      "eval model forward diff: 2.594478409889689e-09\n",
      "eval model running mean diff: 4.099893757825157e-10\n",
      "eval model running var diff: 2.533298015805485e-09\n",
      "train model forward diff: 1.3573475676764701e-09\n",
      "\n",
      "model_t step: 41000, train loss: 1.8525829131949343, val loss: 2.1768750324334745\n",
      "model   step: 41000, train loss: 1.8525829131692935, val loss: 2.1768750324358868\n",
      "eval model forward diff: 2.923783881669806e-09\n",
      "eval model running mean diff: 5.752092135935527e-10\n",
      "eval model running var diff: 1.8838335336113232e-09\n",
      "train model forward diff: 1.4394095915193361e-09\n",
      "\n",
      "model_t step: 42000, train loss: 2.4346998995127325, val loss: 2.175861719531774\n",
      "model   step: 42000, train loss: 2.434699899511897, val loss: 2.175861719531811\n",
      "eval model forward diff: 2.8322912903888664e-09\n",
      "eval model running mean diff: 7.002642909981205e-10\n",
      "eval model running var diff: 2.7635849164653337e-09\n",
      "train model forward diff: 1.485719547389408e-09\n",
      "\n",
      "model_t step: 43000, train loss: 2.0122953466119005, val loss: 2.1806644133079884\n",
      "model   step: 43000, train loss: 2.0122953466278974, val loss: 2.180664413302044\n",
      "eval model forward diff: 2.3201778276416007e-09\n",
      "eval model running mean diff: 6.791531781402682e-10\n",
      "eval model running var diff: 3.2914613257162273e-09\n",
      "train model forward diff: 1.3454259928380452e-09\n",
      "\n",
      "model_t step: 44000, train loss: 1.950157663375461, val loss: 2.180910857275662\n",
      "model   step: 44000, train loss: 1.9501576633734397, val loss: 2.1809108572687235\n",
      "eval model forward diff: 3.039661633508217e-09\n",
      "eval model running mean diff: 6.916249795096974e-10\n",
      "eval model running var diff: 2.892512895869004e-09\n",
      "train model forward diff: 2.1633950186839e-09\n",
      "\n",
      "model_t step: 45000, train loss: 2.224024896616842, val loss: 2.196618983050501\n",
      "model   step: 45000, train loss: 2.2240248966391265, val loss: 2.196618983046893\n",
      "eval model forward diff: 3.1596734118011227e-09\n",
      "eval model running mean diff: 7.308065264055585e-10\n",
      "eval model running var diff: 3.734356823770213e-09\n",
      "train model forward diff: 1.704877128361204e-09\n",
      "\n",
      "model_t step: 46000, train loss: 2.218653604403991, val loss: 2.1877910006270453\n",
      "model   step: 46000, train loss: 2.218653604460326, val loss: 2.1877910006246597\n",
      "eval model forward diff: 3.784002444717771e-09\n",
      "eval model running mean diff: 7.274464364215305e-10\n",
      "eval model running var diff: 5.3204658456706966e-09\n",
      "train model forward diff: 2.1155157625685206e-09\n",
      "\n",
      "model_t step: 47000, train loss: 2.1096014462785835, val loss: 2.1791860969220123\n",
      "model   step: 47000, train loss: 2.10960144627151, val loss: 2.179186096922754\n",
      "eval model forward diff: 4.56892035316514e-09\n",
      "eval model running mean diff: 8.785585592363532e-10\n",
      "eval model running var diff: 4.9353303666066495e-09\n",
      "train model forward diff: 2.2868467119963043e-09\n",
      "\n",
      "model_t step: 48000, train loss: 1.8586285640801397, val loss: 2.185051009769694\n",
      "model   step: 48000, train loss: 1.8586285640928497, val loss: 2.1850510097674527\n",
      "eval model forward diff: 6.146361419823165e-09\n",
      "eval model running mean diff: 1.0496354896361026e-09\n",
      "eval model running var diff: 5.199119357257587e-09\n",
      "train model forward diff: 2.3809323401735583e-09\n",
      "\n",
      "model_t step: 49000, train loss: 2.3717391382934236, val loss: 2.190345821392434\n",
      "model   step: 49000, train loss: 2.371739138325175, val loss: 2.190345821378865\n",
      "eval model forward diff: 4.803720088375485e-09\n",
      "eval model running mean diff: 9.307313808548656e-10\n",
      "eval model running var diff: 5.566668903611571e-09\n",
      "train model forward diff: 3.4572966711721165e-09\n",
      "\n",
      "model_t step: 50000, train loss: 2.0252464850292933, val loss: 2.170509491451725\n",
      "model   step: 50000, train loss: 2.025246485057888, val loss: 2.1705094914513507\n",
      "eval model forward diff: 5.494543486861403e-09\n",
      "eval model running mean diff: 1.2392851189702014e-09\n",
      "eval model running var diff: 7.043666983008734e-09\n",
      "train model forward diff: 3.083844291040805e-09\n",
      "\n",
      "model_t step: 51000, train loss: 2.2222584374538172, val loss: 2.1798014246046735\n",
      "model   step: 51000, train loss: 2.222258437312952, val loss: 2.1798014246098534\n",
      "eval model forward diff: 6.554404019532001e-09\n",
      "eval model running mean diff: 1.2725597242635445e-09\n",
      "eval model running var diff: 9.442189252695243e-09\n",
      "train model forward diff: 2.519629616060115e-09\n",
      "\n",
      "model_t step: 52000, train loss: 2.1097848987618315, val loss: 2.1739237852162097\n",
      "model   step: 52000, train loss: 2.109784898817176, val loss: 2.173923785220703\n",
      "eval model forward diff: 5.720515727780651e-09\n",
      "eval model running mean diff: 1.288310347291599e-09\n",
      "eval model running var diff: 9.672859846432402e-09\n",
      "train model forward diff: 3.486377408989938e-09\n",
      "\n",
      "model_t step: 53000, train loss: 2.0435276266113815, val loss: 2.1736914591810477\n",
      "model   step: 53000, train loss: 2.043527626759561, val loss: 2.1736914591796874\n",
      "eval model forward diff: 8.3308893117362e-09\n",
      "eval model running mean diff: 1.7065224233725473e-09\n",
      "eval model running var diff: 1.3032348533670302e-08\n",
      "train model forward diff: 4.8073289793393315e-09\n",
      "\n",
      "model_t step: 54000, train loss: 1.9348264611882284, val loss: 2.172023841086983\n",
      "model   step: 54000, train loss: 1.9348264611775772, val loss: 2.172023841086398\n",
      "eval model forward diff: 8.664037265404545e-09\n",
      "eval model running mean diff: 1.5897949623422392e-09\n",
      "eval model running var diff: 1.1714504921656044e-08\n",
      "train model forward diff: 3.0587932187131628e-09\n",
      "\n",
      "model_t step: 55000, train loss: 1.8263848860309517, val loss: 2.162804446355811\n",
      "model   step: 55000, train loss: 1.8263848860505187, val loss: 2.162804446350605\n",
      "eval model forward diff: 8.014352514962297e-09\n",
      "eval model running mean diff: 1.464989907162817e-09\n",
      "eval model running var diff: 9.779832055301085e-09\n",
      "train model forward diff: 4.555506638581619e-09\n",
      "\n",
      "model_t step: 56000, train loss: 2.197494376760299, val loss: 2.1748387081117264\n",
      "model   step: 56000, train loss: 2.197494376736782, val loss: 2.1748387081166043\n",
      "eval model forward diff: 1.1280486100773146e-08\n",
      "eval model running mean diff: 1.8348460806283384e-09\n",
      "eval model running var diff: 1.2539125293642428e-08\n",
      "train model forward diff: 1.2084973910475583e-08\n",
      "\n",
      "model_t step: 57000, train loss: 1.7819932091142916, val loss: 2.1767117143600982\n",
      "model   step: 57000, train loss: 1.7819932090674804, val loss: 2.1767117143740644\n",
      "eval model forward diff: 1.218674894332139e-08\n",
      "eval model running mean diff: 2.3199060450451725e-09\n",
      "eval model running var diff: 1.4673567250156339e-08\n",
      "train model forward diff: 5.650709677951227e-09\n",
      "\n",
      "model_t step: 58000, train loss: 2.6284680138608985, val loss: 2.1632011063045513\n",
      "model   step: 58000, train loss: 2.6284680137052843, val loss: 2.1632011063072145\n",
      "eval model forward diff: 9.761060404400723e-09\n",
      "eval model running mean diff: 3.594729403033625e-09\n",
      "eval model running var diff: 1.7093711335292028e-08\n",
      "train model forward diff: 6.583998068965258e-09\n",
      "\n",
      "model_t step: 59000, train loss: 1.8210972297150743, val loss: 2.183127566806728\n",
      "model   step: 59000, train loss: 1.8210972297133785, val loss: 2.183127566817345\n",
      "eval model forward diff: 1.3421812994351967e-08\n",
      "eval model running mean diff: 3.2439655406335532e-09\n",
      "eval model running var diff: 1.6312469597323798e-08\n",
      "train model forward diff: 7.284836733845168e-09\n",
      "\n",
      "model_t step: 60000, train loss: 1.924288775177839, val loss: 2.173253651958867\n",
      "model   step: 60000, train loss: 1.9242887751752213, val loss: 2.1732536519534986\n",
      "eval model forward diff: 1.1212181405539923e-08\n",
      "eval model running mean diff: 3.805995796302142e-09\n",
      "eval model running var diff: 1.9672683038152172e-08\n",
      "train model forward diff: 8.905628678945732e-09\n",
      "\n",
      "model_t step: 61000, train loss: 2.0491148122358585, val loss: 2.1678626366810927\n",
      "model   step: 61000, train loss: 2.049114812443141, val loss: 2.1678626366724547\n",
      "eval model forward diff: 1.6123638424403453e-08\n",
      "eval model running mean diff: 3.756120317555123e-09\n",
      "eval model running var diff: 2.9396986178653606e-08\n",
      "train model forward diff: 6.683614106606228e-09\n",
      "\n",
      "model_t step: 62000, train loss: 2.0959720199992207, val loss: 2.173876226243672\n",
      "model   step: 62000, train loss: 2.0959720200061853, val loss: 2.173876226245071\n",
      "eval model forward diff: 1.357185075434586e-08\n",
      "eval model running mean diff: 3.657377962984487e-09\n",
      "eval model running var diff: 4.041096701712377e-08\n",
      "train model forward diff: 6.906602401102191e-09\n",
      "\n",
      "model_t step: 63000, train loss: 1.9165762372866872, val loss: 2.1625015366423335\n",
      "model   step: 63000, train loss: 1.916576237342464, val loss: 2.1625015366591054\n",
      "eval model forward diff: 1.5681282494739435e-08\n",
      "eval model running mean diff: 3.727762987182359e-09\n",
      "eval model running var diff: 3.7570899280581216e-08\n",
      "train model forward diff: 1.0351677293130024e-08\n",
      "\n",
      "model_t step: 64000, train loss: 1.9647501060971133, val loss: 2.166201987092958\n",
      "model   step: 64000, train loss: 1.9647501065437478, val loss: 2.1662019871042393\n",
      "eval model forward diff: 1.5018034815739156e-08\n",
      "eval model running mean diff: 4.409282716721918e-09\n",
      "eval model running var diff: 4.0920582478065626e-08\n",
      "train model forward diff: 1.0409492823271194e-08\n",
      "\n",
      "model_t step: 65000, train loss: 2.1853314148272913, val loss: 2.162261034586981\n",
      "model   step: 65000, train loss: 2.185331414750783, val loss: 2.1622610345697293\n",
      "eval model forward diff: 1.8144628244343153e-08\n",
      "eval model running mean diff: 4.325526603565777e-09\n",
      "eval model running var diff: 5.7159596167366544e-08\n",
      "train model forward diff: 1.2256591519488325e-08\n",
      "\n",
      "model_t step: 66000, train loss: 2.570481830928141, val loss: 2.164995881903023\n",
      "model   step: 66000, train loss: 2.570481830756789, val loss: 2.1649958819046957\n",
      "eval model forward diff: 2.227578832503241e-08\n",
      "eval model running mean diff: 5.2595736654836855e-09\n",
      "eval model running var diff: 6.387054440892825e-08\n",
      "train model forward diff: 1.12154524278818e-08\n",
      "\n",
      "model_t step: 67000, train loss: 2.103390045730018, val loss: 2.1673158764080123\n",
      "model   step: 67000, train loss: 2.1033900453476386, val loss: 2.167315876365347\n",
      "eval model forward diff: 2.671539345300289e-08\n",
      "eval model running mean diff: 5.399200864175668e-09\n",
      "eval model running var diff: 6.11977100106742e-08\n",
      "train model forward diff: 1.6680631986787375e-08\n",
      "\n",
      "model_t step: 68000, train loss: 2.3790732577922835, val loss: 2.168575505904953\n",
      "model   step: 68000, train loss: 2.379073258063956, val loss: 2.1685755059512046\n",
      "eval model forward diff: 1.9751365432085777e-08\n",
      "eval model running mean diff: 6.918601469507735e-09\n",
      "eval model running var diff: 6.513859318602044e-08\n",
      "train model forward diff: 2.077339722461602e-08\n",
      "\n",
      "model_t step: 69000, train loss: 1.6115403180536743, val loss: 2.1635579424291347\n",
      "model   step: 69000, train loss: 1.6115403182223498, val loss: 2.163557942426626\n",
      "eval model forward diff: 3.557705796630728e-08\n",
      "eval model running mean diff: 6.777945538161134e-09\n",
      "eval model running var diff: 5.9480044001247734e-08\n",
      "train model forward diff: 1.425289397616325e-08\n",
      "\n",
      "model_t step: 70000, train loss: 2.1748810611835756, val loss: 2.1660933313955026\n",
      "model   step: 70000, train loss: 2.1748810608071576, val loss: 2.1660933314115884\n",
      "eval model forward diff: 2.7203912678075426e-08\n",
      "eval model running mean diff: 7.729991646776568e-09\n",
      "eval model running var diff: 5.7279990528513736e-08\n",
      "train model forward diff: 1.8427826820044402e-08\n",
      "\n",
      "model_t step: 71000, train loss: 2.2345597074160377, val loss: 2.156009719538656\n",
      "model   step: 71000, train loss: 2.2345597065119667, val loss: 2.1560097195339294\n",
      "eval model forward diff: 3.396114833797981e-08\n",
      "eval model running mean diff: 7.836558624241263e-09\n",
      "eval model running var diff: 6.331960378247459e-08\n",
      "train model forward diff: 2.679124744275896e-08\n",
      "\n",
      "model_t step: 72000, train loss: 2.2724802659409074, val loss: 2.1595693525603807\n",
      "model   step: 72000, train loss: 2.2724802659480803, val loss: 2.159569352574266\n",
      "eval model forward diff: 3.698227080128902e-08\n",
      "eval model running mean diff: 1.0978535858185978e-08\n",
      "eval model running var diff: 6.591925227894535e-08\n",
      "train model forward diff: 2.1004050498873994e-08\n",
      "\n",
      "model_t step: 73000, train loss: 2.288333421267808, val loss: 2.1539661811559965\n",
      "model   step: 73000, train loss: 2.2883334213655013, val loss: 2.1539661811147357\n",
      "eval model forward diff: 4.182882440417757e-08\n",
      "eval model running mean diff: 1.2165497054184016e-08\n",
      "eval model running var diff: 5.803426006423251e-08\n",
      "train model forward diff: 2.808961774292129e-08\n",
      "\n",
      "model_t step: 74000, train loss: 2.273589851084669, val loss: 2.149875584084783\n",
      "model   step: 74000, train loss: 2.2735898512280457, val loss: 2.1498755840799495\n",
      "eval model forward diff: 3.853761931793542e-08\n",
      "eval model running mean diff: 1.1466153360117914e-08\n",
      "eval model running var diff: 7.07457701309977e-08\n",
      "train model forward diff: 2.0221643026729907e-08\n",
      "\n",
      "model_t step: 75000, train loss: 1.7540302066454192, val loss: 2.1513327189855826\n",
      "model   step: 75000, train loss: 1.7540302062678528, val loss: 2.1513327189539275\n",
      "eval model forward diff: 3.894103528523374e-08\n",
      "eval model running mean diff: 9.311396098610203e-09\n",
      "eval model running var diff: 7.449284566973802e-08\n",
      "train model forward diff: 2.414156208274676e-08\n",
      "\n",
      "model_t step: 76000, train loss: 1.8266632796800364, val loss: 2.1606099289439036\n",
      "model   step: 76000, train loss: 1.826663280076588, val loss: 2.160609928928829\n",
      "eval model forward diff: 4.451161128571357e-08\n",
      "eval model running mean diff: 1.1350846484958765e-08\n",
      "eval model running var diff: 8.850156518747099e-08\n",
      "train model forward diff: 3.154710470631983e-08\n",
      "\n",
      "model_t step: 77000, train loss: 2.1380101983057203, val loss: 2.1572564346394727\n",
      "model   step: 77000, train loss: 2.1380101986826148, val loss: 2.157256434688812\n",
      "eval model forward diff: 4.191555813548575e-08\n",
      "eval model running mean diff: 1.3357075001607654e-08\n",
      "eval model running var diff: 9.822315405472182e-08\n",
      "train model forward diff: 2.8213652747410833e-08\n",
      "\n",
      "model_t step: 78000, train loss: 2.009144708917104, val loss: 2.159813588098532\n",
      "model   step: 78000, train loss: 2.009144709964763, val loss: 2.1598135881919496\n",
      "eval model forward diff: 4.900528027818041e-08\n",
      "eval model running mean diff: 1.4471502662871671e-08\n",
      "eval model running var diff: 8.331626588642393e-08\n",
      "train model forward diff: 2.947291616317571e-08\n",
      "\n",
      "model_t step: 79000, train loss: 2.203019905658502, val loss: 2.162501497939105\n",
      "model   step: 79000, train loss: 2.203019906182393, val loss: 2.1625014979513417\n",
      "eval model forward diff: 5.078574938366387e-08\n",
      "eval model running mean diff: 1.5388441187980106e-08\n",
      "eval model running var diff: 1.1851473402657575e-07\n",
      "train model forward diff: 3.2564255292300004e-08\n",
      "\n",
      "model_t step: 80000, train loss: 1.7964265704549915, val loss: 2.1713701677908768\n",
      "model   step: 80000, train loss: 1.796426570936363, val loss: 2.1713701677602644\n",
      "eval model forward diff: 6.166420529751804e-08\n",
      "eval model running mean diff: 1.6464977381502877e-08\n",
      "eval model running var diff: 8.797189821052598e-08\n",
      "train model forward diff: 3.7271398745097883e-08\n",
      "\n",
      "model_t step: 81000, train loss: 1.715358322627594, val loss: 2.1515330034551026\n",
      "model   step: 81000, train loss: 1.7153583238305505, val loss: 2.15153300349106\n",
      "eval model forward diff: 5.855294693546398e-08\n",
      "eval model running mean diff: 2.1087094292937536e-08\n",
      "eval model running var diff: 1.0004444561673154e-07\n",
      "train model forward diff: 3.017126792848046e-08\n",
      "\n",
      "model_t step: 82000, train loss: 1.8612582889438871, val loss: 2.1486326480390545\n",
      "model   step: 82000, train loss: 1.8612582873048646, val loss: 2.148632648006136\n",
      "eval model forward diff: 5.7653939178337055e-08\n",
      "eval model running mean diff: 2.5475087372939242e-08\n",
      "eval model running var diff: 2.2330402771331137e-07\n",
      "train model forward diff: 4.706764888950943e-08\n",
      "\n",
      "model_t step: 83000, train loss: 1.9565217778040724, val loss: 2.159153392819208\n",
      "model   step: 83000, train loss: 1.956521778303188, val loss: 2.159153392776107\n",
      "eval model forward diff: 7.507579091736716e-08\n",
      "eval model running mean diff: 2.964472711752819e-08\n",
      "eval model running var diff: 1.3828261558046506e-07\n",
      "train model forward diff: 4.5461458153539525e-08\n",
      "\n",
      "model_t step: 84000, train loss: 1.693878387726122, val loss: 2.147921739052276\n",
      "model   step: 84000, train loss: 1.693878388779014, val loss: 2.1479217390004326\n",
      "eval model forward diff: 6.925563145188107e-08\n",
      "eval model running mean diff: 3.5830350686438805e-08\n",
      "eval model running var diff: 1.5019290344753244e-07\n",
      "train model forward diff: 3.745006160738029e-08\n",
      "\n",
      "model_t step: 85000, train loss: 2.0368436019192604, val loss: 2.170827844819496\n",
      "model   step: 85000, train loss: 2.0368436021870338, val loss: 2.170827844781789\n",
      "eval model forward diff: 6.693098564269917e-08\n",
      "eval model running mean diff: 3.869623288466073e-08\n",
      "eval model running var diff: 1.6048321072048566e-07\n",
      "train model forward diff: 3.703700524049225e-08\n",
      "\n",
      "model_t step: 86000, train loss: 2.494328748499383, val loss: 2.1553851679970033\n",
      "model   step: 86000, train loss: 2.494328749380933, val loss: 2.1553851679836824\n",
      "eval model forward diff: 1.0584961618320676e-07\n",
      "eval model running mean diff: 3.424347783109738e-08\n",
      "eval model running var diff: 1.8381227562258573e-07\n",
      "train model forward diff: 4.312503776304766e-08\n",
      "\n",
      "model_t step: 87000, train loss: 2.2117581281645884, val loss: 2.155326686857121\n",
      "model   step: 87000, train loss: 2.211758128736853, val loss: 2.155326686884938\n",
      "eval model forward diff: 7.919127109623503e-08\n",
      "eval model running mean diff: 3.6481358378637196e-08\n",
      "eval model running var diff: 2.2944144006942224e-07\n",
      "train model forward diff: 4.019231081286989e-08\n",
      "\n",
      "model_t step: 88000, train loss: 2.0384058559847267, val loss: 2.158409170037266\n",
      "model   step: 88000, train loss: 2.0384058577633337, val loss: 2.158409169978748\n",
      "eval model forward diff: 8.533040674052472e-08\n",
      "eval model running mean diff: 3.366752210354207e-08\n",
      "eval model running var diff: 2.362465636451816e-07\n",
      "train model forward diff: 5.291507676474794e-08\n",
      "\n",
      "model_t step: 89000, train loss: 1.7997961280766763, val loss: 2.1608705565086495\n",
      "model   step: 89000, train loss: 1.7997961293658866, val loss: 2.160870556492973\n",
      "eval model forward diff: 1.1255145082600393e-07\n",
      "eval model running mean diff: 4.213470639058414e-08\n",
      "eval model running var diff: 3.2597370136500103e-07\n",
      "train model forward diff: 8.642738769104596e-08\n",
      "\n",
      "model_t step: 90000, train loss: 2.140614505773217, val loss: 2.1564596371936293\n",
      "model   step: 90000, train loss: 2.140614502805624, val loss: 2.156459637248321\n",
      "eval model forward diff: 1.0510583248191097e-07\n",
      "eval model running mean diff: 3.906243684070887e-08\n",
      "eval model running var diff: 3.5076809012934973e-07\n",
      "train model forward diff: 7.057660944553845e-08\n",
      "\n",
      "model_t step: 91000, train loss: 2.1103391791946002, val loss: 2.1521690868636623\n",
      "model   step: 91000, train loss: 2.1103391788116115, val loss: 2.1521690868721035\n",
      "eval model forward diff: 1.1977266733964598e-07\n",
      "eval model running mean diff: 3.7504321426240494e-08\n",
      "eval model running var diff: 3.873402931731107e-07\n",
      "train model forward diff: 6.091272775066159e-08\n",
      "\n",
      "model_t step: 92000, train loss: 1.78746434041962, val loss: 2.1495259081836298\n",
      "model   step: 92000, train loss: 1.7874643429230361, val loss: 2.149525908109007\n",
      "eval model forward diff: 1.782797331983943e-07\n",
      "eval model running mean diff: 3.8018823755869846e-08\n",
      "eval model running var diff: 3.869855618177098e-07\n",
      "train model forward diff: 6.252146356189314e-08\n",
      "\n",
      "model_t step: 93000, train loss: 2.4646032871314216, val loss: 2.14871901465224\n",
      "model   step: 93000, train loss: 2.4646032875638606, val loss: 2.1487190145650508\n",
      "eval model forward diff: 1.511839360368583e-07\n",
      "eval model running mean diff: 4.11843776859655e-08\n",
      "eval model running var diff: 5.325094321051438e-07\n",
      "train model forward diff: 6.396279883524869e-08\n",
      "\n",
      "model_t step: 94000, train loss: 1.6934662538834733, val loss: 2.152332590532504\n",
      "model   step: 94000, train loss: 1.6934662544638472, val loss: 2.152332590610222\n",
      "eval model forward diff: 1.2259752457666195e-07\n",
      "eval model running mean diff: 4.101231465547528e-08\n",
      "eval model running var diff: 6.494194906281336e-07\n",
      "train model forward diff: 7.912399357934419e-08\n",
      "\n",
      "model_t step: 95000, train loss: 3.130099879646322, val loss: 2.1559197186057277\n",
      "model   step: 95000, train loss: 3.1300998834472216, val loss: 2.155919718613222\n",
      "eval model forward diff: 1.8620004427205572e-07\n",
      "eval model running mean diff: 4.434806299968841e-08\n",
      "eval model running var diff: 6.337800471101218e-07\n",
      "train model forward diff: 1.0871851907623409e-07\n",
      "\n",
      "model_t step: 96000, train loss: 2.3581395589441425, val loss: 2.1508086769768116\n",
      "model   step: 96000, train loss: 2.358139558831435, val loss: 2.150808676949869\n",
      "eval model forward diff: 1.6449829320919207e-07\n",
      "eval model running mean diff: 5.04636281650761e-08\n",
      "eval model running var diff: 7.179571639426285e-07\n",
      "train model forward diff: 7.230842591354758e-08\n",
      "\n",
      "model_t step: 97000, train loss: 1.7722813172175034, val loss: 2.150632983087584\n",
      "model   step: 97000, train loss: 1.7722813163322717, val loss: 2.150632983152155\n",
      "eval model forward diff: 1.897730363964456e-07\n",
      "eval model running mean diff: 4.761514471596229e-08\n",
      "eval model running var diff: 6.688732980819623e-07\n",
      "train model forward diff: 8.77082018124753e-08\n",
      "\n",
      "model_t step: 98000, train loss: 2.2208790827133753, val loss: 2.144535415029644\n",
      "model   step: 98000, train loss: 2.2208790823958036, val loss: 2.144535414999141\n",
      "eval model forward diff: 1.4931028191611517e-07\n",
      "eval model running mean diff: 4.916956530820471e-08\n",
      "eval model running var diff: 5.194844732159254e-07\n",
      "train model forward diff: 6.979086597524997e-08\n",
      "\n",
      "model_t step: 99000, train loss: 2.4184192523941825, val loss: 2.15083718645925\n",
      "model   step: 99000, train loss: 2.4184192529002333, val loss: 2.150837186552178\n",
      "eval model forward diff: 1.899271770966493e-07\n",
      "eval model running mean diff: 4.5130914116953136e-08\n",
      "eval model running var diff: 4.774549040575948e-07\n",
      "train model forward diff: 8.165728326847699e-08\n",
      "\n",
      "model_t step: 100000, train loss: 2.0053654791008295, val loss: 2.151275949311942\n",
      "model   step: 100000, train loss: 2.0053654794959703, val loss: 2.1512759493421587\n",
      "eval model forward diff: 1.735178831729911e-07\n",
      "eval model running mean diff: 5.4846341690506506e-08\n",
      "eval model running var diff: 4.782934013292106e-07\n",
      "train model forward diff: 9.887718910306376e-08\n",
      "\n",
      "model_t step: 101000, train loss: 1.9497269125899024, val loss: 2.1521758029113496\n",
      "model   step: 101000, train loss: 1.9497269090887726, val loss: 2.1521758028681197\n",
      "eval model forward diff: 1.9066097278752636e-07\n",
      "eval model running mean diff: 5.039590877942146e-08\n",
      "eval model running var diff: 7.674842947835714e-07\n",
      "train model forward diff: 9.85151773491566e-08\n",
      "\n",
      "model_t step: 102000, train loss: 2.436876363969325, val loss: 2.1475308929110595\n",
      "model   step: 102000, train loss: 2.436876363845563, val loss: 2.14753089290092\n",
      "eval model forward diff: 1.9890321212301387e-07\n",
      "eval model running mean diff: 6.225232862533403e-08\n",
      "eval model running var diff: 6.692463614399458e-07\n",
      "train model forward diff: 1.0256673510511405e-07\n",
      "\n",
      "model_t step: 103000, train loss: 1.9017659802522315, val loss: 2.1522050548626086\n",
      "model   step: 103000, train loss: 1.9017659818497121, val loss: 2.152205055058966\n",
      "eval model forward diff: 2.7181619577376637e-07\n",
      "eval model running mean diff: 7.758036080218744e-08\n",
      "eval model running var diff: 7.293076578207547e-07\n",
      "train model forward diff: 1.24649241595165e-07\n",
      "\n",
      "model_t step: 104000, train loss: 1.7168739435650986, val loss: 2.1431794389173757\n",
      "model   step: 104000, train loss: 1.716873943323089, val loss: 2.1431794388496286\n",
      "eval model forward diff: 1.8712185090308253e-07\n",
      "eval model running mean diff: 8.227949521710798e-08\n",
      "eval model running var diff: 8.237237238972739e-07\n",
      "train model forward diff: 1.1008847788929188e-07\n",
      "\n",
      "model_t step: 105000, train loss: 2.047347988566497, val loss: 2.153649469736492\n",
      "model   step: 105000, train loss: 2.0473479838525663, val loss: 2.1536494696302046\n",
      "eval model forward diff: 2.2846686587030263e-07\n",
      "eval model running mean diff: 8.191223344056198e-08\n",
      "eval model running var diff: 7.32848604911851e-07\n",
      "train model forward diff: 1.0584236198596386e-07\n",
      "\n",
      "model_t step: 106000, train loss: 2.2539001369536282, val loss: 2.1571554675340296\n",
      "model   step: 106000, train loss: 2.253900136433065, val loss: 2.1571554674954485\n",
      "eval model forward diff: 2.6751007187186815e-07\n",
      "eval model running mean diff: 7.707994331696e-08\n",
      "eval model running var diff: 6.736267721407785e-07\n",
      "train model forward diff: 1.597693064514516e-07\n",
      "\n",
      "model_t step: 107000, train loss: 2.2806697479886733, val loss: 2.1557101749174192\n",
      "model   step: 107000, train loss: 2.28066975270568, val loss: 2.155710174901404\n",
      "eval model forward diff: 2.540462846756242e-07\n",
      "eval model running mean diff: 8.295526732204905e-08\n",
      "eval model running var diff: 7.962650840909191e-07\n",
      "train model forward diff: 1.4283361826628038e-07\n",
      "\n",
      "model_t step: 108000, train loss: 1.7959117931098991, val loss: 2.154078062223327\n",
      "model   step: 108000, train loss: 1.7959117943492167, val loss: 2.1540780619362425\n",
      "eval model forward diff: 2.5536569303596934e-07\n",
      "eval model running mean diff: 8.40936316137686e-08\n",
      "eval model running var diff: 7.920362747881882e-07\n",
      "train model forward diff: 1.483492040321721e-07\n",
      "\n",
      "model_t step: 109000, train loss: 2.188312714419977, val loss: 2.1464594630483615\n",
      "model   step: 109000, train loss: 2.188312719146326, val loss: 2.146459463089762\n",
      "eval model forward diff: 3.278775402293377e-07\n",
      "eval model running mean diff: 9.425904545778963e-08\n",
      "eval model running var diff: 6.773155689643318e-07\n",
      "train model forward diff: 2.551671695050217e-07\n",
      "\n",
      "model_t step: 110000, train loss: 2.053974282987252, val loss: 2.1529440392388994\n",
      "model   step: 110000, train loss: 2.0539742808579806, val loss: 2.152944038977366\n",
      "eval model forward diff: 2.685022491011324e-07\n",
      "eval model running mean diff: 1.1247399878122621e-07\n",
      "eval model running var diff: 7.626519220593764e-07\n",
      "train model forward diff: 2.3039333463259482e-07\n",
      "\n",
      "model_t step: 111000, train loss: 2.169039182319354, val loss: 2.144416987986473\n",
      "model   step: 111000, train loss: 2.169039180704988, val loss: 2.1444169878043216\n",
      "eval model forward diff: 3.2545830008778864e-07\n",
      "eval model running mean diff: 1.3154305222862206e-07\n",
      "eval model running var diff: 1.156189057383017e-06\n",
      "train model forward diff: 2.029227728073124e-07\n",
      "\n",
      "model_t step: 112000, train loss: 1.8629824711647693, val loss: 2.159050130600287\n",
      "model   step: 112000, train loss: 1.8629824733717508, val loss: 2.159050130346497\n",
      "eval model forward diff: 2.959871032715e-07\n",
      "eval model running mean diff: 1.2713189079249787e-07\n",
      "eval model running var diff: 8.805504592146463e-07\n",
      "train model forward diff: 1.7411520225607546e-07\n",
      "\n",
      "model_t step: 113000, train loss: 2.2586290281934307, val loss: 2.1471366271888512\n",
      "model   step: 113000, train loss: 2.2586290299795087, val loss: 2.1471366270527708\n",
      "eval model forward diff: 3.540883861674615e-07\n",
      "eval model running mean diff: 1.469859709501975e-07\n",
      "eval model running var diff: 9.766133644006914e-07\n",
      "train model forward diff: 1.9561222952546586e-07\n",
      "\n",
      "model_t step: 114000, train loss: 1.9397357736320484, val loss: 2.1612171052926854\n",
      "model   step: 114000, train loss: 1.939735776385684, val loss: 2.16121710515865\n",
      "eval model forward diff: 5.311144573028059e-07\n",
      "eval model running mean diff: 1.7942598340425775e-07\n",
      "eval model running var diff: 8.925886731958599e-07\n",
      "train model forward diff: 1.758957606234901e-07\n",
      "\n",
      "model_t step: 115000, train loss: 2.241600400025931, val loss: 2.1474421495609333\n",
      "model   step: 115000, train loss: 2.2416004011917865, val loss: 2.147442149454856\n",
      "eval model forward diff: 3.9390771533476254e-07\n",
      "eval model running mean diff: 1.31582543971831e-07\n",
      "eval model running var diff: 1.1875684435835865e-06\n",
      "train model forward diff: 1.761316172910199e-07\n",
      "\n",
      "model_t step: 116000, train loss: 2.0063859431337714, val loss: 2.1548467911264937\n",
      "model   step: 116000, train loss: 2.006385943672531, val loss: 2.1548467913317224\n",
      "eval model forward diff: 3.54586604967011e-07\n",
      "eval model running mean diff: 1.2964297546602666e-07\n",
      "eval model running var diff: 1.085675421563792e-06\n",
      "train model forward diff: 3.152854790577919e-07\n",
      "\n",
      "model_t step: 117000, train loss: 2.346790821303452, val loss: 2.1472352134516433\n",
      "model   step: 117000, train loss: 2.346790809104447, val loss: 2.1472352135545094\n",
      "eval model forward diff: 4.3255060777624976e-07\n",
      "eval model running mean diff: 1.2673232951954105e-07\n",
      "eval model running var diff: 9.218618970407988e-07\n",
      "train model forward diff: 2.91139925501227e-07\n",
      "\n",
      "model_t step: 118000, train loss: 1.8603568514517137, val loss: 2.1474986060648678\n",
      "model   step: 118000, train loss: 1.8603568463591733, val loss: 2.1474986064612076\n",
      "eval model forward diff: 4.0215274110799726e-07\n",
      "eval model running mean diff: 1.5611565995499177e-07\n",
      "eval model running var diff: 8.891542506717087e-07\n",
      "train model forward diff: 3.0481776525093096e-07\n",
      "\n",
      "model_t step: 119000, train loss: 2.1035023573827285, val loss: 2.14474014891597\n",
      "model   step: 119000, train loss: 2.103502352612814, val loss: 2.144740149203113\n",
      "eval model forward diff: 5.476817794836109e-07\n",
      "eval model running mean diff: 1.5503779060566103e-07\n",
      "eval model running var diff: 1.484865038037242e-06\n",
      "train model forward diff: 1.8485487895958386e-07\n",
      "\n",
      "model_t step: 120000, train loss: 1.9613456776794267, val loss: 2.145651905245662\n",
      "model   step: 120000, train loss: 1.961345679559639, val loss: 2.1456519055360643\n",
      "eval model forward diff: 4.6443189871681057e-07\n",
      "eval model running mean diff: 1.8846859717314146e-07\n",
      "eval model running var diff: 1.5961453243562573e-06\n",
      "train model forward diff: 3.9026345266535145e-07\n",
      "\n",
      "model_t step: 121000, train loss: 2.0636387900211055, val loss: 2.143920305463539\n",
      "model   step: 121000, train loss: 2.063638792271647, val loss: 2.1439203061538614\n",
      "eval model forward diff: 4.030950897515595e-07\n",
      "eval model running mean diff: 2.0513210952088912e-07\n",
      "eval model running var diff: 1.5314959966872266e-06\n",
      "train model forward diff: 2.739743436563913e-07\n",
      "\n",
      "model_t step: 122000, train loss: 2.0494032520323513, val loss: 2.139500323874427\n",
      "model   step: 122000, train loss: 2.049403247716501, val loss: 2.139500323800933\n",
      "eval model forward diff: 4.852420403267388e-07\n",
      "eval model running mean diff: 1.9633139736185967e-07\n",
      "eval model running var diff: 2.4191591876387974e-06\n",
      "train model forward diff: 2.622570591803708e-07\n",
      "\n",
      "model_t step: 123000, train loss: 2.305739109443619, val loss: 2.1488030735165324\n",
      "model   step: 123000, train loss: 2.3057391014204223, val loss: 2.148803073380303\n",
      "eval model forward diff: 7.780476316554541e-07\n",
      "eval model running mean diff: 2.348671550578274e-07\n",
      "eval model running var diff: 2.3610641335380933e-06\n",
      "train model forward diff: 2.2181029546430864e-07\n",
      "\n",
      "model_t step: 124000, train loss: 1.8955007181774315, val loss: 2.1515812000750354\n",
      "model   step: 124000, train loss: 1.895500720863499, val loss: 2.1515811997977323\n",
      "eval model forward diff: 4.531624069326057e-07\n",
      "eval model running mean diff: 2.590868506402444e-07\n",
      "eval model running var diff: 2.244515911797862e-06\n",
      "train model forward diff: 3.3302527535639115e-07\n",
      "\n",
      "model_t step: 125000, train loss: 2.15559263479717, val loss: 2.143806705201522\n",
      "model   step: 125000, train loss: 2.155592643280009, val loss: 2.1438067051138754\n",
      "eval model forward diff: 5.25695288189354e-07\n",
      "eval model running mean diff: 2.6191002033826294e-07\n",
      "eval model running var diff: 2.3490489411415183e-06\n",
      "train model forward diff: 2.3565216622500884e-07\n",
      "\n",
      "model_t step: 126000, train loss: 2.562670450536678, val loss: 2.1507299820579875\n",
      "model   step: 126000, train loss: 2.562670464288738, val loss: 2.1507299812361005\n",
      "eval model forward diff: 6.281223265958147e-07\n",
      "eval model running mean diff: 2.9226860975839486e-07\n",
      "eval model running var diff: 2.348402233565139e-06\n",
      "train model forward diff: 3.5986260682463467e-07\n",
      "\n",
      "model_t step: 127000, train loss: 2.2814515061616167, val loss: 2.1414103683277492\n",
      "model   step: 127000, train loss: 2.281451505024777, val loss: 2.1414103682575285\n",
      "eval model forward diff: 6.210722451882589e-07\n",
      "eval model running mean diff: 2.8778982885668825e-07\n",
      "eval model running var diff: 2.3274462250810757e-06\n",
      "train model forward diff: 2.845391857686508e-07\n",
      "\n",
      "model_t step: 128000, train loss: 2.3359832129897367, val loss: 2.147503418770323\n",
      "model   step: 128000, train loss: 2.335983226053173, val loss: 2.147503418933307\n",
      "eval model forward diff: 7.323190689412229e-07\n",
      "eval model running mean diff: 3.312037789582334e-07\n",
      "eval model running var diff: 2.552234917629903e-06\n",
      "train model forward diff: 3.232218022120037e-07\n",
      "\n",
      "model_t step: 129000, train loss: 2.013519600710585, val loss: 2.141351427797162\n",
      "model   step: 129000, train loss: 2.013519599963341, val loss: 2.1413514278980474\n",
      "eval model forward diff: 6.888780097469294e-07\n",
      "eval model running mean diff: 3.3057244852585654e-07\n",
      "eval model running var diff: 2.8471213511238602e-06\n",
      "train model forward diff: 3.98610019658463e-07\n",
      "\n",
      "model_t step: 130000, train loss: 2.2574891005207185, val loss: 2.140113203302346\n",
      "model   step: 130000, train loss: 2.2574891132197603, val loss: 2.1401132028369094\n",
      "eval model forward diff: 6.690604177350679e-07\n",
      "eval model running mean diff: 3.6021786087303553e-07\n",
      "eval model running var diff: 3.0033085067771026e-06\n",
      "train model forward diff: 3.661917631347933e-07\n",
      "\n",
      "model_t step: 131000, train loss: 2.3989479910973297, val loss: 2.1374004871659484\n",
      "model   step: 131000, train loss: 2.3989479830989024, val loss: 2.1374004874343973\n",
      "eval model forward diff: 6.1849331212116e-07\n",
      "eval model running mean diff: 3.3403185373259703e-07\n",
      "eval model running var diff: 3.911763457153938e-06\n",
      "train model forward diff: 4.879836064120013e-07\n",
      "\n",
      "model_t step: 132000, train loss: 2.192391201793652, val loss: 2.1538462104531244\n",
      "model   step: 132000, train loss: 2.1923911956228586, val loss: 2.1538462099252413\n",
      "eval model forward diff: 6.378020602770818e-07\n",
      "eval model running mean diff: 2.862306685003091e-07\n",
      "eval model running var diff: 3.50838382701113e-06\n",
      "train model forward diff: 5.121858528411849e-07\n",
      "\n",
      "model_t step: 133000, train loss: 2.0840522259702214, val loss: 2.142148479356855\n",
      "model   step: 133000, train loss: 2.084052232805645, val loss: 2.1421484795607113\n",
      "eval model forward diff: 8.519433114528852e-07\n",
      "eval model running mean diff: 3.7182818246606075e-07\n",
      "eval model running var diff: 3.6574623720753152e-06\n",
      "train model forward diff: 6.661893090509352e-07\n",
      "\n",
      "model_t step: 134000, train loss: 2.1879766367966518, val loss: 2.141192917356225\n",
      "model   step: 134000, train loss: 2.187976627002537, val loss: 2.141192917336162\n",
      "eval model forward diff: 7.655541827134016e-07\n",
      "eval model running mean diff: 3.582691272541183e-07\n",
      "eval model running var diff: 4.1611905032823415e-06\n",
      "train model forward diff: 7.39137453242833e-07\n",
      "\n",
      "model_t step: 135000, train loss: 2.1594114562953606, val loss: 2.143696819763268\n",
      "model   step: 135000, train loss: 2.1594114468530545, val loss: 2.143696820123522\n",
      "eval model forward diff: 9.214666865098309e-07\n",
      "eval model running mean diff: 3.420395318087799e-07\n",
      "eval model running var diff: 3.9742259332342655e-06\n",
      "train model forward diff: 6.834271051303631e-07\n",
      "\n",
      "model_t step: 136000, train loss: 2.05675627415429, val loss: 2.1432367811859567\n",
      "model   step: 136000, train loss: 2.056756275602547, val loss: 2.1432367811839597\n",
      "eval model forward diff: 7.816022100648468e-07\n",
      "eval model running mean diff: 3.791766509664285e-07\n",
      "eval model running var diff: 3.680747198586687e-06\n",
      "train model forward diff: 8.053228031101867e-07\n",
      "\n",
      "model_t step: 137000, train loss: 2.187904602753398, val loss: 2.1431021362709215\n",
      "model   step: 137000, train loss: 2.1879046157946886, val loss: 2.14310213511236\n",
      "eval model forward diff: 9.899799726653669e-07\n",
      "eval model running mean diff: 3.2850354703128914e-07\n",
      "eval model running var diff: 3.7780298782763566e-06\n",
      "train model forward diff: 1.093602432833407e-06\n",
      "\n",
      "model_t step: 138000, train loss: 2.0451038355813003, val loss: 2.1411790447537387\n",
      "model   step: 138000, train loss: 2.0451038024077386, val loss: 2.141179044548381\n",
      "eval model forward diff: 9.5621811624369e-07\n",
      "eval model running mean diff: 4.079678772583861e-07\n",
      "eval model running var diff: 5.496002103200226e-06\n",
      "train model forward diff: 6.218345012243276e-07\n",
      "\n",
      "model_t step: 139000, train loss: 1.8888592450612798, val loss: 2.14414346674976\n",
      "model   step: 139000, train loss: 1.8888592356024718, val loss: 2.1441434661740515\n",
      "eval model forward diff: 1.1962949679755752e-06\n",
      "eval model running mean diff: 4.6315414703457236e-07\n",
      "eval model running var diff: 5.111020101367103e-06\n",
      "train model forward diff: 6.780814674200997e-07\n",
      "\n",
      "model_t step: 140000, train loss: 1.8608526877968454, val loss: 2.141333390434017\n",
      "model   step: 140000, train loss: 1.8608526859819823, val loss: 2.1413333910112717\n",
      "eval model forward diff: 1.2695794984196596e-06\n",
      "eval model running mean diff: 5.332460304430242e-07\n",
      "eval model running var diff: 4.302039968706595e-06\n",
      "train model forward diff: 5.319106595180756e-07\n",
      "\n",
      "model_t step: 141000, train loss: 1.861081745258061, val loss: 2.1402115042283474\n",
      "model   step: 141000, train loss: 1.8610817376260003, val loss: 2.1402115045844528\n",
      "eval model forward diff: 1.0693732477307094e-06\n",
      "eval model running mean diff: 5.592112923835657e-07\n",
      "eval model running var diff: 4.076986471091004e-06\n",
      "train model forward diff: 5.509989409180349e-07\n",
      "\n",
      "model_t step: 142000, train loss: 2.031657175397061, val loss: 2.139714078720672\n",
      "model   step: 142000, train loss: 2.0316571695959107, val loss: 2.1397140806135524\n",
      "eval model forward diff: 1.1970397804095256e-06\n",
      "eval model running mean diff: 5.409367922482033e-07\n",
      "eval model running var diff: 4.304723461245885e-06\n",
      "train model forward diff: 9.061854172598416e-07\n",
      "\n",
      "model_t step: 143000, train loss: 2.1958570144425393, val loss: 2.1349668198028624\n",
      "model   step: 143000, train loss: 2.195857028779895, val loss: 2.1349668194686156\n",
      "eval model forward diff: 1.2071979349492779e-06\n",
      "eval model running mean diff: 5.523732919954227e-07\n",
      "eval model running var diff: 5.4450119932880625e-06\n",
      "train model forward diff: 6.791806967942904e-07\n",
      "\n",
      "model_t step: 144000, train loss: 2.0200352239329655, val loss: 2.147451628257576\n",
      "model   step: 144000, train loss: 2.020035228327087, val loss: 2.1474516274050273\n",
      "eval model forward diff: 1.4227827849211394e-06\n",
      "eval model running mean diff: 5.855663811615841e-07\n",
      "eval model running var diff: 5.264134756544081e-06\n",
      "train model forward diff: 5.435462950842407e-07\n",
      "\n",
      "model_t step: 145000, train loss: 2.309852591961957, val loss: 2.1392007392270322\n",
      "model   step: 145000, train loss: 2.309852600783833, val loss: 2.1392007384597074\n",
      "eval model forward diff: 1.4751505972299128e-06\n",
      "eval model running mean diff: 5.872519199812132e-07\n",
      "eval model running var diff: 5.665192361448135e-06\n",
      "train model forward diff: 6.062848177945668e-07\n",
      "\n",
      "model_t step: 146000, train loss: 2.2139039523527786, val loss: 2.138681559529398\n",
      "model   step: 146000, train loss: 2.213903960477033, val loss: 2.138681559868203\n",
      "eval model forward diff: 1.5830814703576834e-06\n",
      "eval model running mean diff: 5.478242792733568e-07\n",
      "eval model running var diff: 6.86677873318331e-06\n",
      "train model forward diff: 6.659250733065392e-07\n",
      "\n",
      "model_t step: 147000, train loss: 2.328328658003223, val loss: 2.146764864049556\n",
      "model   step: 147000, train loss: 2.328328675108025, val loss: 2.1467648639271277\n",
      "eval model forward diff: 1.3499123188154272e-06\n",
      "eval model running mean diff: 5.976470998092509e-07\n",
      "eval model running var diff: 8.185979709196545e-06\n",
      "train model forward diff: 8.89742747034461e-07\n",
      "\n",
      "model_t step: 148000, train loss: 2.2216011603562245, val loss: 2.145722913805456\n",
      "model   step: 148000, train loss: 2.2216011689637756, val loss: 2.145722915083713\n",
      "eval model forward diff: 1.8718300802689214e-06\n",
      "eval model running mean diff: 5.486460850079311e-07\n",
      "eval model running var diff: 8.184453974990902e-06\n",
      "train model forward diff: 8.032875093988423e-07\n",
      "\n",
      "model_t step: 149000, train loss: 2.2953437708470465, val loss: 2.1484705264826305\n",
      "model   step: 149000, train loss: 2.2953437803674346, val loss: 2.1484705225251637\n",
      "eval model forward diff: 2.614855187044185e-06\n",
      "eval model running mean diff: 6.41667300183002e-07\n",
      "eval model running var diff: 8.765238192154357e-06\n",
      "train model forward diff: 8.51260061551784e-07\n",
      "\n",
      "model_t step: 150000, train loss: 2.3469028379911574, val loss: 2.142960947225937\n",
      "model   step: 150000, train loss: 2.34690283675856, val loss: 2.142960948266843\n",
      "eval model forward diff: 1.636143088123987e-06\n",
      "eval model running mean diff: 7.428718680202451e-07\n",
      "eval model running var diff: 1.1764834482619335e-05\n",
      "train model forward diff: 1.0366195910549436e-06\n",
      "\n",
      "model_t step: 151000, train loss: 2.3649200038349836, val loss: 2.123058671018559\n",
      "model   step: 151000, train loss: 2.364920024452519, val loss: 2.123058671754686\n",
      "eval model forward diff: 1.3901154840567642e-06\n",
      "eval model running mean diff: 7.701395521308996e-07\n",
      "eval model running var diff: 9.778223017065102e-06\n",
      "train model forward diff: 7.648995459241803e-07\n",
      "\n",
      "model_t step: 152000, train loss: 2.005380206367889, val loss: 2.1186728903333845\n",
      "model   step: 152000, train loss: 2.0053802044220763, val loss: 2.118672890930032\n",
      "eval model forward diff: 1.3303468970882193e-06\n",
      "eval model running mean diff: 8.048453481457329e-07\n",
      "eval model running var diff: 9.2702593121885e-06\n",
      "train model forward diff: 9.517183778839922e-07\n",
      "\n",
      "model_t step: 153000, train loss: 2.138843334414019, val loss: 2.1176835766783824\n",
      "model   step: 153000, train loss: 2.1388433206548014, val loss: 2.117683577359284\n",
      "eval model forward diff: 1.3920526216182338e-06\n",
      "eval model running mean diff: 8.542109197762215e-07\n",
      "eval model running var diff: 8.860557414891446e-06\n",
      "train model forward diff: 7.768616867576128e-07\n",
      "\n",
      "model_t step: 154000, train loss: 2.1247908032549443, val loss: 2.1169666947880184\n",
      "model   step: 154000, train loss: 2.1247908120204095, val loss: 2.116966695610304\n",
      "eval model forward diff: 1.2783377909553195e-06\n",
      "eval model running mean diff: 8.543528848825588e-07\n",
      "eval model running var diff: 8.418246522978734e-06\n",
      "train model forward diff: 7.407165059092335e-07\n",
      "\n",
      "model_t step: 155000, train loss: 2.407516523811134, val loss: 2.1168511984952043\n",
      "model   step: 155000, train loss: 2.4075164937441405, val loss: 2.116851199043484\n",
      "eval model forward diff: 1.2746659803219984e-06\n",
      "eval model running mean diff: 8.523631449541824e-07\n",
      "eval model running var diff: 8.16540054415782e-06\n",
      "train model forward diff: 8.896382133194436e-07\n",
      "\n",
      "model_t step: 156000, train loss: 1.9471414892928238, val loss: 2.115120358818995\n",
      "model   step: 156000, train loss: 1.947141498960478, val loss: 2.115120359529816\n",
      "eval model forward diff: 1.283770911131299e-06\n",
      "eval model running mean diff: 8.33076529982435e-07\n",
      "eval model running var diff: 8.181693260667089e-06\n",
      "train model forward diff: 5.828137936703115e-07\n",
      "\n",
      "model_t step: 157000, train loss: 2.229399623037824, val loss: 2.1151530676979533\n",
      "model   step: 157000, train loss: 2.2293996432813934, val loss: 2.1151530685912956\n",
      "eval model forward diff: 1.1427104531502863e-06\n",
      "eval model running mean diff: 8.145922767432978e-07\n",
      "eval model running var diff: 7.946361790800438e-06\n",
      "train model forward diff: 5.823533629767397e-07\n",
      "\n",
      "model_t step: 158000, train loss: 2.003567585242639, val loss: 2.1145369839856234\n",
      "model   step: 158000, train loss: 2.0035675776631114, val loss: 2.1145369847762914\n",
      "eval model forward diff: 1.1261112824456632e-06\n",
      "eval model running mean diff: 8.09262228340657e-07\n",
      "eval model running var diff: 8.105824576887244e-06\n",
      "train model forward diff: 1.0002308044043673e-06\n",
      "\n",
      "model_t step: 159000, train loss: 2.0047751822167896, val loss: 2.115077419919871\n",
      "model   step: 159000, train loss: 2.004775186295349, val loss: 2.1150774203763785\n",
      "eval model forward diff: 1.117991280885633e-06\n",
      "eval model running mean diff: 7.919469151573821e-07\n",
      "eval model running var diff: 7.617333494636114e-06\n",
      "train model forward diff: 4.666839057421157e-07\n",
      "\n",
      "model_t step: 160000, train loss: 1.8660172107325594, val loss: 2.1130164596713086\n",
      "model   step: 160000, train loss: 1.8660172058902638, val loss: 2.1130164601592067\n",
      "eval model forward diff: 1.1094024973956618e-06\n",
      "eval model running mean diff: 7.669305288615647e-07\n",
      "eval model running var diff: 7.3246174565611e-06\n",
      "train model forward diff: 1.0549838540541145e-06\n",
      "\n",
      "model_t step: 161000, train loss: 2.393908977277286, val loss: 2.1142126110230337\n",
      "model   step: 161000, train loss: 2.3939089773057773, val loss: 2.114212611221902\n",
      "eval model forward diff: 1.0562799558355707e-06\n",
      "eval model running mean diff: 7.447340482080733e-07\n",
      "eval model running var diff: 7.141543221678148e-06\n",
      "train model forward diff: 8.694267314979243e-07\n",
      "\n",
      "model_t step: 162000, train loss: 1.751904251351692, val loss: 2.114175457802808\n",
      "model   step: 162000, train loss: 1.7519042526357398, val loss: 2.114175458187042\n",
      "eval model forward diff: 1.1928883920098166e-06\n",
      "eval model running mean diff: 7.353890829620013e-07\n",
      "eval model running var diff: 6.798881429403991e-06\n",
      "train model forward diff: 7.468893510331753e-07\n",
      "\n",
      "model_t step: 163000, train loss: 2.0848897899908674, val loss: 2.1130369803967866\n",
      "model   step: 163000, train loss: 2.084889817869705, val loss: 2.113036981085946\n",
      "eval model forward diff: 1.259690266153246e-06\n",
      "eval model running mean diff: 7.246127595728069e-07\n",
      "eval model running var diff: 6.708933085519675e-06\n",
      "train model forward diff: 5.413991743807856e-07\n",
      "\n",
      "model_t step: 164000, train loss: 1.7581157109785124, val loss: 2.112762519216993\n",
      "model   step: 164000, train loss: 1.7581157118171111, val loss: 2.112762519778383\n",
      "eval model forward diff: 1.0108475935466377e-06\n",
      "eval model running mean diff: 7.263930599243906e-07\n",
      "eval model running var diff: 6.685312314402836e-06\n",
      "train model forward diff: 6.336126578965207e-07\n",
      "\n",
      "model_t step: 165000, train loss: 1.9259049839054059, val loss: 2.1124635192794763\n",
      "model   step: 165000, train loss: 1.925904991859903, val loss: 2.112463519915088\n",
      "eval model forward diff: 1.1399241745813082e-06\n",
      "eval model running mean diff: 7.114563951304831e-07\n",
      "eval model running var diff: 6.603594101761701e-06\n",
      "train model forward diff: 7.002374067255346e-07\n",
      "\n",
      "model_t step: 166000, train loss: 1.8152508898788975, val loss: 2.1120882630717963\n",
      "model   step: 166000, train loss: 1.8152508918736572, val loss: 2.112088263454979\n",
      "eval model forward diff: 1.0067051499362822e-06\n",
      "eval model running mean diff: 7.073253844858129e-07\n",
      "eval model running var diff: 6.757823712177924e-06\n",
      "train model forward diff: 7.614163521907358e-07\n",
      "\n",
      "model_t step: 167000, train loss: 2.2051915992888453, val loss: 2.114152309308552\n",
      "model   step: 167000, train loss: 2.2051916057635803, val loss: 2.114152309632571\n",
      "eval model forward diff: 1.1380481232114992e-06\n",
      "eval model running mean diff: 7.032305839871356e-07\n",
      "eval model running var diff: 6.697568494473671e-06\n",
      "train model forward diff: 8.404012332707822e-07\n",
      "\n",
      "model_t step: 168000, train loss: 2.223898780301336, val loss: 2.1127343723468863\n",
      "model   step: 168000, train loss: 2.2238987882692856, val loss: 2.1127343729077523\n",
      "eval model forward diff: 1.0206347069718902e-06\n",
      "eval model running mean diff: 6.882977627853393e-07\n",
      "eval model running var diff: 6.49649604156366e-06\n",
      "train model forward diff: 8.340039321375059e-07\n",
      "\n",
      "model_t step: 169000, train loss: 1.9113054224463768, val loss: 2.111611109032652\n",
      "model   step: 169000, train loss: 1.9113054053234855, val loss: 2.1116111095659416\n",
      "eval model forward diff: 1.0662537042183118e-06\n",
      "eval model running mean diff: 6.72284860225858e-07\n",
      "eval model running var diff: 6.547552942492985e-06\n",
      "train model forward diff: 6.604366848961263e-07\n",
      "\n",
      "model_t step: 170000, train loss: 1.7791053363787763, val loss: 2.112077798097434\n",
      "model   step: 170000, train loss: 1.7791053315666965, val loss: 2.1120777987016166\n",
      "eval model forward diff: 1.3224761063745927e-06\n",
      "eval model running mean diff: 6.781460184512866e-07\n",
      "eval model running var diff: 6.79758215937909e-06\n",
      "train model forward diff: 4.468491496822935e-07\n",
      "\n",
      "model_t step: 171000, train loss: 2.0747449623256395, val loss: 2.1129878352898\n",
      "model   step: 171000, train loss: 2.0747449684458417, val loss: 2.1129878356785046\n",
      "eval model forward diff: 1.2800089483899768e-06\n",
      "eval model running mean diff: 6.634531297677881e-07\n",
      "eval model running var diff: 6.794018844402672e-06\n",
      "train model forward diff: 5.204985664697404e-07\n",
      "\n",
      "model_t step: 172000, train loss: 2.4900062951905095, val loss: 2.112711045049679\n",
      "model   step: 172000, train loss: 2.4900063096761373, val loss: 2.1127110451164444\n",
      "eval model forward diff: 1.3010558826964669e-06\n",
      "eval model running mean diff: 6.510388290692504e-07\n",
      "eval model running var diff: 6.482626019987947e-06\n",
      "train model forward diff: 5.828387616979569e-07\n",
      "\n",
      "model_t step: 173000, train loss: 1.8505203157719938, val loss: 2.1121679203071215\n",
      "model   step: 173000, train loss: 1.8505203343352181, val loss: 2.1121679201112245\n",
      "eval model forward diff: 1.3200807392754754e-06\n",
      "eval model running mean diff: 6.425535690190998e-07\n",
      "eval model running var diff: 6.459852329498972e-06\n",
      "train model forward diff: 9.269063334649275e-07\n",
      "\n",
      "model_t step: 174000, train loss: 1.999491618522078, val loss: 2.112174632930354\n",
      "model   step: 174000, train loss: 1.99949162990985, val loss: 2.112174632926387\n",
      "eval model forward diff: 1.3269576805141625e-06\n",
      "eval model running mean diff: 6.361653341890872e-07\n",
      "eval model running var diff: 6.171882944272511e-06\n",
      "train model forward diff: 5.415086041793415e-07\n",
      "\n",
      "model_t step: 175000, train loss: 1.731388543174296, val loss: 2.1112745336756835\n",
      "model   step: 175000, train loss: 1.7313885406732812, val loss: 2.1112745338486363\n",
      "eval model forward diff: 1.3494939510028114e-06\n",
      "eval model running mean diff: 6.611199734329887e-07\n",
      "eval model running var diff: 6.336469681400558e-06\n",
      "train model forward diff: 4.770912109819392e-07\n",
      "\n",
      "model_t step: 176000, train loss: 2.1371861185321204, val loss: 2.1114068849547123\n",
      "model   step: 176000, train loss: 2.137186125313652, val loss: 2.111406885351201\n",
      "eval model forward diff: 1.1182540419207854e-06\n",
      "eval model running mean diff: 6.450955800652025e-07\n",
      "eval model running var diff: 6.213258018306078e-06\n",
      "train model forward diff: 6.285019606977471e-07\n",
      "\n",
      "model_t step: 177000, train loss: 1.9209639358175883, val loss: 2.1098298683482217\n",
      "model   step: 177000, train loss: 1.9209639350943264, val loss: 2.1098298685319965\n",
      "eval model forward diff: 1.1318286021744228e-06\n",
      "eval model running mean diff: 6.270054480062015e-07\n",
      "eval model running var diff: 6.368693107106083e-06\n",
      "train model forward diff: 4.439703067404821e-07\n",
      "\n",
      "model_t step: 178000, train loss: 2.4078483245296787, val loss: 2.110522813275956\n",
      "model   step: 178000, train loss: 2.407848318078462, val loss: 2.110522813588314\n",
      "eval model forward diff: 1.196388129565129e-06\n",
      "eval model running mean diff: 6.193496373541052e-07\n",
      "eval model running var diff: 6.426774888268483e-06\n",
      "train model forward diff: 6.046826883654433e-07\n",
      "\n",
      "model_t step: 179000, train loss: 2.079856255425086, val loss: 2.1117545509075355\n",
      "model   step: 179000, train loss: 2.079856259255361, val loss: 2.111754551334407\n",
      "eval model forward diff: 1.0125470278898874e-06\n",
      "eval model running mean diff: 6.113275214936209e-07\n",
      "eval model running var diff: 6.376181516998258e-06\n",
      "train model forward diff: 8.122408348132382e-07\n",
      "\n",
      "model_t step: 180000, train loss: 2.1742766611893285, val loss: 2.1112500926173485\n",
      "model   step: 180000, train loss: 2.1742766359399743, val loss: 2.111250092965358\n",
      "eval model forward diff: 1.0527588146236333e-06\n",
      "eval model running mean diff: 6.099103249113114e-07\n",
      "eval model running var diff: 6.217950186737653e-06\n",
      "train model forward diff: 5.035471248859835e-07\n",
      "\n",
      "model_t step: 181000, train loss: 1.8092228000941355, val loss: 2.110279330728208\n",
      "model   step: 181000, train loss: 1.8092228084089204, val loss: 2.110279330906745\n",
      "eval model forward diff: 9.971154999455223e-07\n",
      "eval model running mean diff: 6.082074222035772e-07\n",
      "eval model running var diff: 6.134807563284994e-06\n",
      "train model forward diff: 5.185005282903887e-07\n",
      "\n",
      "model_t step: 182000, train loss: 1.6781704855204114, val loss: 2.1120398291517604\n",
      "model   step: 182000, train loss: 1.6781704914242803, val loss: 2.1120398293933067\n",
      "eval model forward diff: 1.0566596793104566e-06\n",
      "eval model running mean diff: 5.966695120207532e-07\n",
      "eval model running var diff: 5.975254566692456e-06\n",
      "train model forward diff: 5.45397170537143e-07\n",
      "\n",
      "model_t step: 183000, train loss: 2.4642845056681093, val loss: 2.111632830662795\n",
      "model   step: 183000, train loss: 2.4642845095160126, val loss: 2.111632831090477\n",
      "eval model forward diff: 1.0770787550185013e-06\n",
      "eval model running mean diff: 5.890095016169994e-07\n",
      "eval model running var diff: 6.057152432958901e-06\n",
      "train model forward diff: 1.0996119259587545e-06\n",
      "\n",
      "model_t step: 184000, train loss: 2.359365519535533, val loss: 2.109429823363814\n",
      "model   step: 184000, train loss: 2.359365527600482, val loss: 2.1094298238210314\n",
      "eval model forward diff: 9.804069891483636e-07\n",
      "eval model running mean diff: 5.762675718834487e-07\n",
      "eval model running var diff: 6.277608093796516e-06\n",
      "train model forward diff: 3.921169209886166e-07\n",
      "\n",
      "model_t step: 185000, train loss: 1.8465720361514226, val loss: 2.1111781241953755\n",
      "model   step: 185000, train loss: 1.8465720343701486, val loss: 2.1111781245799146\n",
      "eval model forward diff: 9.002815564151945e-07\n",
      "eval model running mean diff: 5.567716652166155e-07\n",
      "eval model running var diff: 5.882136150603401e-06\n",
      "train model forward diff: 6.197611330804875e-07\n",
      "\n",
      "model_t step: 186000, train loss: 1.977999207598842, val loss: 2.110918412474054\n",
      "model   step: 186000, train loss: 1.9779992026363347, val loss: 2.1109184129381164\n",
      "eval model forward diff: 9.295614733506596e-07\n",
      "eval model running mean diff: 5.589357239266235e-07\n",
      "eval model running var diff: 5.634879954641292e-06\n",
      "train model forward diff: 5.49867421817396e-07\n",
      "\n",
      "model_t step: 187000, train loss: 1.8990387349467144, val loss: 2.1112432288267295\n",
      "model   step: 187000, train loss: 1.8990387291974113, val loss: 2.111243229255574\n",
      "eval model forward diff: 1.0712705702520253e-06\n",
      "eval model running mean diff: 5.587019993313902e-07\n",
      "eval model running var diff: 5.683933693489962e-06\n",
      "train model forward diff: 5.333621500014374e-07\n",
      "\n",
      "model_t step: 188000, train loss: 2.131658308124345, val loss: 2.1116017140363357\n",
      "model   step: 188000, train loss: 2.13165831142261, val loss: 2.1116017143502193\n",
      "eval model forward diff: 1.1408280709002128e-06\n",
      "eval model running mean diff: 5.683591108862629e-07\n",
      "eval model running var diff: 5.2728710784322175e-06\n",
      "train model forward diff: 4.3651747105855065e-07\n",
      "\n",
      "model_t step: 189000, train loss: 2.2500398537002053, val loss: 2.109850536898189\n",
      "model   step: 189000, train loss: 2.2500398468954645, val loss: 2.1098505370127354\n",
      "eval model forward diff: 1.0254712363710894e-06\n",
      "eval model running mean diff: 5.699775238454663e-07\n",
      "eval model running var diff: 4.9556681460671825e-06\n",
      "train model forward diff: 6.480364324046306e-07\n",
      "\n",
      "model_t step: 190000, train loss: 1.8174712055188034, val loss: 2.1100226295849973\n",
      "model   step: 190000, train loss: 1.8174712105583066, val loss: 2.110022630026324\n",
      "eval model forward diff: 1.0121962989995126e-06\n",
      "eval model running mean diff: 5.825320803509726e-07\n",
      "eval model running var diff: 5.052492696222544e-06\n",
      "train model forward diff: 4.8687336717812e-07\n",
      "\n",
      "model_t step: 191000, train loss: 1.7748037240118084, val loss: 2.110605361524517\n",
      "model   step: 191000, train loss: 1.7748037003112365, val loss: 2.1106053619775125\n",
      "eval model forward diff: 1.0780370949259321e-06\n",
      "eval model running mean diff: 5.718666082898949e-07\n",
      "eval model running var diff: 5.51935116277491e-06\n",
      "train model forward diff: 6.883754890552041e-07\n",
      "\n",
      "model_t step: 192000, train loss: 1.9661917948103382, val loss: 2.1106093070749345\n",
      "model   step: 192000, train loss: 1.966191804133424, val loss: 2.110609307391174\n",
      "eval model forward diff: 1.0968705979053084e-06\n",
      "eval model running mean diff: 5.746438578313473e-07\n",
      "eval model running var diff: 5.716318497661632e-06\n",
      "train model forward diff: 5.997772785093503e-07\n",
      "\n",
      "model_t step: 193000, train loss: 1.8902117295232603, val loss: 2.109885776104649\n",
      "model   step: 193000, train loss: 1.8902117317332845, val loss: 2.1098857765314\n",
      "eval model forward diff: 9.758801042814014e-07\n",
      "eval model running mean diff: 5.510710563427779e-07\n",
      "eval model running var diff: 5.582365190548444e-06\n",
      "train model forward diff: 6.328158468349443e-07\n",
      "\n",
      "model_t step: 194000, train loss: 2.2197773845532836, val loss: 2.109633735656287\n",
      "model   step: 194000, train loss: 2.219777403333171, val loss: 2.1096337359876878\n",
      "eval model forward diff: 9.750217782045212e-07\n",
      "eval model running mean diff: 5.569353689338641e-07\n",
      "eval model running var diff: 5.393855616375731e-06\n",
      "train model forward diff: 5.223429253575773e-07\n",
      "\n",
      "model_t step: 195000, train loss: 2.3011067987784677, val loss: 2.108708362698024\n",
      "model   step: 195000, train loss: 2.30110679746517, val loss: 2.108708363067324\n",
      "eval model forward diff: 9.034271972740981e-07\n",
      "eval model running mean diff: 5.603740387272182e-07\n",
      "eval model running var diff: 5.313622551739172e-06\n",
      "train model forward diff: 7.544239788437324e-07\n",
      "\n",
      "model_t step: 196000, train loss: 1.6385331124840832, val loss: 2.1093864593269265\n",
      "model   step: 196000, train loss: 1.6385331318003704, val loss: 2.109386459532573\n",
      "eval model forward diff: 8.792101215959747e-07\n",
      "eval model running mean diff: 5.603669057663296e-07\n",
      "eval model running var diff: 5.348355955447914e-06\n",
      "train model forward diff: 8.233796049417208e-07\n",
      "\n",
      "model_t step: 197000, train loss: 2.0832590908752087, val loss: 2.1095440243602948\n",
      "model   step: 197000, train loss: 2.0832590882551254, val loss: 2.1095440245427417\n",
      "eval model forward diff: 8.7729458009278e-07\n",
      "eval model running mean diff: 5.551836554928968e-07\n",
      "eval model running var diff: 5.761447965824118e-06\n",
      "train model forward diff: 3.7936547858663516e-07\n",
      "\n",
      "model_t step: 198000, train loss: 1.912179093551496, val loss: 2.108105306459088\n",
      "model   step: 198000, train loss: 1.9121790919289554, val loss: 2.108105306630838\n",
      "eval model forward diff: 8.791780916617142e-07\n",
      "eval model running mean diff: 5.543968741861249e-07\n",
      "eval model running var diff: 5.7831932451790635e-06\n",
      "train model forward diff: 6.162078900473489e-07\n",
      "\n",
      "model_t step: 199000, train loss: 1.988673648166956, val loss: 2.1092541636109914\n",
      "model   step: 199000, train loss: 1.9886736354053514, val loss: 2.1092541637703905\n",
      "eval model forward diff: 9.518131802721541e-07\n",
      "eval model running mean diff: 5.542899117472189e-07\n",
      "eval model running var diff: 5.391218877548454e-06\n",
      "train model forward diff: 4.986485908631266e-07\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import types\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "block_size = 3\n",
    "n_embd = 10\n",
    "n_hidden = 100\n",
    "vocab_size = 27\n",
    "n_layer = 5\n",
    "dtype = torch.float64\n",
    "eval_interval = 1000\n",
    "n_steps = 200000\n",
    "bs = 32\n",
    "ini_lr = 1.0\n",
    "eps=1e-5; momentum=0.001\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# original model\n",
    "model = MLP(vocab_size, block_size, n_embd, n_hidden, n_layer, dtype)\n",
    "# torch model\n",
    "layers = [\n",
    "    nn.Embedding(vocab_size, n_embd), nn.Flatten(),\n",
    "    nn.Linear(n_embd * block_size, n_hidden, bias=False), nn.BatchNorm1d(n_hidden, eps=eps, momentum=momentum), nn.Tanh(),\n",
    "]\n",
    "for _ in range(n_layer-2):\n",
    "    layers.extend([\n",
    "        nn.Linear(n_hidden, n_hidden, bias=False), nn.BatchNorm1d(n_hidden, eps=eps, momentum=momentum), nn.Tanh(),\n",
    "    ])\n",
    "layers.extend([\n",
    "    nn.Linear(n_hidden, vocab_size, bias=False), nn.BatchNorm1d(vocab_size, eps=eps, momentum=momentum),\n",
    "])\n",
    "model_t = nn.Sequential(*layers).to(dtype)\n",
    "# copy parameters\n",
    "for i, p_t in enumerate(model_t.parameters()):\n",
    "    p = model.parameters()[i]\n",
    "    if i == 0 or p.ndim == 1: # i=0 is embedding layer\n",
    "        p_t.data = p.data.clone() # clone to avoid inplace operation\n",
    "    else:\n",
    "        p_t.data = p.data.clone().T\n",
    "# add generate method to model_t\n",
    "model_t.block_size = block_size\n",
    "model_t.generate = types.MethodType(model.generate.__func__, model_t)\n",
    "# optimizer\n",
    "optimizer = SGD(model, ini_lr)\n",
    "optimizer_t = torch.optim.SGD(model_t.parameters(), lr=ini_lr)\n",
    "# loss\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "model_t.train()\n",
    "model.train()\n",
    "for step in range(n_steps):\n",
    "    lr = ini_lr if step < int(n_steps * 0.75) else ini_lr / 10\n",
    "    optimizer.lr = lr\n",
    "    for param_group in optimizer_t.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    idx = torch.randint(0, X_train.shape[0], (bs,))\n",
    "    x, y = X_train[idx], Y_train[idx]\n",
    "\n",
    "    # ----- torch -----\n",
    "    # forward\n",
    "    logits_t = model_t(x)\n",
    "    loss_t = F.cross_entropy(logits_t, y)\n",
    "    # backward\n",
    "    loss_t.backward()\n",
    "    # update\n",
    "    optimizer_t.step()\n",
    "    optimizer_t.zero_grad()\n",
    "\n",
    "    # ----- manual -----\n",
    "    # forward\n",
    "    logits = model(x)\n",
    "    loss = loss_fn(logits, y)\n",
    "    # backward\n",
    "    # since grad buffer is stored in model class, we need to call backward imediately after forward\n",
    "    # otherwise, grad buffer will be overwritten by next forward\n",
    "    h_grad = loss_fn.backward(grad=1.0) # last layer, dloss=1.0\n",
    "    model.backward(h_grad)\n",
    "    # update\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # eval\n",
    "    if step % eval_interval == 0: \n",
    "        # val loss is actually one step later than train loss\n",
    "        x, y = X_val, Y_val\n",
    "        model_t.eval()\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_t = model_t(x)\n",
    "            val_loss_t = F.cross_entropy(logits_t, y)\n",
    "            print(f'model_t step: {step}, train loss: {loss_t.item()}, val loss: {val_loss_t.item()}')\n",
    "        logits = model(x)\n",
    "        val_loss = loss_fn(logits, y)\n",
    "        print(f'model   step: {step}, train loss: {loss.item()}, val loss: {val_loss.item()}')\n",
    "        print(f\"eval model forward diff: {(model(x) - model_t(x)).abs().max()}\")\n",
    "        rm, rv = model.net.layers[3].running_mean, model.net.layers[3].running_var\n",
    "        rmt, rvt = model_t[3].running_mean, model_t[3].running_var\n",
    "        print(f\"eval model running mean diff: {(rm - rmt).abs().max()}\")\n",
    "        print(f\"eval model running var diff: {(rv - rvt).abs().max()}\")\n",
    "        model_t.train()\n",
    "        model.train()\n",
    "    \n",
    "        x, y = X_train[idx], Y_train[idx]\n",
    "        print(f\"train model forward diff: {(model(x) - model_t(x)).abs().max()}\")\n",
    "        print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  test loss: 2.109030602058709\n",
      "model_t test loss: 2.1090306021799923\n",
      "\n",
      "...anuel.\n",
      "...avann.\n",
      "...aarian.\n",
      "...dan.\n",
      "...shan.\n",
      "...silvin.\n",
      "...alaya.\n",
      "...jermann.\n",
      "...elianna.\n",
      "...anna.\n",
      "\n",
      "...anuel.\n",
      "...avann.\n",
      "...aarian.\n",
      "...dan.\n",
      "...shan.\n",
      "...silvin.\n",
      "...alaya.\n",
      "...jermann.\n",
      "...elianna.\n",
      "...anna.\n"
     ]
    }
   ],
   "source": [
    "x, y = X_test, Y_test\n",
    "\n",
    "model.eval()\n",
    "logits = model(x)\n",
    "test_loss = loss_fn(logits, y).item()\n",
    "print(f'model  test loss: {test_loss}')\n",
    "model_t.eval()\n",
    "logits = model_t(x)\n",
    "test_loss = F.cross_entropy(logits, y).item()\n",
    "print(f'model_t test loss: {test_loss}')\n",
    "print()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "for _ in range(10):\n",
    "    out = model.generate('.' * block_size, max_new_tokens=10, do_sample=True, temperature=0.5)\n",
    "    print(out)\n",
    "print()\n",
    "torch.manual_seed(42)\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        out = model_t.generate('.' * block_size, max_new_tokens=10, do_sample=True, temperature=0.5)\n",
    "        print(out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
